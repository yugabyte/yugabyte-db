###########################################
#        Yugaware default configuration   #
###########################################
# You can override these in application.conf or through system properties.

# Install custom ObjectMapper
play.modules.disabled += "play.core.ObjectMapperModule"
play.modules.enabled  += "com.yugabyte.yw.modules.CustomObjectMapperModule"
play.modules.enabled  += "com.yugabyte.yw.modules.PerfAdvisorDBModule"
play.cache.bindCaches = ["runtimeConfigsCache"]


## Secret key
# http://www.playframework.com/documentation/latest/ApplicationSecret
# ~~~~~
# The secret key is used to sign Play's session cookie.
# This must be changed for production, but we don't recommend you change it in this file.
play.crypto.secret="changeme"
play.http.secret.key=${play.crypto.secret}

play.http.parser.maxDiskBuffer=100GB
play.forms.binding.directFieldAccess=true

kamon.instrumentation.logback.mdc.copy.enabled=false

pekko {
  # On slow machines we sometimes see that logger is not able to start in 5 secs
  logger-startup-timeout=30s
  actor {
    default-dispatcher {
      type = "com.yugabyte.yw.common.logging.MDCPropagatingDispatcherConfigurator"
    }
  }
  coordinated-shutdown {
    phases {
      service-requests-done {
        timeout = 150s
      }
    }
  }
}

application.home = "."
log.override.path = ${application.home}"/logs"

# ============= START DATABASE RELATED CONFIGURATION ==================================

# Lets disable the default play evolutions and use flyway db (overridden by yugabyted)
play.evolutions.enabled=false
# We use our own flyway initializer see: com.yugabyte.yw.common.ybflyway.PlayInitializer
# So we do not need to enable flyway-play module
# play.modules.enabled += "org.flywaydb.play.PlayModule"

db {
  # Tracking default postgresql connection details
  default {
    host="localhost"
    port=5432
    dbname="yugaware"
    username="postgres"
    username=${?DB_USERNAME}
    password=""
    password=${?DB_PASSWORD}
    driver="org.postgresql.Driver"
    url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}
    logStatements=true
    pg_dump_path=""
    pg_restore_path=""
    # Config about flyway migrations.
    migration {
      table=schema_version
      initOnMigrate=true
      auto=true
      ignoreMigrationPatterns=["*:missing","*:future"]
      outOfOrder=true
      scriptsDirectory="default_"
      # We want to use postgres db in production
      # The migration scripts will be under resources/db.migration.default.postgres
      # with common scripts under resources/db.migration.default.common
      locations=["common","postgres"]
    }
  }
  perf_advisor {
    # We assume both databases are on the same PG instance. If not - this needs to be overriden
    username=${db.default.username}
    password=${db.default.password}
    url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.perf_advisor.dbname}
    createDatabaseUrl="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}
    migration {
      ignoreMigrationPatterns=["*:missing","*:future"]
      table=schema_version
    }
  }
}

ebean {
  default = ["com.yugabyte.yw.*"]
}
# =============  END DATABASE RELATED CONFIGURATION ==================================

yb {
  url = ""
  seedData=false
  multiTenant=false
  use_kubectl=true
  use_new_helm_naming=true
  universe_version_check_mode=NEVER  # possible values: NEVER, HA_ONLY, ALWAYS
  universe_boot_script = ""
  is_platform_downgrade_allowed=false
  is_platform_downgrade_allowed=${?YB_IS_PLATFORM_DOWNGRADE_ALLOWED}
  start_master_on_stop_node=true
  use_k8s_custom_resources=true
  start_master_on_remove_node=true
  use_spot_instances = false
  log.logEnvVars=false
  devops {
    home = ""
    command_timeout = 3 hours
  }
  diag {
    thread_dumps.gcs {
      enabled = false
      bucket = ""
      credentials = ""
    }
  }
  thirdparty.packagePath = /opt/third-party
  grafana.accessKey="changeme"
  allow_db_version_more_than_yba_version=false
  skip_version_checks=false
  skip_version_checks=${?YB_SKIP_VERSION_CHECKS}
  fixPaths=false
  ysql_timeout_secs = 180
  num_cores_to_keep=5
  install_locales_db_nodes=false
  use_single_zone=false
  max_volume_count = 32
  ws = ${play.ws}

  rbac {
    use_new_authz=true
  }

  wellKnownCA {
    trustStore {
      path=${?javax.net.ssl.trustStore}""
      password=${?javax.net.ssl.trustStorePassword}""
      type=${?javax.net.ssl.trustStoreType}""
      javaHomePaths = [${java.home}"/lib/security/jssecacerts",  ${java.home}"/lib/security/cacerts"]
    }
  }
  certVerifyBackupRestore{
    is_enforced = true
  }
  regex {
    release_pattern {
      ybdb = "[^.]+yugabyte-(?:ee-)?(.*)-(alma|centos|linux|el8|darwin)(.*).tar.gz"
      helm = "[^.]+yugabyte-(.*)-helm.tar.gz"
    }
  }
  docker {
    network = "bridge"
    release = "/opt/yugabyte/ybc/release"
  }
  swamper {
    targetPath = ""
    rulesPath = ""
  }
  pitr {
    create_poll_delay = 1s
    create_timeout = 15 minutes
    restore_poll_delay = 1s
    restore_timeout = 15 minutes
  }

  ui {
    enable_dedicated_nodes=true
    show_cost=true

    feature_flags {
      perf_advisor=true
      provider_redesign=true
      granular_metrics=true
      edit_in_use_provider=true
      gflag_multiline_conf=true
      enable_troubleshooting=false
      enable_per_process_metrics=true
      enable_rf_change=false
      off_cluster_pitr_enabled=false
    }

    xcluster {
      enable_skip_bootstrapping=false
      dr: {
        # Show underlying xCluster configs used in DR on the xCluster config tab
        # Relevant xCluster operations can and should be done through the DR UI instead.
        # This exists only as a fallback and a way to get further information on the YBA UI.
        show_xcluster_config = false
      }
    }

    metrics: {
      enable_download_pdf=false
    }
  }

  edit_provider {
    new {
      allow_wait=true
      wait_for_tasks_timeout_ms=1200000 # 20 mins
      wait_for_tasks_step_ms=5000 # 5 sec
      # Keeping the image bundle validations off by default
      # Will turn this on once we make image bundle as first class
      # object in provider.
      disable_image_bundle_validations = false
      allow_used_bundle_edit = false
    }
  }

  provider {
    validate_ntp_server_count = 1
    # This can contain comma separated values, like "/,/tmp,"
    monitored_mount_roots = "/"
    vm_os_patching = true
    kubernetes_provider_validation = true
    azure_provider_validation = true
    gcp_provider_validation = true
  }

  universe {
    auth {
      is_enforced = false
    }
    network_load_balancer {
      custom_health_check_ports = [],
      custom_health_check_protocol = "TCP",
      custom_health_check_paths = [],
    }
    user_tags {
      is_enforced = false
      enforced_tags = []
      dev_tags = []
    }
    otel_collector_enabled = false
    geo_partitioning_enabled = false
    validate_local_release = true
    otel_collector_metrics_port = 8889
    audit_logging_enabled = false
    allow_connection_pooling = false
    default_service_scope_for_k8s="AZ" # possible: Namespaced, AZ
    consistency_check {
      enabled = true
      test_pending = false # NEVER SET IN PROD
      update_delay_secs = 0 # NEVER SET IN PROD
      ysql_timeout_secs = 30
    }
  }

  xcluster {
    db_scoped {
      creationEnabled = false
    }
    bootstrap_producer_timeout = 2 minutes
    k8s_tls_support = true
    operation_timeout = 30 minutes
    enable_auto_flag_validation = true
    transactional {
      enabled = true
      pitr {
        default_retention_period = 1d
        default_snapshot_interval = 6h
      }
      wait_for_replication_drain_timeout = 2h
      allow_multiple_configs = true
    }
    dr {
      enabled = true
    }
    is_bootstrap_required_rpc_pool {
        # initial and minimum number of threads used by live node poller
        core_threads = 10
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 50
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }
    use_ybc = true
    sleep_time_before_restore = 20s
    ensure_sync_get_replication_status = false
    bootstrap_producer_timeout_ms = 120000
    xcluster_sync_scheduler_interval = 10 minutes
    xcluster_metrics_scheduler_interval = 2 minutes
    xcluster_sync_on_universe = true
    get_api_timeout_ms = 15000
    db_sync_timeout_ms = 120000
  }

  # Enable/Disable Runtime Config UI under Admin section
  runtime_conf_ui {
    enable_for_all = false
    tag_filter = ["PUBLIC"]
  }

  filepaths {
    # Provider scope config that specifies the tmp directory path to be used for performing
    # operation on the nodes as part of onprem nodes. ex - preflight checks.
    remoteTmpDirectory = "/tmp"
    tmpDirectory = "/tmp"
  }

  health_checks {
    check_clock_time_drift = true
    time_drift_wrn_threshold_ms = 200
    time_drift_err_threshold_ms = 400
    clock_sync_service_required = true
  }

  # Alerts thresholds
  alert {
    # Value of maximum allowed clock skew before an alert is generated (in ms).
    max_clock_skew_ms = 250
    # Value of maximum allowed replication lag before an alert is generated (in ms).
    replication_lag_ms = 180000
    # Value of maximum allowed percents of used memory on nodes.
    max_memory_cons_pct = 90
    # Alert rules configuration sync interval in seconds.
    config_sync_interval_sec = 60
    # Maximum allowed number of nodes with health check errors.
    health_check_nodes = 0
    # Maximum allowed number of nodes with inactive cronjob.
    inactive_cronjob_nodes = 0
    # For how long do we let the alert be in database after it has resolved
    resolved_retention_duration = 120 days
    # Value of average CPU usage which triggers warning alert
    max_cpu_usage_pct_warn = 90
    # Value of average CPU usage which triggers severe alert
    max_cpu_usage_pct_severe = 95
    # CPU usage alert metric aggregation interval
    cpu_usage_interval_secs = 1800
    # Value of node disk usage which triggers severe alert
    max_node_disk_usage_pct_severe = 70
    # Value of node monitored volume usage which triggers severe alert
    max_node_system_disk_usage_pct_severe = 80
    # Value of node file descriptors usage which triggers severe alert
    max_node_fd_usage_pct_severe = 70
    # Value of allowed OOM kills per 10 minutes before severe alert is triggered
    max_oom_kills_severe = 3
    # Value of allowed OOM kills per 10 minutes before warning alert is triggered
    max_oom_kills_warning = 1
    # Value of node certificate expiry in days which triggers severe alert
    max_node_cert_expiry_days_severe = 30
    # Value of encryption at rest config expiry in days which triggers severe alert
    max_enc_at_rest_config_expiry_days_severe = 3
    # Maximum average latency for YSQL operations
    max_ysql_opavg_latency = 10000
    # Maximum average latency for YCQL operations
    max_ycql_opavg_latency = 10000
    # Maximum P99 latency for YSQL operations
    max_ysql_p99_latency = 60000
    # Maximum P99 latency for YCQL operations
    max_ycql_p99_latency = 60000
    # Maximum number of YSQL connections
    max_ysql_connections = 300
    # Maximum number of YCQL connections
    max_ycql_connections = 1000
    # Maximum number of YEDIS connections
    max_yedis_connections = 1000
    # Maximum YSQL throughput
    max_ysql_throughput = 100000
    # Maximum YCQL throughput
    max_ycql_throughput = 100000
    # Underreplicated masters threshold which triggers severe alert
    underreplicated_masters_secs_severe = 900
    # Underreplicated tablet threshold which triggers severe alert
    underreplicated_tablets_secs_severe = 300
    # Leaderless tablet threshold which triggers severe alert
    leaderless_tablets_secs_severe = 300
    # Value of days to expiry for SSH keys which triggers severe alert
    ssh_key_config_expiry_days_severe = 30
    pagerduty {
      ws = ${play.ws}
    }
    slack {
      ws = ${play.ws}
    }
    webhook {
      ws = ${play.ws}
    }
  }

  troubleshooting{
    ws = ${play.ws}
    ws.ssl.loose.acceptAnyCertificate = true
  }
  # Used to skip certificates validation for the configure phase.
  # Possible values - ALL, HOSTNAME, NONE
  #(the latter is used for skipping validation of commonName and subjectAltName)
  tls{
    skip_cert_validation = NONE
    enable_config_validation = true
  }
  commissioner {
    # initial and minimum number of threads used by commissioner
    core_threads = 50

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 200

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 0
  }

  maintenance {
    # For how long do we let the maintenance window be in database after it has finished
    retention_duration = 1200 days
  }

  task {
    # initial and minimum number of threads used by each task
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 10

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 1000

    # Whether overriding universe lock is allowed when force option is selected.
    # If it is disabled, force option will wait for the lock to be released.
    override_force_universe_lock = false

    # How long force lock should retry acquiring the universe's lock when
    # `override_force_universe_lock` is false. If no unit is selected, it will be in milliseconds.
    max_force_universe_lock_timeout = "1800s"

    # Verify current cluster state (from db perspective) before running task
    verify_cluster_state = true

    # On YBA restart, auto retry aborted tasks within the time window, due to yba shutdown
    auto_retry_on_yba_restart_time_window = 0h

    eviction_wait_time = 2 minutes

    queued_lifetime = 3 minutes

    upgrade {
        batch_roll_enabled = false
        batch_roll_enabled_k8s = false
        batch_roll_auto_percent = 0
        batch_roll_auto_number = 0
    }
  }

  backup_task {
    # initial and minimum number of threads used by each backup task
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 10

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 2000
  }

  import {
    # initial and minimum number of threads used by import controller
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 200

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 1000
  }

  authtoken {
    # Expiry time of auth token
    # https://github.com/lightbend/config/blob/main/HOCON.md#duration-format for supported formats
    token_expiry = 12 hours
  }

  # We delete completed task info form database.
  # Following config is for that task Garbage collection:
  taskGC {
    # How frequently do we check for completed tasks in database
    gc_check_interval = 1 days

    # For how long do we let the task be in database after it has completed
    task_retention_duration = 120 days
  }

  # Config for backup Garbage collection
  backupGC {
    # backup GC schedule run
    gc_run_interval = 15 minutes
    number_of_retries = 3
  }

  auto_master_failover {
    poller_interval = 2 minutes
    detect_interval = 1 minute
    failover_task_delay = 1 minute
    sync_master_addrs_task_delay = 1 minute
    enabled = false
    master_follower_lag_soft_threshold = 10 minutes
    master_follower_lag_hard_threshold = 30 minutes
    master_follower_lag_threshold_error = 1 minute
    cooldown = 30 minutes
    max_task_retries = 1

    executor {
      core_threads = 3
      # max number of threads we will grow to if needed before starting to reject tasks
      max_threads = 50
      # duration for which thread pool will stay inflated before it shrinks back to core_threads
      thread_ttl = 1 minute
      # capacity of the thread pool queue
      queue_capacity = 10
    }
  }

  snapshot_schedule {
    run_interval = 5 minutes
  }

  snapshot_cleanup {
    delete_orphan_on_startup = true
  }

  aws {
    # default volume count for aws instance types with EBS Only storage info
    default_volume_count = 1

    # Default AWS instance type
    default_instance_type = "c5.4xlarge"

    # default volume size for aws instance types with EBS Only storage info
    default_volume_size_gb = 250

    # List of supported AMI arch
    supported_arch_types = ["x86_64", "arm64"]

    # List of supported AMI root device
    supported_root_device_type = ["ebs"]

    # List of supported AMI platform
    supported_platform = ["linux"]

    storage {
      # Default storage type
      default_storage_type = "GP3"

      # GP3 free PIOPS
      gp3_free_piops = 3000

      # GP3 free throughput in MiB/sec
      gp3_free_throughput = 125
    }

    enable_imdsv2_support = true
    iam_credentials_num_retries = 10

    disk_resize_cooldown_hours = 6
  }

  gcp {
    # default volume size for gcp instance types
    default_volume_size_gb = 375

    operations {
      # Polling interval for checking the operation status
      status_polling_interval = 5s

      # Timeout interval for operations to complete
      timeout_interval = 180s
    }


    # Default GCP instance type
    default_instance_type = "n1-standard-1"

    storage {
      default_storage_type = "Persistent"
    }

    permissions_sa {
      # additional permissions that are required for a Service Account to create a new VPC
      permissions_new_vpc = ["compute.networks.create", "compute.networks.addPeering"]
      # granular permissions of a Service Account
      min_permissions_sa = ["compute.instances.create", "compute.images.useReadOnly", "compute.instanceTemplates.useReadOnly", "compute.networks.use", "compute.addresses.use", "compute.subnetworks.use", "compute.instances.setMetadata", "compute.instances.setTags", "compute.instances.setLabels", "compute.instances.setServiceAccount", "compute.disks.create", "compute.instances.delete"]
    }
  }

  azure {
    # default volume size for azure instance types
    default_volume_size_gb = 250

    # Default Azure instance type
    default_instance_type = "Standard_DS2_v2"

    storage {
      default_storage_type = "Premium_LRS"
    }

    custom_params {
      vm = ""
      disk = ""
      network = ""
    }

    vm {
      ignore_plan = false
    }

    show_premiumv2_storage_type = false
  }

  kubernetes {
    storageClass = ""
    pullSecretName = ""
    # Default value is empty here but set in helm chart.
    yugawareImageRepository = ""
    yugawareImageTag = ""
    # default volume count for kubernetes
    default_volume_count = 1

    # default volume size for kubernetes instance types
    default_volume_size_gb = 50

    # Default kubernetes instance type
    default_instance_type = "small"

    # default memory in gb
    default_memory_size_gb = 4

    # min memory in gb
    min_memory_size_gb = 2

    # max memory in gb
    max_memory_size_gb = 1000

    # default CPU cores
    default_cpu_cores = 2

    # min CPU cores
    min_cpu_cores = 0.5

    # max CPU cores
    max_cpu_cores = 100

    operator {
      enabled = false
      block_api_operator_owned_resources = true
      namespace = ""
      crash_yba_on_operator_failure = true
      customer_uuid=""
    }

    wait_for_gflag_sync_sec = 90
  }

  pwdpolicy {
    default_min_length = 8
    default_min_uppercase = 1
    default_min_lowercase = 1
    default_min_digits = 1
    default_min_special_chars = 1
    default_disallowed_chars = "`"
  }
  metrics {
    host="localhost"
    port="9090"
    protocol="http"
    # Ideally this should not contain path - so that we can build any url out of it,
    # but for backward compatibility we leave it as is and just replace /api/v1
    # with other path where needed
    url = ${yb.metrics.protocol}"://"${yb.metrics.host}":"${yb.metrics.port}/api/v1
    # Empty means - use url above without /api/v1 suffix
    external.url = ""
    auth = false
    auth_username = ""
    auth_password = ""
    default_points = 250
    link.use_browser_fqdn = true
    management.enabled = true
    db_read_write_test = true
    ysqlsh_connectivity_test = true
    cqlsh_connectivity_test = true
    # Scrape target configuration sync interval in seconds.
    config_sync_interval_sec = 60
    scrape_interval = "10s"
    # May want to increase this one in case federation
    # request starts taking more than 10 seconds to complete.
    scrape_interval_standby = ${yb.metrics.scrape_interval}
    collection_level="NORMAL"
    ws = ${play.ws}
    # By default we don't care about promehteus cert content - as it's co-located with YBA
    # Checking hostname and issuer will not work for replicated self signed, for sure, as
    # we're connecting through docker gateway IF + we don't have this cert in YBA truststore
    ws.ssl.loose.acceptAnyCertificate = true
  }
  # sets logging level for file and stdout logs
  logging {
    config="DEBUG"
    rollover_pattern = "yyyy-MM-dd"
    max_history = "30"
    search_timeout_secs = "60"
    enable_task_failed_request_logs = true
    fileNamePrefix = ""
  }
  storage.path="/opt/yugabyte"
  upgrade {
    #  Allow for leader blacklisting during universe upgrades
    blacklist_leaders = true
    blacklist_leader_wait_time_ms = 60000
    max_follower_lag_threshold_ms = 60000
    allow_downgrades=false
    allow_downgrades=${?YB_UPGRADE_ALLOW_DOWNGRADES}
    single_connection_ysql_upgrade=false
    allow_upgrade_on_transit_universe = false
    promote_auto_flag = true
    auto_flag_update_sleep_time_ms = 60000
    promote_flags_forcefully = false
    ysql_upgrade_timeout_sec = 1800
    skip_finalize = false
    enable_rollback_support = true
  }
  edit {
    wait_for_leaders_on_preferred=true
    wait_before_blacklist_clear = 90000 ms
  }
  releases {
    path = "/opt/yugabyte/releases"
    num_releases_to_keep_default = 3
    num_releases_to_keep_cloud = 2
    download_helm_chart_http_timeout = 60s
    use_redesign = true
    artifacts {
      # Depcreated, use relative_upload path
      upload_path = "/opt/yugabyte/upload/release_artifacts"
      # Relative path from yb.storage.path
      relative_upload_path = "upload/release_artifacts"
    }
  }
  ha {
    logScriptOutput = false
    num_backup_retention = 10
    prometheus_config_dir = "/prometheus_configs"
    replication_schedule_enabled = false
    replication_frequency = 30 minutes
    # 0 - never shutdown
    # 1 - only shutdown promoted instance
    # 2 - shutdown both promoted and demoted instance
    shutdown_level = 2
    test_request_timeout = 10 seconds
    test_connection_timeout = 10 seconds
    ws = ${play.ws}
    # Override this ws config in runtime_config at global level
    # Reference: https://github.com/playframework/play-ws/blob/main/play-ws-standalone/src/main/resources/reference.conf
    # Example:
#      {
#      ssl {
#         loose.acceptAnyCert = false
#         trustManager = {
#           stores += { # append to certs defined in play.ws
#               type = "PEM"
#               data = """-----BEGIN CERTIFICATE-----
# MIIDzTCCArWgAwIBAgIQCjeHZF5ftIwiTv0b7RQMPDANBgkqhkiG9w0BAQsFADBa
# ... You can use triple quoted string for multiline data ...
# -----END CERTIFICATE-----"""
#           }
#           stores += {
#             ... you can trust multiple certs ...
#           }
#        }
#      }
#    }
  }

  wait_for_server_timeout = 300000 ms
  wait_for_yqlserver_retry = 30000 ms

  wait_for_master_leader_timeout = 30000 ms
  # Timeout for proxy endpoint request of db node
  proxy_endpoint_timeout = 1 minute

  wait_for_lb_for_added_nodes = false
  always_wait_for_data_move = true

  wait_for_clock_sync {
    # Inline refers to wait before starting the processes. Eg, it handles out-of-band restart.
    inline_enabled = true
    max_acceptable_clock_skew = 500ms
    timeout = 5m
    enabled = true
  }
  node_ops {
    leader_blacklist {
      fail_on_timeout = false
    }
    destroy_server_timeout= 10m
  }
  checks {
    nodes_safe_to_take_down {
      timeout = 10m
      enabled = true
      parallelism = 5
    }
    under_replicated_tablets {
      timeout = 10m
      enabled = true
    }
    change_master_config {
      timeout = 10m
      enabled = true
    }
    follower_lag {
      timeout = 10m
      enabled = true
      max_threshold = 60000ms
    }
    wait_for_server_ready {
      timeout: 10m
    }
    cluster_membership {
      enabled = true
      timeout = 1m
    }
    leaderless_tablets {
      timeout = 5m
      enabled = true
    }
    node_disk_size {
      target_usage_percentage = 100
    }
  }

  health {
    max_num_parallel_checks = 25
    max_num_parallel_node_checks = 50
    consistency_check_parallelism = 20
    # Email address to send alerts to at YugaByte.
    default_email = ""
    default_email = ${?YB_ALERTS_EMAIL}

    debug_email = false

    # Default timeout for establishing the SMTP connection, in msec.
    smtp_connection_timeout_ms = 30000
    # Default timeout for sending the mail messages, in msec.
    smtp_timeout_ms = 60000

    # Interval at which to check the status of every universe. Default: 5 minutes.
    check_interval_ms = 300000
    # Interval at which to store the status of every universe in DB. Default: 5 minutes.
    store_interval_ms = 300000
    # Interval at which to send a status report email. Default: 12 hours.
    status_interval_ms = 43200000
    # Interval at which to check for DDL atomicity. Default: 1 hour.
    ddl_atomicity_interval_sec = 3600
    ddl_atomicity_check_enabled = true
    logOutput = false
    nodeCheckTimeoutSec = 180

    trigger_api.enabled = ${yb.cloud.enabled}
  }

  fs_stateless {
    suppress_error = ${yb.cloud.enabled}
    // 10MB
    max_file_size_bytes = 10000000
    max_files_count_persist = 10000
    disable_sync_db_to_fs_startup = ${yb.cloud.enabled}
  }

  perf_advisor {
    # disabled by default - can enable for particular universe
    enabled = false
    # max number of threads to support parallel querying of nodes
    max_threads = 22
    # interval for perf advisor scheduler runs, in minutes
    scheduler_interval_mins = 5
    # Perf advisor scheduler universe batch size
    universe_batch_size = 5
    # default interval for perf advisor runs for the universe, in minutes.
    universe_frequency_mins = 10

    cleanup {
      gc_check_interval = 1 days
      # For how long do we let the recommendation be in database
      rec_retention_duration = 30 days
      # For how long do we let the PA runs be in database
      pa_run_retention_duration = 30 days
    }
  }

  user {
    disable_v1_api_token = false
  }

  security {
    enable_detailed_logs = false
    enable_auth_for_proxy_metrics = true
    use_oauth = false
    use_oauth = ${?USE_OAUTH}
    type = ""
    type = ${?YB_SECURITY_TYPE}
    clientID = ""
    clientID = ${?YB_OIDC_CLIENT_ID}
    secret = ""
    secret = ${?YB_OIDC_SECRET}
    discoveryURI = ""
    discoveryURI = ${?YB_OIDC_DISCOVERY_URI}
    oidcProviderMetadata = ""
    oidcProviderMetadata = ${?YB_OIDC_PROVIDER_METADATA}
    oidcScope = ""
    oidcScope = ${?YB_OIDC_SCOPE}
    oidcEmailAttribute = ""
    oidcEmailAttribute = ${?YB_OIDC_EMAIL_ATTR}
    oidcRefreshTokenEndpoint = ""
    oidcRefreshTokenEndpoint = ${?YB_REFRESH_TOKEN_ENDPOINT}
    oidcRefreshTokenInterval = 5 minutes
    showJWTInfoOnLogin = false
    showJWTInfoOnLogin = ${?YB_SHOW_JWT_TOKEN}
    enable_external_script = false
    enable_external_script = ${?ENABLE_EXTERNAL_SCRIPT}
    oidc_feature_enhancements = true
    oidc_enable_auto_create_users = true
    oidc_group_claim = "groups"
    oidc_default_role = "ReadOnly"
    ssh2_enabled = false
    group_mapping_rbac_support = false
    ldap {
      use_ldap = "false"
      ldap_url = ""
      ldap_port = ""
      ldap_basedn = ""
      ldap_dn_prefix = "CN="
      ldap_customeruuid = ""
      ldap_service_account_distinguished_name = ""
      ldap_service_account_password = ""
      enable_ldaps = false
      enable_ldap_start_tls = false
      use_search_and_bind = false
      ldap_search_attribute = ""
      ldap_search_filter = ""
      ldap_group_search_filter = ""
      ldap_group_search_scope = "SUBTREE"
      ldap_group_search_base_dn = ""
      ldap_group_member_of_attribute = "memberOf"
      ldap_group_use_query = false
      ldap_group_use_role_mapping = false
      ldap_default_role = "ReadOnly"
      ldap_tls_protocol="TLSv1_2"
      enforce_server_cert_verification = true
      ldap_universe_sync = "false"
      page_query_size = 2
    }
    forbidden_ips="169.254.169.254"
    custom_hooks {
      enable_custom_hooks = false
      enable_sudo = false
      enable_api_triggered_hooks = ${yb.cloud.enabled}
    }
    ssh_keys {
      enable_ssh_key_expiration = true
      ssh_key_expiration_threshold_days = 365
    }
    default.access.key = "yugabyte-default"
    enable_db_query_api = false
  }

  ansible {

    # strategy can be linear, mitogen_linear or debug
    strategy = "linear"
    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#default-timeout
    conn_timeout_secs = 120

    # verbosity of ansible logs, 0 to 4 (more verbose)
    verbosity = 0
    # debug output (can include secrets in output)
    debug = false

    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#diff-always
    diff_always = false

    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#default-local-tmp
    local_temp = "/tmp/ansible_tmp/"
  }

  customer_task_db_query_limit = 2000
  task_info_db_query_batch_size = 10
  cloud {
    enabled = false
    requestIdHeader = "X-REQUEST-ID"
  }

  ybc_flags {
    nfs_dirs = "/tmp/nfs,/nfs"
    listen_on_all_interfaces_k8s = true
  }

  internal {
    # Enabling removes supported instance type filtering on AWS providers.
    allow_unsupported_instances = false
    # Enabled fault injected paths with failure rates. E.g ybops.cloud.azure.utils.test1=0.2.
    ybops_fault_injected_paths = ""
    # Disable node agent property in provider details.
    disable_node_agent_provider_property = ${yb.cloud.enabled}
  }

  dbmem {
    postgres {
      max_mem_mb = 0
      # Max memory value for read replicas.
      # -1: use the same value as max_mem_mb
      # 0: Do not set read replica max mem
      # >0: Set read replica max mem mb to this value
      rr_max_mem_mb = -1
    }

    checks {
      # Used in checking memAvailable.
      mem_available_limit_kb = 716800
      # memory check timeout in secs
      timeout = 30
    }
  }

  backup {
    pg_based = false
    disable_xxhash_checksum = false
    log.verbose = false
    minIncrementalScheduleFrequencyInSecs = 900
    enable_sse = false
    allow_table_by_table_backup_ycql = false
    delete_expired_backup_max_gc_size = 10
    use_server_broadcast_address_for_yb_backup = true
    skip_metadata_validation = false
    always_backup_tablespaces = false
    skip_config_based_preflight_validation = false
    pit_enabled_backups_history_retention_buffer_time_secs = 3600
  }

  logs {
    cmdOutputDelete = true
    max_msg_size = 2M
    shell.output_retention_hours = 1
    shell.output_dir_max_size = 10K
  }

  audit {
    log {
      # Used in AuditAction to enable audit logging checks
      verifyLogging = false
      outputToStdout = false
      outputToFile = true
      rolloverPattern = "yyyy-MM-dd"
      maxHistory = "30"
      fileNamePrefix = ""
    }
  }

  configure_db_api {
    ysql = true
    ycql = true
  }

  snapshot_creation {
    # Config for attempts and delays for snapshot creation
    max_attempts = 80
    delay = 15
  }

  support_bundle {
    # default N days of logs to get if no dates specified
    default_date_range = 7
    application_logs_regex_pattern = "application-log-\\d{4}-\\d{2}-\\d{2}(\\.gz)?"
    application_logs_sdf_pattern = "'application-log-'yyyy-MM-dd"
    k8s_mount_point_prefix = "/mnt/disk"
    default_mount_point_prefix = "/mnt/d"
    universe_logs_regex_pattern = "((?:.*)(?:yb-)(?:master|tserver)(?:.*))(\\d{8})-(?:\\d*)\\.(?:.*)"
    postgres_logs_regex_pattern = "((?:.*)(?:postgresql)-)(.{10})(?:.*)"
    ybc_logs_regex_pattern = "((?:.*)(?:log)(?:.*))(\\d{8})-(?:\\d*)\\.(?:.*)"
    retention_days = 10
    k8s_enabled = true
    onprem_enabled = true
    allow_cores_collection = true
    node_check_timeout_sec = 60
    # default N minutes of prom dump to get if no dates for prom dump specified
    default_prom_dump_range = 15
    # default batch duration for the prometheus dump in minutes
    batch_duration_prom_dump_mins = 15
    # Query resolution step width number of seconds.
    step_prom_dump_secs = 60
  }
  # certificate issued would be with expiry of following
  tlsCertificate {
    root.expiryInYears = 4
    server.maxLifetimeInYears = 1
    organizationName = "example.com"
  }

  # External script object used to store script details
  external_script {
    content = null
    params = null
    schedule = null
  }

  query_stats {
    excluded_queries = [
      "SET extra_float_digits = 3",
      "SELECT jsonb_agg(x) FROM (SELECT stats_reset from pg_stat_statements_info) as x"
    ]
    slow_queries {
       limit = 50
       # Descending sort possible values: total_time, max_time, mean_time, rows, calls
       # columns of pg_stat_statements view
       # See https://www.postgresql.org/docs/current/pgstatstatements.html#id-1.11.7.39.6
       order_by = "total_time"
       set_enable_nestloop_off = true
       timeout_secs = 180
       query_length = 16384
    }

    # Different wait times for live queries
    # This is runtime configurable but will take effect only after restart.
    live_queries {
      ws = ${play.ws}
    }
    live_queries.ws.timeout.connection = 10 seconds
    live_queries.ws.timeout.idle = 30 seconds
    live_queries.ws.timeout.request = 30 seconds
    live_queries.ws.ssl.loose.acceptAnyCertificate = true

    # initial and minimum number of threads used by import controller
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 50

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 500
  }

  # Allow http to https redirects for node UI
  node_ui {
    https {
      enabled = true
    }
    ws = ${play.ws}
  }
  node_ui.ws.ssl.loose.acceptAnyCertificate = true
  node_ui.ws.timeout.connection = 10 seconds
  node_ui.ws.timeout.idle = 30 seconds
  node_ui.ws.timeout.request = ${yb.proxy_endpoint_timeout}

  attach_detach {
    enabled = false
  }

  auto_yba_backups {
    num_cloud_retention = 5
  }

  job_scheduler {
    poller_interval = 2m
    instance_record_ttl = 20m
  }

  node_agent {
    connect_timeout = 10 seconds
    poller_interval = 2 minutes
    retention_duration = 30 days
    reinstall_parallelism = 5
    use_ynp_provisioning = false

    client {
        enabled = true
        token_lifetime = 30 minutes
        allow_mix_mode = true
    }

    server {
        install_path = /opt/yugabyte
        port = 9070
    }

    enabler {
        # Set it to 0 to disable the enabler
        scan_interval = 0 minutes
        install_timeout = 30 minutes
        reinstall_cooldown = 24 hours
        universe_installer {
            # initial and minimum number of threads used by universe installer
            core_threads = 1
            # max number of threads we will grow to if needed before starting to reject tasks
            max_threads = 5
            # duration for which thread pool will stay inflated before it shrinks back to core_threads
            thread_ttl = 1 minute
            # capacity of the thread pool queue
            queue_capacity = 100
        }
        node_installer {
            # initial and minimum number of threads used by node installer
            core_threads = 10
            # max number of threads we will grow to if needed before starting to reject tasks
            max_threads = 30
            # duration for which thread pool will stay inflated before it shrinks back to core_threads
            thread_ttl = 1 minute
            # capacity of the thread pool queue
            queue_capacity = 100
        }
    }

    grpc_executor {
        # initial and minimum number of threads used by channel
        core_threads = 10
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 100
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }

    live_node_poller {
        # initial and minimum number of threads used by live node poller
        core_threads = 10
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 100
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }

    dead_node_poller {
        # initial and minimum number of threads used by dead node poller
        core_threads = 4
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 10
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }

    upgrader {
        # initial and minimum number of threads used by upgrader
        core_threads = 10
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 30
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue.
        queue_capacity = 50
    }

    ansible_offloading {
      enabled = false
    }

    preflight_checks {
      # [min, max) version of python on the target host (DB node).
      min_python_version = 2.7
      max_python_version = 3.7
      user = yugabyte
      user_group = yugabyte
      min_prometheus_space_mb = 100
      min_tmp_dir_space_mb = 100
      min_home_dir_space_mb = 100
      min_mount_point_dir_space_mb = 100
      ulimit_core = unlimited
      ulimit_open_files = 1048576
      ulimit_user_processes = 12000
      swappiness = 0
      ssh_timeout = 10
      vm_max_map_count = 262144
    }

    releases {
        # Path to the node-agent releases.
        path = "/opt/yugabyte/node-agent/releases"
    }

    node_action {
        use_java_client = true
    }
  }

  gflags {
    allow_user_override = false
    notify_peer_of_removal_from_cluster = false
    allow_during_prefinalize = false
  }

  helm {
    timeout_secs = 900
    packagePath = ""
  }

  features {
    cert_reload {
      enabled = true
      supportedVersions = ["2.14+"]
    }
  }

  kms {
    refresh_interval = 12 hours
  }

  api {
    backward_compatible_date = ${yb.cloud.enabled}
    mode {
      strict = false
      safe = false
    }
  }

  env_proxy_selector {
    enabled = true
    enabled = ${?ENV_PROXY_SELECTOR_ENABLED}
  }

  test {
    logtofile = false
  }
}

ybc {
  releases {
    stable_version = "2.2.0.0-b9"
    path = "/opt/yugabyte/ybc/releases"
  }
  compatible_db_version = "2.15.0.0-b1"
  docker {
    release = "/opt/yugabyte/ybc/release"
  }
  upgrade {
    scheduler_interval = 2 minute,
    universe_batch_size = 5,
    node_batch_size = 15,
    allow_scheduled_upgrade = true
    force_shutdown = false
    poll_result_tries = 10
    poll_result_sleep_ms = 10000
  }
  k8s {
    enabled = true
  }
  timeout {
    admin_operation_timeout_ms = 120000
    socket_read_timeout_ms = 30000
    operation_timeout_ms = 60000
  }
  provider {
    enabled = true
  }
  universe {
    enabled = true
  }
  client_settings {
    max_unavailable_retries = 10
    wait_each_unavailable_retry_ms = 2000
    max_inbound_msg_size_bytes = 104857600
    deadline_ms = 1200000
    keep_alive_ping_ms = 360000
    keep_alive_ping_timeout_ms = 120000
  }
}

runtime_config {
  included_objects = [
    "yb.external_script"
    "yb.ha.ws"
    "yb.query_stats.live_queries.ws"
    "yb.alert.pagerduty.ws"
    "yb.alert.slack.ws"
    "yb.alert.webhook.ws"
    "yb.metrics.ws"
    "yb.troubleshooting.ws"
    "yb.perf_advisor"
    "yb.runtime_conf_ui.tag_filter"
    "yb.universe.network_load_balancer.custom_health_check_ports"
    "yb.universe.network_load_balancer.custom_health_check_paths"
    "yb.universe.user_tags.enforced_tags"
    "yb.xcluster.sleep_time_before_restore"
  ]
  included_paths = [
    # Do not add new runtime keys here
    # Read https://docs.google.com/document/d/1NAURMNdtOexYnfYN9mOSDChtrP2T4qkRhxsFdah7uwM
  ]
  excluded_paths = [
    # this ws config is in included_objects:
    "yb.query_stats.live_queries.ws.",
    "yb.query_stats.core_threads",
    "yb.query_stats.max_threads",
    "yb.query_stats.queue_capacity",
    "yb.query_stats.thread_ttl",
    "yb.perf_advisor.universe_batch_size",
    "yb.perf_advisor.scheduler_interval_mins",
    "yb.perf_advisor.cleanup.gc_check_interval".
    "yb.releases.path",
    "yb.wellKnownCA.trustStore.javaHomePaths"
  ]
  scope_strictness.enabled = true
  data_validation.enabled = true
}

kamon {
  prometheus {
    embedded-server {
      hostname = "localhost"
      port = 9095
    }
  }
  modules {
    status-page.enabled = false
    process-metrics.enabled = false
  }
}
