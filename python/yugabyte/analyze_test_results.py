#!/usr/bin/env python3
# Copyright (c) YugabyteDB, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
# in compliance with the License.  You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software distributed under the License
# is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
# or implied.  See the License for the specific language governing permissions and limitations
# under the License.

import argparse
import dataclasses
import functools
import json
import logging
import os

from dataclasses import dataclass
from typing import Dict, List, Any, Union, cast, Tuple, Set, Optional
from xml.dom import minidom

from yugabyte import json_util, common_util, file_util, test_descriptor
from yugabyte.test_descriptor import SimpleTestDescriptor, LanguageOfTest


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        '--aggregated-json-test-results',
        help='Aggregated JSON report of test results generated by aggregate_test_reports.py. '
             'Usually named test_results.json.',
        required=True)
    parser.add_argument(
        '--planned-tests',
        help='Spark planned test list produced by run_tests_on_spark.py. Usually named '
             'planned_tests.json.',
        required=True)
    parser.add_argument(
        '--run-tests-on-spark-report',
        help='Full build report produced by run_tests_on_spark.py. Usually named '
             'full_build_report.json.gz.',
        required=True)
    parser.add_argument(
        '--archive-dir',
        help='Directory of the build archive. Usually named "archive" and is located at '
             'a relative path similar to builds/<build_number>/archive under a Jenkins job '
             'directory. If this is not specified, we attempt to derive this from the '
             'values of other arguments.')
    parser.add_argument(
        '--successful-tests-out-path',
        help='Write list of test descriptors of tests that succeeded according to both types of '
             'build reports to the given file.')
    parser.add_argument(
        '--test-list-out-path',
        help='Write the list of test descriptors from both types of reports to this file.')
    parser.add_argument(
        '--analysis-out-path',
        help='Write the analysis to this file as well as stdout.')
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose output')
    return parser.parse_args()


# Report from a single Spark task.
# Look at python/yugabyte/test_data/clang16_debug_centos7_run_tests_on_spark_full_report.json.gz
# for an example, and at run_tests_on_spark.py for the code that produces this report.
SparkTaskReport = Dict[str, Union[str, int, float, List[str]]]


@dataclass
class AnalysisResult:
    num_tests_planned: int = 0
    num_tests_in_junit_xml: int = 0
    num_tests_in_spark: int = 0
    num_tests_did_not_run: int = 0

    num_failed_tests_in_junit_xml: int = 0
    num_failed_tests_in_spark: int = 0

    # Number of tests that failed in both types of reports.
    num_unique_failed_tests: int = 0

    num_dedup_errors_in_junit_xml: int = 0

    # Tests for which there is a discrepancy whether they fail or not.
    num_tests_failed_in_spark_but_not_junit_xml: int = 0
    num_tests_failed_in_junit_xml_but_not_spark: int = 0

    # Tests that are missing one type of report altogether.
    num_tests_without_junit_xml_report: int = 0
    num_tests_without_spark_report: int = 0

    # Total number of unique test descriptors found across any types of reports (union).
    num_unique_test_results: int = 0

    # Total number of unique test descriptors found across both types of reports (intersection).
    num_unique_tests_present_in_both_report_types: int = 0

    # Number of tests that succeeded according to both types of test reports.
    num_successful_tests: int = 0


class SingleBuildAnalyzer:
    # Look at aggregate_test_reports.py for the format of this dictionary.
    test_reports_from_junit_xml: List[Dict[str, Any]]

    # The key is a "test descriptor".
    run_tests_on_spark_report: Dict[str, SparkTaskReport]

    archive_dir: Optional[str]
    successful_tests_out_path: Optional[str]
    test_list_out_path: Optional[str]

    def __init__(self,
                 aggregated_test_results_path: str,
                 planned_tests_path: str,
                 run_tests_on_spark_report_path: str,
                 archive_dir: Optional[str],
                 successful_tests_out_path: Optional[str] = None,
                 test_list_out_path: Optional[str] = None) -> None:
        """
        :param aggregated_test_results_path: Path to the JSON file containing the aggregated test
            results. Look at aggregate_test_reports.py for the format of this dictionary, and at
            python/yugabyte/test_data/clang16_debug_centos7_test_report_from_junit_xml.json.gz for
            an example.
        :param planned_tests_path: Path to the JSON file containing list of tests to be run.
            Example: python/yugabyte/test_data/planned_tests.json
        :param run_tests_on_spark_report_path: Path to the JSON file containing the full build
            report produced by run_tests_on_spark.py. As an example, look at the following file:
            python/yugabyte/test_data/clang16_debug_centos7_run_tests_on_spark_full_report.json.gz
        """
        logging.info("Reading aggregated JUnit XML test results from %s",
                     aggregated_test_results_path)
        self.test_reports_from_junit_xml = cast(
            List[Dict[str, Any]],
            json_util.read_json_file(aggregated_test_results_path)['tests'])
        logging.info("Reading planned Spark test list from %s", planned_tests_path)
        self.planned_tests_list = json_util.read_json_file(planned_tests_path)
        logging.info("Reading full Spark build report from %s", run_tests_on_spark_report_path)
        self.run_tests_on_spark_report = cast(
            Dict[str, SparkTaskReport],
            json_util.read_json_file(run_tests_on_spark_report_path)['tests'])

        if (archive_dir is None and
                os.path.basename(os.path.dirname(aggregated_test_results_path)) == 'archive'):
            archive_dir = os.path.dirname(aggregated_test_results_path)
        self.archive_dir = archive_dir
        self.successful_tests_out_path = successful_tests_out_path
        self.test_list_out_path = test_list_out_path

    @staticmethod
    def deduplicate_junit_test_reports(
            junit_xml_reports: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        FIELDS_THAT_MUST_MATCH = [
            "test_name",
            "class_name",
            "language",
            "log_path",
            "junit_xml_path",
            "extra_error_log_path",
            "fail_tags",
        ]
        ALL_SPECIALLY_HANDLED_FIELDS = set(FIELDS_THAT_MUST_MATCH + [
            "parsing_errors",
            "time",
        ])

        reports_by_key: Dict[Tuple[Any, ...], List[Dict[str, Any]]] = {}
        for report in junit_xml_reports:
            # If one of the fields that must match is missing, we put "null" in place of that field
            # in the key. This happened during local testing for the extra_error_log_path field.
            key = tuple(json.dumps(report.get(field)) for field in FIELDS_THAT_MUST_MATCH)
            if key not in reports_by_key:
                reports_by_key[key] = []
            reports_by_key[key].append(report)

        result: List[Dict[str, Any]] = []

        for report_group in reports_by_key.values():
            all_parsing_errors: List[str] = functools.reduce(
                lambda x, y: x + y,
                [report.get('parsing_errors', []) for report in report_group],
                []
            )
            max_time = max([report['time'] for report in report_group])
            aggregated_report = {}
            for field_name in FIELDS_THAT_MUST_MATCH:
                if field_name in report_group[0]:
                    aggregated_report[field_name] = report_group[0][field_name]
            if all_parsing_errors:
                aggregated_report['parsing_errors'] = all_parsing_errors
            aggregated_report['time'] = max_time

            all_input_keys: Set[str] = functools.reduce(
                lambda x, y: x | y,
                [report.keys() for report in report_group],
                set())
            dedup_failed = False
            for input_key in all_input_keys:
                if input_key in ALL_SPECIALLY_HANDLED_FIELDS:
                    continue
                values = [
                    report[input_key] for report in report_group
                    if input_key in report
                ]
                if len(set(json.dumps(value) for value in values)) == 1:
                    aggregated_report[input_key] = values[0]
                else:
                    logging.error(
                        "Failed to deduplicate field %s: %s. Test reports: %s", input_key, values,
                        ', '.join(json.dumps(report, indent=2) for report in report_group))
                    dedup_failed = True
            if dedup_failed:
                result.extend(junit_xml_reports)
                continue

            result.append(aggregated_report)
        return result

    def analyze(self) -> AnalysisResult:
        result = AnalysisResult()

        # -----------------------------------------------------------------------------------------
        # Analyze test results from the Spark-based test runner
        # -----------------------------------------------------------------------------------------

        # Examples:
        #
        # "com.yugabyte.jedis.TestYBJedis#testTSRangeByTimeInvalidString[1]": {
        #   "artifact_paths": [ ... ],
        #   "elapsed_time_sec": 21.0440514087677,
        #   "error_output_path": "...",
        #   "exit_code": 0,
        #   "language": "Java"
        # }
        #
        # "tests-docdb/doc_operation-test:::DocOperationTest.EarlyFilesFilteredBeforeBigFile": {
        #   "artifact_paths": [ ... ],
        #   "elapsed_time_sec": 4.5080485343933105,
        #   "error_output_path": "...",
        #   "exit_code": 0,
        #   "language": "C++"
        # }

        desc_to_spark_task_report: Dict[SimpleTestDescriptor, SparkTaskReport] = {}

        # TODO: This script does not support multiple test repotitions. Test descriptors are
        #       assumed to be SimpleTestDescriptors with no :::attempt_X suffixes, and
        #       assertions ensure that the name is not unique in the input report.
        #       Even if it is not supported, it should gracefully ignore such tests.
        failed_tests_in_spark: Set[SimpleTestDescriptor] = set()
        for test_desc_str, spark_test_report in self.run_tests_on_spark_report.items():
            test_desc = SimpleTestDescriptor.parse(test_desc_str)
            assert LanguageOfTest.from_str(
                cast(str, spark_test_report['language'])) == test_desc.language
            assert test_desc not in desc_to_spark_task_report
            desc_to_spark_task_report[test_desc] = spark_test_report
            if spark_test_report['exit_code'] != 0:
                failed_tests_in_spark.add(test_desc)

        result.num_tests_in_spark = len(desc_to_spark_task_report)

        # -----------------------------------------------------------------------------------------
        # Analyze test results from the aggregated JSON-formatted report based on JUnit XML files
        # -----------------------------------------------------------------------------------------

        # Example from C++ (based on a JUnit-compatible XML file produced by gtest):
        # {
        #   "value_param": "0",
        #   "file": "../../src/yb/rocksdb/db/log_test.cc",
        #   "line": "634",
        #   "status": "run",
        #   "result": "completed",
        #   "time": 0.0,
        #   "timestamp": "2023-06-26T14:40:22.579",
        #   "test_name": "ReadFourthMiddleBlock/0",
        #   "class_name": "bool/LogTest",
        #   "language": "cxx",
        #   "cxx_rel_test_binary": "tests-rocksdb/log_test",
        #   "log_path": "...",
        #   "junit_xml_path": "...",
        #   "extra_error_log_path": "..."
        # }

        # Example from a Java test:
        # {
        #   "time": 43.452,
        #   "test_name": "testReadPointInReadCommittedIsolation",
        #   "class_name": "org.yb.pgsql.TestPgTransactions",
        #   "language": "java",
        #   "log_path": "...",
        #   "junit_xml_path": "...",
        #   "extra_error_log_path": "..."
        # }

        # The difficulty with this type of a report is that there can be duplicate records for the
        # same test descriptor because of multiple errors produced by the same test.

        # First, collect the list of all reports for each test descriptor.

        desc_to_test_reports_from_junit_xml: Dict[SimpleTestDescriptor, List[Dict[str, Any]]] = {}
        for test_report in self.test_reports_from_junit_xml:
            test_desc = SimpleTestDescriptor(
                language=LanguageOfTest.from_str(test_report['language']),
                cxx_rel_test_binary=test_report.get('cxx_rel_test_binary'),
                class_name=test_report.get('class_name'),
                test_name=test_report.get('test_name'),
            )
            if test_desc not in desc_to_test_reports_from_junit_xml:
                desc_to_test_reports_from_junit_xml[test_desc] = []
            desc_to_test_reports_from_junit_xml[test_desc].append(test_report)

        # Then, for each test descriptor, deduplicate the test reports.

        deduped_junit_reports_dict: Dict[SimpleTestDescriptor, List[Dict[str, Any]]] = {}
        for test_desc, junit_xml_reports in sorted(desc_to_test_reports_from_junit_xml.items()):
            if len(junit_xml_reports) == 1:
                deduped_junit_reports_dict[test_desc] = junit_xml_reports
                continue
            logging.info("Test descriptor %s has %d reports from JUnit XML files:",
                         test_desc, len(junit_xml_reports))
            for i, junit_xml_report in enumerate(junit_xml_reports):
                logging.info("  %d. %s", i + 1, json.dumps(junit_xml_report, indent=2))
            deduped_junit_reports = self.deduplicate_junit_test_reports(junit_xml_reports)
            deduped_junit_reports_dict[test_desc] = deduped_junit_reports
            if len(deduped_junit_reports) == 1:
                logging.info("Successfully deduplicated test reports for descriptor %s "
                             "into a single report: %s", test_desc,
                             json.dumps(deduped_junit_reports[0], indent=2))
            elif len(deduped_junit_reports) < len(junit_xml_reports):
                logging.warning("Partially deduplicated test reports for descriptor %s into "
                                "%d reports: %s", test_desc, len(deduped_junit_reports))
                for dedup_result_report in deduped_junit_reports:
                    logging.info("  Deduplicated report: %s", json.dumps(
                        dedup_result_report, indent=2))
            else:
                logging.error("Failed to deduplicate test reports for descriptor %s", test_desc)
                result.num_dedup_errors_in_junit_xml += 1

            del deduped_junit_reports

        result.num_tests_in_junit_xml = len(deduped_junit_reports_dict)

        # After deduplication, determine test failures in JUnit XML reports.

        failed_tests_in_junit_xml: Set[SimpleTestDescriptor] = set()
        for test_desc, junit_xml_reports in sorted(deduped_junit_reports_dict.items()):
            for junit_xml_report in junit_xml_reports:
                if (junit_xml_report.get('num_errors', 0) > 0 or
                        junit_xml_report.get('num_failures', 0) > 0):
                    failed_tests_in_junit_xml.add(test_desc)

        # Compare the spark planned tests to spark & junit results.
        result.num_tests_planned = len(self.planned_tests_list)
        planned_desc_list = [SimpleTestDescriptor.parse(td_str)
                             for td_str in self.planned_tests_list]
        for test_desc in planned_desc_list:
            spark_task_report = desc_to_spark_task_report.get(test_desc)
            reports_from_junit_xml = deduped_junit_reports_dict.get(test_desc)
            if spark_task_report is None and reports_from_junit_xml is None:
                logging.info("Test descriptor %s has no results", test_desc)
                result.num_tests_did_not_run += 1
                result.num_tests_without_junit_xml_report += 1
                result.num_tests_without_spark_report += 1
            elif reports_from_junit_xml is None:
                logging.info("Test descriptor %s has no reports from JUnit XML files", test_desc)
                result.num_tests_without_junit_xml_report += 1
            elif spark_task_report is None:
                logging.info("Test descriptor %s has no report from Spark", test_desc)
                result.num_tests_without_spark_report += 1

        for test_desc in sorted(failed_tests_in_spark):
            if test_desc not in failed_tests_in_junit_xml:
                logging.info(
                    "Test %s failed in Spark but not in JUnit XML files",
                    test_desc)
                result.num_tests_failed_in_spark_but_not_junit_xml += 1

        for test_desc in sorted(failed_tests_in_junit_xml):
            if test_desc not in failed_tests_in_spark:
                logging.info(
                    "Test %s failed in JUnit XML files but not in Spark",
                    test_desc)
                result.num_tests_failed_in_junit_xml_but_not_spark += 1

        result.num_failed_tests_in_spark = len(failed_tests_in_spark)
        if result.num_failed_tests_in_spark:
            logging.info("Found %d tests that failed in Spark", result.num_failed_tests_in_spark)

        result.num_failed_tests_in_junit_xml = len(failed_tests_in_junit_xml)
        if result.num_failed_tests_in_junit_xml:
            logging.info("Found %d tests that failed in JUnit XML files",
                         result.num_failed_tests_in_junit_xml)

        result.num_unique_failed_tests = len(failed_tests_in_spark | failed_tests_in_junit_xml)
        if result.num_unique_failed_tests:
            logging.info("%d unique failed tests from both types of reports",
                         result.num_unique_failed_tests)

        all_test_descs = (set(desc_to_spark_task_report.keys()) |
                          set(desc_to_test_reports_from_junit_xml.keys()))
        result.num_unique_test_results = len(all_test_descs)
        logging.info("Found %d unique tests total" % result.num_unique_test_results)

        tests_present_in_both = (set(desc_to_spark_task_report.keys()) &
                                 set(desc_to_test_reports_from_junit_xml.keys()))
        result.num_unique_tests_present_in_both_report_types = len(tests_present_in_both)

        successful_tests = tests_present_in_both - failed_tests_in_junit_xml - failed_tests_in_spark
        result.num_successful_tests = len(successful_tests)
        logging.info("Found %d tests successful according to both report types",
                     result.num_successful_tests)

        test_descriptor.write_test_descriptors_to_file(
            self.successful_tests_out_path, successful_tests, 'successful tests')
        test_descriptor.write_test_descriptors_to_file(
            self.test_list_out_path, planned_desc_list, 'all tests')

        return result


def main() -> None:
    args = parse_args()
    common_util.init_logging(verbose=args.verbose)
    result = SingleBuildAnalyzer(
        args.aggregated_json_test_results,
        args.planned_tests,
        args.run_tests_on_spark_report,
        args.archive_dir,
        args.successful_tests_out_path,
        args.test_list_out_path
    ).analyze()

    stats = ''
    for field in dataclasses.fields(result):
        logging.info("%s: %s", field.name, getattr(result, field.name))
        stats += f"{field.name}: {getattr(result, field.name)}\n"

    if args.analysis_out_path:
        logging.info("Writing the analysis stats to %s", args.analysis_out_path)
        file_util.write_file(stats, args.analysis_out_path)


if __name__ == '__main__':
    main()
