#!/usr/bin/env python
from __future__ import unicode_literals

import argparse
import atexit
import json
import logging
import multiprocessing
import os
import re
import resource
import shutil
from distutils.spawn import find_executable
import subprocess
import sys
import time
import traceback
import uuid
import tempfile
import socket
import tarfile
import operator
import string
from datetime import datetime
from signal import SIGABRT, SIGINT, SIGKILL, SIGTERM, SIG_DFL, SIG_IGN, signal
from threading import Thread

# Version-dependent imports
PY_VERSION = sys.version_info[0]
if PY_VERSION < 3:
    import Queue as queue
    from urllib2 import Request, urlopen, URLError, HTTPError
    from urllib import urlencode
    from random import SystemRandom
    _sysrand = SystemRandom()
    PASSWORD_GENNERATOR = _sysrand.choice
else:
    import queue
    from urllib.request import Request, urlopen
    from urllib.error import URLError, HTTPError
    from urllib.parse import urlencode
    import secrets
    PASSWORD_GENNERATOR = secrets.choice

"""
Run `yugabyted` to start a single-node YugabyteDB process. If no options are specified,
`yugabyted` will assume the following default directory tree:

yugabyte
+-- var
    |
    +-- conf
        |   +-- yugabyted.conf
     +-- logs
         |   +-- master & tserver & yugaware
     +-- data
+-- bin
|   |   +-- yugabyted
|   |   +-- yb-master
|   |   +-- yb-tserver
|   |   +-- ...
+-- ui
|   |   +-- bin...
|   |   +-- ...
"""
# OS constants
OS_DETAILS = os.uname()
OS_NAME = OS_DETAILS[0]

# Script constants.
SCRIPT_NAME = os.path.basename(__file__)
YUGABYTE_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
TRUE_CHOICES = ["true", "True", "t", "T", "yes", "Yes", "y", "Y", "1"]
FALSE_CHOICES = ["false", "False", "f", "F", "no", "No", "n", "N", "0"]
BOOL_CHOICES = TRUE_CHOICES + FALSE_CHOICES
FAULT_TOLERANCE_CHOICES = ["zone", "region", "cloud"]
START_FAULT_TOLERANCE_CHOICES = ["none", "zone", "region", "cloud"]
SLACK_LINK = "https://www.yugabyte.com/slack"
COMMUNITY_REWARDS_LINK = "https://www.yugabyte.com/community-rewards/"
HELP_LINK = "https://docs.yugabyte.com/latest/faq/"
YUGABYTED_LINK = "https://docs.yugabyte.com/preview/reference/configuration/yugabyted/"
YUGABYTED_START = "https://docs.yugabyte.com/preview/reference/configuration/yugabyted/#start"
GENERATE_SERVER_CERTS = "https://docs.yugabyte.com/preview/secure/tls-encryption/\
server-certificates/#create-the-server-certificates"
DEFAULT_DEMO_DATABASE = "northwind"
DEFAULT_FAULT_TOLERANCE = "zone"
DEFAULT_START_FAULT_TOLERANCE = "none"
SAMPLE_DATA_LINKS = {
    "retail": "https://docs.yugabyte.com/latest/quick-start/explore-ysql/",
    "chinook": "https://docs.yugabyte.com/latest/sample-data/chinook/",
    "sports": "https://docs.yugabyte.com/latest/sample-data/sportsdb/",
    "northwind": "https://docs.yugabyte.com/latest/sample-data/northwind/"
}
EXIT_SIGNALS = (SIGABRT, SIGINT, SIGTERM)
REQUIRED_LOOPBACK_ADDRESSES = [
    '127\.0\.0\.2',
    '127\.0\.0\.3',
    '127\.0\.0\.4',
    '127\.0\.0\.5',
    '127\.0\.0\.6',
    '127\.0\.0\.7'
]
MAX_PROC = {
    'Linux' : 12000,
    'Darwin' : 2500,
}
PREREQS_ERROR_MSGS = {
    'open_files' :'open files ulimits value set low. Please set soft and hard limits to 1048576.',
    'max_user_processes' :'max user processes ulimits value set low.' \
        ' Please set soft and hard limits to {}.'.format(MAX_PROC[OS_NAME]),
    'transparent_hugepages' :'Transparent hugepages disabled. Please enable transparent_hugepages.',
    'ntp/chrony' :'ntp/chrony package is missing for clock synchronization. For centos 7, ' +
        'we recommend installing either ntp or chrony package and for centos 8, ' +
        'we recommend installing chrony package.',
}
QUICK_START_LINKS = {
    'mac' : 'https://docs.yugabyte.com/preview/quick-start/',
    'linux' : 'https://docs.yugabyte.com/preview/quick-start/linux/',
}
CONFIG_LINK = "https://docs.yugabyte.com/latest/deploy/manual-deployment/system-config"

# Help Message Constants
PREFIX = {
    'yugabyted' : "YugabyteDB command-line interface for creating" +
                    " and configuring YugabyteDB cluster.",
    'start' : "Install YugabyteDB and start a single node cluster.\n\n" +
                "Use --join flag to join other nodes that are part of the same cluster.",
    'stop' : "",
    'destroy' : "",
    'status' : "",
    'version' : "",
    'collect_logs' : "",
    'configure' : "",
    'configure data_placement': "",
    'configure encrypt_at_rest': "",
    'configure admin_operation': "",
    'configure_read_replica' : "",
    'configure_read_replica new': "",
    'configure_read_replica modify': "",
    'configure_read_replica delete': "",
    'connect' : "",
    'connect ycql' : "",
    'connect ysql' : "",
    'demo' : "",
    'demo connect' : "",
    'demo destroy' : "",
    'cert' : "",
    'cert generate_server_certs' : "",
}

USAGE = {
    'yugabyted' : "yugabyted [command] [flags]",
    'start' : "yugabyted start [flags]",
    'stop' : "yugabyted stop [flags]",
    'destroy' : "yugabyted destroy [flags]",
    'status' : "yugabyted status [flags]",
    'version' : "yugabyted version [flags]",
    'collect_logs' : "yugabyted collect_logs [flags]",
    'configure' : "yugabyted configure [command] [flags]",
    'configure data_placement': "yugabyted configure data_placement [flags]",
    'configure encrypt_at_rest': "yugabyted configure encrypt_at_rest [flags]",
    'configure admin_operation' : "yugabyted configure admin_operation [flags]",
    'configure_read_replica' : "yugabyted configure_read_replica [command] [flags]",
    'configure_read_replica new': "yugabyted configure_read_replica new [flags]",
    'configure_read_replica modify': "yugabyted configure_read_replica modify [flags]",
    'configure_read_replica delete' : "yugabyted configure_read_replica delete",
    'connect' : "yugabyted connect [command] [flags]",
    'connect ycql' : "yugabyted connect ycql [flags]",
    'connect ysql' : "yugabyted connect ysql [flags]",
    'demo' : "yugabyted demo [command] [flags]",
    'demo connect' : "yugabyted demo connect [flags]",
    'demo destroy' : "yugabyted demo destroy [flags]",
    'cert' : "yugabyted cert [command] [flags]",
    'cert generate_server_certs' : "yugabyted cert generate_server_certs [flag]",
}

EXAMPLE = {
    'start' : "# Create a single-node local cluster:\n" +
              "yugabyted start\n\n"+
              "# Create a single-node locally and join other nodes " +
              "that are part of the same cluster:\n" +
              "yugabyted start --join=host:port,[host:port]\n\n" +
              "# Create a secure cluster:\n" +
              "yugabyted start --secure --certs_dir=<path_to_certs_dir>\n\n",
    'stop' : "",
    'destroy' : "",
    'status' : "",
    'version' : "",
    'collect_logs' : "",
    'cert' : "# Create node sever certificates:\n" +
             "yugabyted cert generate_server_certs --hostnames=<comma_seperated_hostnames>\n\n",
    'configure_read_replica' : "# Configure a new read replica cluster:\n" +
                  "yugabyted configure_read_replica new --rf=<replication_factor> " +
                  "--data_placement_constraint=<placement_policy_for_rr_cluster>\n\n" +
                  "# Modify an existing read replica cluster:\n" +
                  "yugabyted configure_read_replica modify --rf=<new_replication_factor> " +
                  "--data_placement_constraint=<new_placement_policy_for_rr_cluster>\n\n" +
                  "# Delete an existing read replica cluster:\n" +
                  "yugabyted configure_read_replica delete\n\n",
    "new" : "# Configure a new read replica cluster:\n" +
            "yugabyted configure_read_replica new --rf=<replication_factor> " +
            "--data_placement_constraint=<placement_policy_for_rr_cluster>",
    "modify" : "# Modify an existing read replica cluster:\n" +
               "yugabyted configure_read_replica modify --rf=<new_replication_factor> " +
               "--data_placement_constraint=<new_placement_policy_for_rr_cluster>",
    "delete" : "# Delete an existing read replica cluster:\n" +
               "yugabyted configure_read_replica delete",
    'configure' : "# Configure a multi-zone cluster:\n" +
                  "yugabyted configure data_placement --fault_tolerance=zone\n\n" +
                  "# Enable encryption at rest:\n" +
                  "yugabyted configure encrypt_at_rest --enable\n\n" +
                  "For more examples use 'yugabyted configure [command] -h'\n\n" +
                  "# Execute yb-admin command on the YugabyteDB cluster:\n" +
                  "yugabyted configure admin_operation --command <yb-admin_coomand>",
    'data_placement' : "# Configure a multi-zone cluster:\n" +
                       "yugabyted configure data_placement --fault_tolerance=zone\n\n" +
                       "# Configure a multi-region cluster:\n" +
                       "yugabyted configure data_placement --fault_tolerance=region\n\n" +
                       "# Configure a multi-zone cluster with the specified placement info and " + \
                       "rf:\n" + "yugabyted configure data_placement --fault_tolerance=zone " +
                       "--constraint_value=cloud1.region1.zone1,cloud2.region2.zone2," +
                       "cloud3.region3.zone3 --rf=3\n\n" +
                       "# Configure a multi-zone cluster with the order of preference" \
                       " to place the primary copy of the data:\n" +
                       "yugabyted configure data_placement  --constraint_value=" +
                       "cloud1.region1.zone1:<preference_number>," +
                       "cloud2.region2.zone2:<preference_number>," +
                       "cloud3.region3.zone3:<preference_number>\n\n",

    'encrypt_at_rest' : "# Enable encryption at rest for a cluster:\n" +
                        "yugabyted configure encrypt_at_rest --enable\n\n" +
                        "# Disable encryption at rest for a cluster:\n" +
                        "yugabyted configure encrypt_at_rest --disable\n\n",
    'admin_operation' : "# Execute yb-admin command on the YugabyteDB cluster:\n" +
                         "yugabyted configure admin_operation --command 'get_universe_config'\n\n",
}

EPILOG_COMMON = "Run '{} [command] -h' for help with specific commands.".format(SCRIPT_NAME)

EPILOG_SPECIFIC = {
    'start' : "Use conf file to configure advanced flags. Learn more about advanced flags " +
                "refer to the docs page: {}.\n\n".format(YUGABYTED_START),
    'stop' : "",
    'destroy' : "",
    'status' : "",
    'version' : "",
    'collect_logs' : "",
    'configure' : "",
    'cert' : "",
}

# YugabyteDB configs.
IP_ANY = "0.0.0.0"
IP_LOCALHOST = "127.0.0.1"
DEFAULT_BIND_IP = IP_ANY
DEFAULT_MASTER_RPC_PORT = 7100
DEFAULT_TSERVER_RPC_PORT = 9100
DEFAULT_MASTER_WEBSERVER_PORT = 7000
DEFAULT_TSERVER_WEBSERVER_PORT = 9000
DEFAULT_YSQL_PORT = 5433
DEFAULT_YCQL_PORT = 9042
DEFAULT_WEBSERVER_PORT = 7200
DEFAULT_YUGABYTED_UI_PORT = 15433
DEFAULT_CALLHOME = True
DEFAULT_YSQL_USER = "yugabyte"
DEFAULT_YSQL_PASSWORD = "yugabyte"
DEFAULT_YSQL_DB = "yugabyte"
DEFAULT_CLOUD_PROVIDER = "cloud1"
DEFAULT_CLOUD_REGION = "datacenter1"
DEFAULT_CLOUD_ZONE = "rack1"
YSQL_PASSWORD_LENGTH_WARNING = "Warning: Your 'YSQL_PASSWORD' length is greater than 99 characters.\
Please set 'PGPASSWORD' in environment variables to use 'bin/ysqlsh'."
DEFAULT_YCQL_USER = "cassandra"
DEFAULT_YCQL_PASSWORD = "cassandra"
DEFAULT_YCQL_KEYSPACE = None
VERSION_METADATA_PATH = os.path.join(YUGABYTE_DIR, "version_metadata.json")
YUGABYTE_API_CLIENT_PROGRAMS = {
    "ysql": "ysqlsh",
    "ycql": "ycqlsh",
}
YB_NUM_SHARDS_PER_TSERVER = 1
YSQL_NUM_SHARDS_PER_TSERVER = 1
METRICS_SNAPSHOT_LIST = [
    "handler_latency_yb_tserver_TabletServerService_Read_count",
    "handler_latency_yb_tserver_TabletServerService_Write_count",
    "handler_latency_yb_tserver_TabletServerService_Read_sum",
    "handler_latency_yb_tserver_TabletServerService_Write_sum",
    "disk_usage", "cpu_usage", "node_up"
]

# YugaWare configs. These have their own separate subdirectory to preserve our itest flow.
YUGAWARE_DIR = os.path.join(YUGABYTE_DIR, "ui")
YUGAWARE_BIN_DIR = os.path.join(YUGAWARE_DIR, "bin")
YUGAWARE_CONF = os.path.join(YUGAWARE_DIR, "conf/application.yugabyted.conf")
WEBSERVER_DB = "system_platform"
DEMO_DB_PREFIX = "yb_demo_"

BREW_CONF_FILE = "/usr/local/etc/yugabyted.conf"

ALERT_WARNING = "Warning"
ULIMIT_ERR_CODE = "LOW_ULIMITS"
TS_MASTER_ADDRS_FLAG = "tserver_master_addrs"

start_time_sec = time.time()

# Finds the path where a particular file is present from
# amongst the supplied paths.
def search_file_in_paths(dir_candidates, file_name):
    for candidate in dir_candidates:
        if os.path.exists(os.path.join(candidate, file_name)):
            Output.log("Found directory {} for"
                        " file {}".format(candidate, file_name))
            return os.path.join(candidate, file_name)

    # If post_install.sh script isn't found then don't error out
    # The caller assumes that the environment is dev and skips
    # performing the post installation steps
    if(file_name == "post_install.sh"):
        return None

    # If yugabyted-ui is not found, don't error out
    # Caller should set self.configs.saved_data.get("ui") to false if needed
    if(file_name == "yugabyted-ui"):
        Output.log(
            "Yugabyte {} file not found in paths {}. Please check "
            "the paths.".format(file_name, dir_candidates),
            logging.WARN
        )
        return None

    Output.log_error_and_exit(
        "Yugabyte {} file not found in paths {}. Please check "
        "the paths.".format(file_name, dir_candidates)
    )

# Checks if all given files exist in the given path
def check_files_in_path(path, files):
    has_error = False

    for file in files:
        if not os.path.exists(os.path.join(path, file)):
            has_error = True
            Output.log("{} file not found.".format(os.path.join(path, file)))

    return not has_error

# Finds the path of a particular YB binary
def find_binary_location(binary_name):
    # Default if tar is downloaded
    dir_candidates = [
        os.path.join(YUGABYTE_DIR, "bin")
    ]

    # Development environment
    dir_candidates += [
        os.path.join(YUGABYTE_DIR, "build", "latest", "bin"),
    ]

    # Development environment for UI
    dir_candidates += [
        os.path.join(YUGABYTE_DIR, "build", "latest", "gobin"),
    ]

    return search_file_in_paths(dir_candidates, binary_name)

# Finds the path of the sample data
def find_sample_data_location(data_file):
    # Default if tar is downloaded
    dir_candidates = [
        os.path.join(YUGABYTE_DIR, "share")
    ]

    # Development environment
    dir_candidates += [
        os.path.join(YUGABYTE_DIR, "sample")
    ]

    return search_file_in_paths(dir_candidates, data_file)

# Finds the path of the version_metadata.json file
def find_version_metadata_location(version_file):
    # Default if tar is downloaded
    dir_candidates = [
        os.path.join(YUGABYTE_DIR)
    ]

    # Development environment
    dir_candidates += [
        os.path.join(YUGABYTE_DIR, "build", "latest")
    ]

    return search_file_in_paths(dir_candidates, version_file)

# Creates the Head Title for yugabyted CLI
def get_cli_title():
    title = Output.make_green(Output.make_green("Yugabyted CLI") + ": YugabyteDB command line")
    extra_len = len(Output.make_green(""))
    div_line = "+" + "-" * 98 + "+" + "\n"
    cli_title = div_line
    cli_title += ("| {:^" + str(105 + extra_len) + "} |\n").format(title)
    cli_title += div_line
    return cli_title

class ControlScript(object):
    def __init__(self):
        self.configs = None
        self.processes = {}
        self.stop_callhome = False
        self.alerts = []
        self.script = None
        self.setup_env_init = EnvBasedCredentials()

    # Starts YugabyteDB node.
    def start(self):
        if self.script.is_running():
            Output.print_out("{} is already running!".format(SCRIPT_NAME))
            sys.exit(1)

        Output.print_and_log("Starting {}...".format(SCRIPT_NAME))
        self.set_env_vars()

        if self.configs.temp_data.get("background"):
            # In daemon mode, self.daemonize() forks. The child process then executes
            # normal control flow. The parent process waits for the child process until
            # a status message can be printed to the terminal and then exits within daemonize.
            self.daemonize()
        self.script.write_pid(os.getpid())

        self.set_signals(self.kill_children)
        atexit.register(self.kill_children)
        Output.script_exit_func = self.kill_children

        if self.configs.saved_data.get("read_replica"):
            self.start_rr_process()
        else:
            self.start_processes()

    # Kills all processes related to yugabyted
    def kill_all_procs(self):
        pid = self.script.get_pid()
        if pid:
            pgid = os.getpgid(pid)
            if not pgid:
                Output.log("PGID could not be found for {} process".format(SCRIPT_NAME) +
                    "with PID {}. Is {} running?".format(pid, SCRIPT_NAME))
                return ("No PGID", pid)
            else:
                try:
                    os.killpg(pgid, SIGTERM)
                except OSError as err:
                    return (err, pid)
        self.script.delete_pidfile()
        return (None, pid)

    # Kills currently running yugabyted process if it exists.
    def stop(self, *args):
        (err, pid) = self.kill_all_procs()
        if err:
            Output.log_error_and_exit(
                "Failed to shut down {}: {}. Please check PID in {}".format(
                    SCRIPT_NAME, err, self.script.pidfile))
        elif pid:
            self.script.wait_until_stop(pid)
            Output.print_out("Stopped {} using config {}.".format(SCRIPT_NAME, self.conf_file))
        sys.exit(0)

    # Prints status of YugabyteDB.
    def status(self):
        if len(os.listdir(self.configs.saved_data.get("data_dir"))) != 0:
            Output.print_out(self.get_status_string())
        else:
            Output.print_out("{} is not running.".format(SCRIPT_NAME))

    # Destroy the YugabyteDB cluster.
    def destroy(self):
        (err, pid) = self.kill_all_procs()
        if err:
            Output.log_error_and_exit(
                "Failed to shut down {}: {}. Please check PID in {}".format(
                    SCRIPT_NAME, err, self.script.pidfile))
        elif pid:
            self.script.wait_until_stop(pid)
            Output.print_out("Stopped {} using config {}.".format(SCRIPT_NAME, self.conf_file))
        logpath = self.configs.saved_data.get("log_dir")
        datapath = self.configs.saved_data.get("data_dir")
        gen_certs_dir = self.configs.saved_data.get("gen_certs_dir")
        certs_dir = self.configs.saved_data.get("certs_dir")
        config_path = os.path.dirname(self.conf_file)
        is_config_passed=False
        for arg in sys.argv[1:]:
            if (arg.startswith("--config")):
                is_config_passed=True
                break

        if (self.conf_file == BREW_CONF_FILE):
            Output.print_out("{} destroy is not supported for brew installations.".format(
                SCRIPT_NAME))
            return

        if os.path.isdir(logpath):
            shutil.rmtree(logpath)
            Output.print_out("Deleted logs at {}.".format(logpath))

        if os.path.isdir(datapath):
            shutil.rmtree(datapath)
            Output.print_out("Deleted data at {}.".format(datapath))

        if os.path.isdir(gen_certs_dir):
            shutil.rmtree(gen_certs_dir)
            Output.print_out("Deleted generated certs at {}.".format(gen_certs_dir))

        if os.path.isdir(certs_dir):
            shutil.rmtree(certs_dir)
            Output.print_out("Deleted certs at {}.".format(certs_dir))

        if is_config_passed:
            if os.path.exists(self.conf_file):
                os.remove(self.conf_file)
                Output.print_out("Deleted conf file at {}.".format(self.conf_file))
        else:
            if os.path.isdir(config_path):
                shutil.rmtree(config_path)
                Output.print_out("Deleted conf at {}.".format(config_path))

        sys.exit(0)

    # Prints YugabyteDB version.
    def version(self):
        VERSION_METADATA_PATH = find_version_metadata_location("version_metadata.json")
        print(VERSION_METADATA_PATH)
        with open(VERSION_METADATA_PATH) as metadata:
            data = json.load(metadata)
            title = "Version".format(SCRIPT_NAME)
            output = "\n" + "-" * 70 + "\n"
            output += ("| {:^66} |\n").format(title)
            output += "-" * 70 + "\n"
            build = data.get("build_number")
            try:
                version = "{}-b{}".format(data.get("version_number"), int(build))
            except ValueError as e:
                version = "{} ({})".format(data.get("version_number"), build)
            for k, v in [
                    ("Version", version),
                    ("Build Time", data.get("build_timestamp")),
                    ("Build Hash", data.get("git_hash"))]:
                output_k = Output.make_yellow(k)
                extra_len = len(Output.make_yellow(""))
                output += ("| {:" + str(15 + extra_len) + "}: {:<49} |\n").format(output_k, v)
            output += "-" * 70 + "\n"
            Output.print_out(output)

    # Starts an interactive YSQL shell.
    def connect_ysql(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{} is not running. Cannot connect to YSQL.".format(SCRIPT_NAME))
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        is_password_passed=False
        for arg in sys.argv[1:]:
            if arg.startswith("--password"):
                is_password_passed = True
                break

        if self.configs.saved_data.get("secure") and is_password_passed:
            ysql_proxy.connect_with_password()
        else:
            ysql_proxy.connect_without_password()

    # Starts an interactive YCQL shell.
    def connect_ycql(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{} is not running. Cannot connect to YCQL.".format(SCRIPT_NAME))
        if self.configs.saved_data.get("secure"):
            self.setup_env_init.setup_cert_file_path(self.configs.
                            saved_data.get("ca_cert_file_path"))
        ycql_proxy = YcqlProxy(ip=self.advertise_ip(),
                        port=self.configs.saved_data.get("ycql_port"),
                        secure=self.configs.saved_data.get("secure"))
        is_password_passed=False
        for arg in sys.argv[1:]:
            if arg.startswith("--password"):
                is_password_passed = True
                break

        if self.configs.saved_data.get("secure") and is_password_passed:
            ycql_proxy.connect_with_password()
        else:
            ycql_proxy.connect_without_password()

    # Creates demo database and starts an interactive shell into it. Destroys the sample database
    # after shell quits.
    def demo(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{0} is not running. Please run `{0} start` before starting a demo.".format(
                    SCRIPT_NAME))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use demo commands " +
                                      "from a read replica node.")

        db_name = DEMO_DB_PREFIX + self.configs.temp_data.get("demo_db")
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        if ysql_proxy.db_exists(db_name):
            Output.log_error_and_exit(
                "Demo is already running. Concurrent demos are currently unsupported.")
        # TODO: Race condition currently exists when running demo too close to each other. This
        # will be solved when concurrent isolated demos are implemented.
        Output.print_out("Now creating demo database")
        self.create_demo()

        # Ignore kill SIGINT to match normal ysqlsh and psql behavior.
        signal(SIGINT, SIG_IGN)
        signal(SIGABRT, self.destroy_demo)
        signal(SIGTERM, self.destroy_demo)
        atexit.register(self.destroy_demo)
        self.connect_demo()
        self.set_signals(SIG_DFL)

    # Create target demo database if it does not exist.
    def create_demo(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{0} is not running. Please run `{0} start` before starting a demo.".format(
                    SCRIPT_NAME))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use demo commands " +
                                      "from a read replica node.")

        demo_db = self.configs.temp_data.get("demo_db")
        db_name = DEMO_DB_PREFIX + demo_db
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        if ysql_proxy.db_exists(db_name):
            Output.print_out("Demo database {} already exists.".format(demo_db))
            return

        Output.print_out(
            "Initializing {} demo database. This may take up to a minute...".format(demo_db))
        # Create demo database.
        Output.log("Creating database {}...".format(db_name))
        ysql_proxy.create_db(db_name)

        # Populate demo database.
        Output.log("Populating {} with sample data...".format(db_name))
        files = []
        for name in Configs.get_demo_info()[demo_db]["files"]:
            files.append(os.path.join(find_sample_data_location(name)))
        ysql_proxy.load_files(files, db=db_name)

        msg = "Successfully loaded sample database!"
        Output.print_and_log(msg)

    # Run YSQL shell in target demo database.
    def connect_demo(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{0} is not running. Please run `{0} start` before starting a demo.".format(
                    SCRIPT_NAME))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use demo commands " +
                                      "from a read replica node.")

        demo_db = self.configs.temp_data.get("demo_db")
        db_name = DEMO_DB_PREFIX + demo_db
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        if not ysql_proxy.db_exists(db_name):
            self.create_demo()

        # Ignore kill SIGINT to match normal ysqlsh and psql behavior.
        signal(SIGINT, SIG_IGN)
        website = Output.make_underline(SAMPLE_DATA_LINKS[demo_db])
        Output.print_out(Configs.get_demo_info()[demo_db]["examples"])
        Output.print_out(
            "For more, go to {}\n".format(website)
        )
        ysql_proxy.connect_with_password(db=db_name)

    # Destroy target demo database if it exists.
    def destroy_demo(self, signum=None, frame=None):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{0} is not running. Please run `{0} start` before starting a demo.".format(
                    SCRIPT_NAME))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use demo commands " +
                                      "from a read replica node.")

        demo_db = self.configs.temp_data.get("demo_db")
        db_name = DEMO_DB_PREFIX + demo_db
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        if ysql_proxy.db_exists(db_name):
            ysql_proxy.drop_db(db_name)
        msg = "Deleted demo database {}.".format(demo_db)
        Output.print_and_log(msg)

    def collect_logs(self):
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))
        logpath = self.configs.saved_data.get("log_dir")
        collect_at_dir=self.configs.temp_data.get("collect_at_dir")
        if not os.path.exists(logpath):
            Output.print_and_log("No logs directory at {}".format(logpath))
            return
        tmpprefix = "yugabyted-{}s.tar.gz".format(datetime.now().strftime("%Y-%m-%d-%Hh%Mm%S.%f"))
        tarpath = os.path.join(collect_at_dir, tmpprefix)
        with tarfile.open(name=tarpath, mode='w:gz', dereference=True) as archive:
            for item in os.listdir(logpath):
                item_path = os.path.join(logpath, item)
                arcname = os.path.relpath(item_path, logpath)
                archive.add(item_path, arcname=arcname)
        status_details = [
                (Output.make_yellow("Status"),"Logs collected successfully.")
        ]
        if self.configs.temp_data.get("collect_logs_stdout"):
            status_details = [
                (Output.make_yellow("Status"), "Logs collected successfully." + \
                    "Find the displayed logs below.")
            ]
            Output.log("Logs are packaged into {}".format(tarpath))
            if tarfile.is_tarfile(tarpath):
                with open(tarpath, 'rb') as tar_fd:
                    if sys.version_info[0] == 3:
                        sys.stdout.buffer.write(tar_fd.read())
                    else:
                        sys.stdout.write(tar_fd.read())
        display_msg= "Collected logs can be found at: {}".format(tarpath)
        status_details.extend([
                (Output.make_yellow("Collected logs path"), display_msg)
        ])
        Output.print_and_log(self.get_status_string_common(status_details))

    # Configuring the primary cluster data placement policy
    def configure_data_placement(self):
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use `yugabyted " +
                                      "configure data_placement` command from a read replica " +
                                      "node. Please use `yugabyted configure_read_replica " +
                                      "[new/modify/delete]` instead.")

        fault_tolerance = self.configs.temp_data.get("fault_tolerance")
        replication_factor = self.configs.temp_data.get("replication_factor")

        if fault_tolerance == "cloud":
            Output.log_error_and_exit("Cloud based fault tolerance is not supported yet.")

        current_masters_with_port_list = self.get_all_masters()
        current_masters_without_port_list = [ master.split(':')[0] for
                                             master in current_masters_with_port_list ]
        leader_master = self.get_leader_master().split(':')[0]
        master_addr = "{}:{}".format(leader_master,
                                        self.configs.saved_data.get("master_rpc_port"))

        leader_master_http_endpoint = "{}:{}".format(leader_master,
                        self.configs.saved_data.get("master_webserver_port"))
        placement_uuid = self.configs.saved_data.get("placement_uuid")

        all_tserver_info = self.get_all_tserver_info(leader_master_http_endpoint)
        placement_locations = self.get_all_nodes_locations(all_tserver_info, placement_uuid)

        if len(placement_locations) < int(replication_factor):
            Output.print_and_log(msg = Output.make_red("ERROR") + ": not enough nodes to set-up " +
                                "a RF-{} cluster".format(replication_factor), level=logging.ERROR)
            sys.exit(1)

        new_masters = self.get_new_valid_masters(current_masters_without_port_list,
                leader_master, placement_locations)

        masters_to_remove = [ master for master in current_masters_without_port_list
                             if master not in new_masters ]
        masters_to_add = [ master for master in new_masters
                          if master not in current_masters_without_port_list ]

        if not masters_to_remove and not masters_to_add:
            Output.log("Desired fault tolerance is already setup. No Masters to move.")
        else :
            track_masters_to_remove = masters_to_remove[:]
            track_masters_to_add = masters_to_add[:]
            if masters_to_remove and masters_to_add:
                for i in range(len(masters_to_remove)):
                    self.replace_master(master_addr, masters_to_remove[i], masters_to_add[i])
                    track_masters_to_remove.remove(masters_to_remove[i])
                    track_masters_to_add.remove(masters_to_add[i])

            if len(track_masters_to_add) != 0:
                for master in track_masters_to_add:
                    self.add_master_for_data_placement(master_addr, master)

            if len(track_masters_to_remove) != 0:
                for master in track_masters_to_remove:
                    self.remove_master_for_data_placement(master_addr, master)

        current_masters_with_port_list = self.get_all_masters()
        if self.configs.temp_data.get("constraint_value"):
            constraint_value = self.configs.temp_data.get("constraint_value")
            placement_info,priority_info = self.parse_constraint_value(constraint_value)
        else:
            placement_info = []
            priority_info = []
            for master in current_masters_with_port_list:
                placement_info.append(placement_locations[master.split(":")[0]])
            placement_info = ",".join(placement_info)

        if not self.is_placement_constraint_valid_length(placement_info):
            Output.log_error_and_exit(Output.make_red("FAILED") + ": Invalid number of " +
                                      "placement constraint specified.")

        if not self.is_placement_constraint_valid_values(placement_locations, placement_info):
            Output.log_error_and_exit(Output.make_red("FAILED") + ": Invalid values provided " +
                                      "for placement constraint.")

        master_addresses = ",".join(current_masters_with_port_list)

        if not YBAdminProxy.modify_placement_info(master_addresses, placement_info,
                                                  placement_uuid, replication_factor):
            Output.log_error_and_exit("FAILED: Setting of placement info of masters")
        else:
            Output.log("Configure step was successful.")

        if priority_info:
            priority_info = list(set(priority_info))
            if not YBAdminProxy.set_preferred_zones(master_addresses, priority_info):
                Output.log_error_and_exit(Output.make_red("FAILED") +
                                          ": Unsuccessful in setting preference for zones")
            else:
                Output.log("Successful in setting preference for zones.")

        status_details = self.get_configure_status_details(placement_locations,
                                                           new_masters, priority_info)

        Output.print_out(self.get_status_string_common(status_details[0], status_details[1]))

    # Generate the node server certificates
    def cert_generate_server_certs(self):
        if self.check_openssl():
            Output.log_error_and_exit(Output.make_red("Error") + ": openssl not installed. " +
                "Can't create certificates.")

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use `yugabyted cert " +
                                      "generate_server_certs` command from a read replica node. " +
                                      "Please use a primary cluster node to generate certs.")

        gen_certs_dir = self.configs.saved_data.get("gen_certs_dir")
        root_certs_dir = os.path.join(gen_certs_dir, "root-ca")

        status_details = []
        if not self.generate_ca_certs(root_certs_dir=root_certs_dir):
            status_details = [
                (Output.make_yellow("Status"), "Cert generation failed. Please check the logs."),
            ]
            Output.print_out(self.get_status_string_common(status_details))
            sys.exit(1)

        hostnames = self.configs.temp_data.get("hostnames")
        hostnames = hostnames.split(',')

        generated_certs_hostnames = self.generate_node_server_certs(hostnames=hostnames,
                                                                    gen_certs_dir=gen_certs_dir)
        if len(generated_certs_hostnames):
            status_details = [
                (Output.make_yellow("Status"), "Server nodes certs generation for hostnames " +
                                "{} successful.".format(",".join(generated_certs_hostnames))),
                (Output.make_yellow("Server node certs path"),
                        "Certs can be found at: {}".format(gen_certs_dir+"/<hostname>")),
            ]
        else:
            status_details = [
                (Output.make_yellow("Status"), "No certs generated. All hostnames already have " +
                                               "certs generated."),
                (Output.make_yellow("Server node certs path"),
                        "Certs can be found at: {}".format(gen_certs_dir+"/<hostname>")),
            ]

        Output.print_out(self.get_status_string_common(status_details))

    # Generate the root certificates
    def generate_ca_certs(self, root_certs_dir):
        generate_certs = False

        if os.path.isdir(root_certs_dir):
            if self.check_root_cert_files(root_certs_dir):
                Output.log("Found an existing root certs directory {}.".format(root_certs_dir) +
                    " Found appropriate files. Using the existing root certs.")
            else:
                Output.print_and_log(Output.make_yellow(Output.ANIMATION_WARNING + " WARNING:") +
                    " Found an existing root certs directory {}.".format(root_certs_dir) +
                    " Appropriate files not found. Removing these and creating new root certs." +
                    " Please recreate any server certs created with previous root certs.")
                shutil.rmtree(root_certs_dir)

                generate_certs = True
        else:
            generate_certs = True

        if generate_certs:
            Output.log("Creating root certs...")
            if not os.path.isdir(root_certs_dir):
                os.makedirs(root_certs_dir)

            status = OpenSSLProxy.generate_root_ca_certs(root_certs_dir=root_certs_dir)

            if not status:
                return False

        return True

    # Generate node server certififcates
    def generate_node_server_certs(self, hostnames, gen_certs_dir):
        root_certs_dir = os.path.join(gen_certs_dir, "root-ca")

        existing_certs_hostnames = []
        generated_certs_hostnames = []
        # Generate certs for each server node
        for hostname in hostnames:
            # Check whether the hostname is already present in the certs database or not.
            if self.check_hostname_in_root_ca_database(hostname=hostname,
                                                       root_certs_dir=root_certs_dir):
                existing_certs_hostnames.append(hostname)
                continue

            # Check whether the hostname cert folder is present or not
            node_certs_dir = os.path.join(gen_certs_dir, hostname)
            if os.path.isdir(node_certs_dir):
                Output.log("Found an existing cert directory {} for ".format(node_certs_dir) +
                    "hostname {}. But no data entry found in ".format(hostname) +
                    "root-ca certs database. Removing...")
                shutil.rmtree(node_certs_dir)

            status = OpenSSLProxy.generate_node_server_certs(root_certs_dir=root_certs_dir,
                hostname=hostname, server_cert_dir=node_certs_dir)

            if not status:
                status_details = [
                    (Output.make_yellow("Status"), "Server node cert generation failed for " +
                        "hostname {}. Please check the logs.".format(hostname)),
                ]
                Output.print_out(self.get_status_string_common(status_details))
                exit(1)
            else:
                generated_certs_hostnames.append(hostname)

        if len(existing_certs_hostnames):
            Output.print_and_log(Output.make_yellow(Output.ANIMATION_WARNING + " WARNING:") +
                " Hostnames {} ".format(Output.make_green(",".join(existing_certs_hostnames))) +
                "already have their certifcates generated according to root-ca certs database " +
                "index. Please use the previously generated certs. Skipping these hostnames....")


        return generated_certs_hostnames

    # Check if all root certificate files are present in a path or not
    def check_root_cert_files(self, path):
        root_certs_files = ["ca.crt", "ca.key", "ca.conf", "index.txt",
            "index.txt.attr", "serial.txt"]

        return check_files_in_path(path, root_certs_files)

    # Check if a hostname is present in the root-ca database
    def check_hostname_in_root_ca_database(self, hostname, root_certs_dir):
        root_ca_database_file = os.path.join(root_certs_dir, "index.txt")

        with open(root_ca_database_file, 'r') as certs_database_file:
            certs_database = certs_database_file.read()
            if hostname in certs_database:
                return True

        return False

    # Use the masters to check if the masters has the key with <key_id>.
    def is_key_in_all_masters(self, key_id):
        masters = ",".join(self.get_all_masters())

        # IPs of the masters which are currently in the cluster
        master_ips = [master.split(':')[0] for master in masters.split(',')]

        # Masters which have the key with id as key_id
        masters_with_key = YBAdminProxy.check_key_in_masters(master_addrs=masters, key_id=key_id)
        for i, master in enumerate(masters_with_key):
            master = master.split(':')[0].split()[1]
            masters_with_key[i] = master

        ret = True
        if len(masters_with_key) < len(master_ips):
            ret = False

        for master in master_ips:
            if master not in masters_with_key:
                ret = False

        return ret

    # Use the masters to check if encryption at rest is enabled.
    def is_encryption_at_rest_enabled(self):
        masters = self.configs.saved_data.get("current_masters")
        result = YBAdminProxy.check_encryption(master_addrs=masters)

        if result is None:
            return None

        if "ENABLED" in result:
            return True
        else:
            return False

    # Configuring encryption at rest
    def configure_encrypt_at_rest(self):
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))

        if self.configs.saved_data.get("read_replica"):
            Output.log_error_and_exit(Output.make_red("ERROR") + ": Cannot use `yugabyted " +
                                      "configure encrypt_at_rest` command from a read replica " +
                                      "node. Please use a primary cluster node to enable EAR.")

        status_details = []
        has_errors = False

        # Enable encryption at rest.
        if self.configs.temp_data.get("enable_encrypt_at_rest"):
            Output.log("Trying to enable encryption at rest.")

            # Check whether the encryption at rest is already enabled.
            encryption_enabled = self.is_encryption_at_rest_enabled()
            if encryption_enabled is None:
                Output.log_error_and_exit("Error checking status of encryption-at-rest.")
            elif encryption_enabled:
                Output.log("Encryption at rest already enabled.")
                has_errors = True
                status_details = [
                    (Output.make_yellow("Status"), "Encryption at rest already enabled.")
                ]

            # Setting the key directory and name
            key_dir = os.path.join(self.configs.saved_data.get("data_dir"), "encrypt-keys")
            keyname = "encrypt_at_rest"

            if not has_errors:
                # Check openssl is installed in the machine or not.
                if self.check_openssl():
                    Output.log_error_and_exit(Output.make_red("Error") + ": openssl not " +
                        "installed. Can't create universe key for encryption.")

                # Generate the universe key.
                status = OpenSSLProxy.generate_key(key_dir=key_dir, keyname=keyname)

                if not status:
                    Output.log(Output.make_red("Error") + ": Universe key generation failed for " +
                            "encryption at rest.")
                    has_errors = True
                    status_details = [
                        (Output.make_yellow("Status"), "Universe key generation failed for " +
                            "enabling encryption at rest. Please check the logs.")
                    ]
                else:
                    masters = ",".join(self.get_all_masters())
                    key_id = str(uuid.uuid4())
                    key_path = key_dir + "/" + keyname + ".key"

                    # Copy the generated universe key to all the masters
                    if not has_errors and not YBAdminProxy.copy_key_to_masters(
                            master_addrs=masters, key_id=key_id, key_path=key_path):
                        Output.log(Output.make_red("Error") + ": cannot copy the " +
                            "generated key to the masters.")
                        has_errors = True
                        status_details = [
                            (Output.make_yellow("Status"), "Encryption at rest failed. " +
                                "Please check the logs."),
                        ]

                    # Check whether all masters have the key.
                    if not has_errors:
                        if not self.is_key_in_all_masters(key_id=key_id):
                            Output.log(Output.make_red("Error") + ": Universe key " +
                                "{} not found in all masters.".format(key_id))
                            has_errors = True
                            status_details = [
                                (Output.make_yellow("Status"), "Encryption at rest failed. " +
                                    "Please check the logs."),
                            ]
                        else:
                            Output.log("Copied key {} to all the masters".format(key_id))

                    # Enable encryption at rest.
                    if not has_errors and not YBAdminProxy.enable_encryption_using_key(
                            master_addrs=masters, key_id=key_id):
                        Output.log(Output.make_red("Error") + ": cannot enable " +
                            "encryption at rest.")
                        has_errors = True
                        status_details = [
                            (Output.make_yellow("Status"), "Encryption at rest failed. " +
                                "Please check the logs."),
                        ]

                    # Check whether the encryption at rest has been enabled or not.
                    if not has_errors:
                        encryption_enabled = self.is_encryption_at_rest_enabled()
                        if encryption_enabled is None:
                            Output.log_error_and_exit("Error checking status of " +
                                "encryption-at-rest.")
                        elif not encryption_enabled:
                            Output.log(Output.make_red("Error") + ": cannot enable " +
                                "encryption at rest.")
                            has_errors = True
                            status_details = [
                                (Output.make_yellow("Status"), "Encryption at rest failed. " +
                                    "Please check the logs."),
                            ]
                        else:
                            out = "Encryption at rest enabled with key: {}".format(key_id)
                            Output.log(out)
                            status_details = [
                                (Output.make_yellow("Status"), out),
                            ]

            Output.log("Deleting the universe key generated for encryption at rest.")
            if os.path.isdir(key_dir):
                shutil.rmtree(key_dir)
        # Disabling encryption at rest
        else:
            Output.log("Trying to disable encryption at rest.")

            # Check whether encryption at rest is already disabled.
            encryption_enabled = self.is_encryption_at_rest_enabled()
            if encryption_enabled is None:
                Output.log_error_and_exit("Error checking status of encryption-at-rest.")
            elif not encryption_enabled:
                Output.log("Encryption at rest is already disabled.")
                has_errors = True
                status_details = [
                    (Output.make_yellow("Status"), "Encryption at rest is already disabled.")
                ]

            masters = ",".join(self.get_all_masters())

            # Disable encryption at rest
            if not has_errors and not YBAdminProxy.disable_encryption(master_addrs=masters):
                Output.log(Output.make_red("Error") + ": cannot disable encryption at rest.")
                has_errors = True
                status_details = [
                    (Output.make_yellow("Status"), "Disabling encryption at rest failed. " +
                        "Please check the logs."),
                ]

            # Check whether encryption at rest has been disabled or not.
            if not has_errors:
                encryption_enabled = self.is_encryption_at_rest_enabled()
                if encryption_enabled is None:
                    Output.log_error_and_exit("Error checking status of encryption-at-rest.")
                elif encryption_enabled:
                    Output.log(Output.make_red("Error") + ": cannot disable encryption at rest.")
                    has_errors = True
                    status_details = [
                        (Output.make_yellow("Status"), "Disabling encryption at rest failed. " +
                            "Please check the logs."),
                    ]
                else:
                    out = "Encryption at rest disabled."
                    Output.log(out)
                    status_details = [
                        (Output.make_yellow("Status"), out),
                    ]

        Output.print_out(self.get_status_string_common(status_details))

    # handle the admin operations passthrough to yb-admin command.
    def configure_admin_operation(self, timeout=10):

        master_addrs = self.configs.temp_data.get("admin_operation_master_addresses")
        if not master_addrs:
            master_addrs = ",".join(self.get_all_masters())
        command = self.configs.temp_data.get("admin_command")
        command_args = command.split(" ")
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs]
        for cmd_var in command_args:
            cmd += [cmd_var]

        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)

        status_details = {}
        status_details = [(Output.make_yellow("Admin operation"),
                           "Find the response below."),]
        Output.print_out(self.get_status_string_common(status_details))
        if 0 == ret_code:
          Output.print_out(out)
        else:
          Output.log(err)
          Output.print_out(Output.make_red(err))

    # Checks if the read replica cluster has been already configured or not.
    def is_read_replica_configured(self, cluster_config):
        if cluster_config and "replicationInfo" in cluster_config:
            if "readReplicas" in cluster_config["replicationInfo"]:
                return True

        return False

    # Configuring the read replica cluster data placement policy
    def configure_read_replica_new(self):
        # Check if a node is running or not
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))

        # Check if read replica cluster has been already configured?
        current_masters_with_port_list = self.get_all_masters()
        master_addresses = ",".join(current_masters_with_port_list)
        cluster_config = YBAdminProxy.get_cluster_config(master_addresses)
        if self.is_read_replica_configured(cluster_config):
            Output.print_and_log(Output.make_red("ERROR") + ": Read replica cluster has already " +
                                 "been configured. If you want to change the configuration " +
                                 "please use 'yugabyted configure_read_replica modify' command.")
            sys.exit(1)

        leader_master = self.get_leader_master().split(':')[0]
        leader_master_http_endpoint = "{}:{}".format(leader_master,
                        self.configs.saved_data.get("master_webserver_port"))

        all_tserver_info = self.get_all_tserver_info(leader_master_http_endpoint)

        placement_uuid = self.configs.saved_data.get("placement_uuid")
        # Use read replica placement UUID if yugabyted is not running in a read replica node.
        if not self.configs.saved_data.get("read_replica"):
            placement_uuid = [uuid for uuid in list(all_tserver_info.keys())
                              if uuid!=placement_uuid][0]

        placement_locations = self.get_all_nodes_locations(all_tserver_info, placement_uuid)

        rf_and_placement_policy_list = self.get_rr_rf_and_placement_constraint(placement_locations)
        rr_replication_factor = rf_and_placement_policy_list[0]
        rr_placement_constraint = rf_and_placement_policy_list[1]

        status_details = list()
        if not YBAdminProxy.set_read_replica_placement(master_addresses, rr_placement_constraint,
                                                  placement_uuid, rr_replication_factor):
            display_msg = "FAILED: Setting of cluster config of read replica cluster."
            status_details.extend([
                (Output.make_yellow("Status"), display_msg)
            ])
        else:
            display_msg = [
                "Configuration successful.",
                "The primary cluster will begin asynchronous replication with " +
                "the read replica cluster.",
            ]
            status_details.extend([
                (Output.make_yellow("Status"), display_msg[0]),
                (Output.make_yellow(""), display_msg[1]),
            ])

        Output.print_out(self.get_status_string_common(status_details))

    # Configuring the read replica cluster data placement policy
    def configure_read_replica_modify(self):
        # Check if a node is running or not
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))

        # Check if read replica cluster has been already configured?
        current_masters_with_port_list = self.get_all_masters()
        master_addresses = ",".join(current_masters_with_port_list)
        cluster_config = YBAdminProxy.get_cluster_config(master_addresses)
        if not self.is_read_replica_configured(cluster_config):
            Output.print_and_log(Output.make_red("ERROR") + ": Read replica cluster has not " +
                                 "been configured. Please set the read replica cluster " +
                                 "configuration using 'yugabyted configure_read_replica new' " +
                                 "command before modifying it.")
            sys.exit(1)

        leader_master = self.get_leader_master().split(':')[0]
        leader_master_http_endpoint = "{}:{}".format(leader_master,
                        self.configs.saved_data.get("master_webserver_port"))

        all_tserver_info = self.get_all_tserver_info(leader_master_http_endpoint)

        placement_uuid = self.configs.saved_data.get("placement_uuid")
        # Use read replica placement UUID if yugabyted is not running in a read replica node.
        if not self.configs.saved_data.get("read_replica"):
            placement_uuid = [uuid for uuid in list(all_tserver_info.keys())
                              if uuid!=placement_uuid][0]

        placement_locations = self.get_all_nodes_locations(all_tserver_info, placement_uuid)

        self.validate_new_config_for_rr_modify(cluster_config)

        rf_and_placement_policy_list = self.get_rr_rf_and_placement_constraint(placement_locations)
        rr_replication_factor = rf_and_placement_policy_list[0]
        rr_placement_constraint = rf_and_placement_policy_list[1]

        status_details = list()
        if not YBAdminProxy.modify_read_replica_placement(master_addresses, rr_placement_constraint,
                                                  placement_uuid, rr_replication_factor):
            display_msg = "FAILED: Modifying of cluster config of read replica cluster."
            status_details.extend([
                (Output.make_yellow("Status"), display_msg)
            ])
        else:
            display_msg = [
                "Configuration successful.",
                "The primary cluster will begin asynchronous replication with " +
                "the read replica cluster.",
            ]
            status_details.extend([
                (Output.make_yellow("Status"), display_msg[0]),
                (Output.make_yellow(""), display_msg[1]),
            ])

        Output.print_out(self.get_status_string_common(status_details))

    # Configuring the read replica cluster data placement policy
    def configure_read_replica_delete(self):
        # Check if a node is running or not
        if not self.script.is_running():
            Output.log_error_and_exit(Output.make_red("ERROR") + ": No YugabyteDB node " +
                "is running in the data_dir {}".format(self.configs.saved_data.get("data_dir")))

        # Check if read replica cluster has been already configured?
        current_masters_with_port_list = self.get_all_masters()
        master_addresses = ",".join(current_masters_with_port_list)
        cluster_config = YBAdminProxy.get_cluster_config(master_addresses)
        if not self.is_read_replica_configured(cluster_config):
            Output.print_and_log(Output.make_red("ERROR") + ": Read replica cluster has not " +
                                 "been configured.")
            sys.exit(1)

        status_details = list()
        if not YBAdminProxy.delete_read_replica_placement(master_addresses):
            display_msg = "FAILED: Deletion of read replica cofiguration."
            status_details.extend([
                (Output.make_yellow("Status"), display_msg)
            ])
        else:
            status_details.extend([
                    (Output.make_yellow("Status"), "Configure delete step was successful."),
            ])

        Output.print_out(self.get_status_string_common(status_details))

    # Check that `openssl` binary is in our PATH
    # This actually returns true when it is _not_ present!
    def check_openssl(self):
        return find_executable('openssl') == None

    # Transparent Hugepage check.  Not all distros have this enabled so we need to check for
    # /sys/kernel/mm/transparent_hugepage before checking enabled
    def linux_transparent_hugepage_check(self):
        thp_dir = "/sys/kernel/mm/transparent_hugepage"
        thp_mode = "disabled"
        if os.path.exists(thp_dir):
            with open(os.path.join(thp_dir, 'enabled'), 'r') as thp_enabled:
                thp_enabled = thp_enabled.read()
                thp_mode = re.search("\[(.*)\]", thp_enabled).group(1)
        return thp_mode == 'always'

    # All Pre-requisites check for Linux
    def linux_prereqs_check(self):
        prereqs_failed = set()
        prereqs_warn = set()
        prereqs_failed_flag = False
        prereqs_warn_flag = False

        if not self.linux_transparent_hugepage_check():
            prereqs_warn.add('transparent_hugepages')
            prereqs_warn_flag = True

        ntp_check = find_executable('ntpd')
        chrony_check = find_executable('chronyd')
        if not ntp_check and not chrony_check:
            prereqs_warn.add('ntp/chrony')
            prereqs_warn_flag = True

        return (prereqs_failed_flag, prereqs_failed, prereqs_warn_flag, prereqs_warn)

    # All Pre-requistes check for MacOS
    def mac_prereqs_check(self):
        prereqs_failed = set()
        prereqs_warn = set()
        prereqs_failed_flag = False
        prereqs_warn_flag = False

        return (prereqs_failed_flag, prereqs_failed, prereqs_warn_flag, prereqs_warn)

    # Checks whether the prerequisites are met or not
    def prereqs_check(self, ulimits=False):
        help_msg = ""
        # Check pre-reqs as per machine OS
        if OS_NAME == "Linux":
            check = self.linux_prereqs_check()
            help_msg = "Please review the \'Quick start for Linux\' docs and rerun "\
                "the start command: " + Output.make_underline(QUICK_START_LINKS['linux'])+'\n'
        else:
            check = self.mac_prereqs_check()
            help_msg+="Please review the \'Quick start for macOS\' docs and rerun "\
                "the start command: "+ Output.make_underline(QUICK_START_LINKS['mac'])+'\n'

        # Get pre-req failures and warnings
        prereqs_failed_flag, prereqs_failed, prereqs_warn_flag, prereqs_warn = check

        msg = " System checks. Following pre-reqs are not met:\n"

        # Display message for failures
        failed_msg = ""
        for prereq in prereqs_failed:
            failed_msg+=("- "+PREREQS_ERROR_MSGS[prereq]+"\n")

        # Display message for warnings
        warning_msg = ""
        warnings = dict()
        for prereq in prereqs_warn:
            warning_msg+=("- "+PREREQS_ERROR_MSGS[prereq]+"\n")
            warnings[str(prereq)] = str(PREREQS_ERROR_MSGS[prereq])

        # Ulimits display message
        ulimit_msg = {
            'open_files' :"- "+PREREQS_ERROR_MSGS['open_files']+"\n",
            'max_user_processes' :"- "+PREREQS_ERROR_MSGS['max_user_processes']+"\n",
        }

        final_msg = ""
        check_status = None
        # Final display message in case of failures
        if prereqs_failed_flag:
            final_msg += (Output.ANIMATION_FAIL + " " + Output.make_red("FAILED") + ": " + msg)
            final_msg += (failed_msg+warning_msg)
            if ulimits:
                for ulimit in ulimits:
                    final_msg+=ulimit_msg[ulimit]
            final_msg += ("\n"+help_msg+"\n")
            check_status = Output.ANIMATION_FAIL
        # Final display message in case of warnings
        elif prereqs_warn_flag:
            final_msg = warnings
            if ulimits:
                for ulimit in ulimits:
                    final_msg[str(ulimit)] = str(PREREQS_ERROR_MSGS[ulimit])
            final_msg["help_msg"] = help_msg
            check_status = Output.ANIMATION_WARNING
        # Final display message in case of Ulimits failures
        elif ulimits:
            final_msg = dict()
            for ulimit in ulimits:
                final_msg[str(ulimit)] = str(PREREQS_ERROR_MSGS[ulimit])
            final_msg["help_msg"] = help_msg
            check_status = Output.ANIMATION_WARNING
        # All pre-reqs passsed
        else:
            final_msg = Output.ANIMATION_SUCCESS + " " + "System checks"
            check_status = Output.ANIMATION_SUCCESS

        result = {
            'msg' : final_msg,
            'status' : check_status,
        }

        return result

    # Checks yb-master and yb-tserver are running. Returns failed processes.
    # TODO: Check postmaster.pid.
    def get_failed_node_processes(self):
        failed_processes = []
        for process in ["tserver",]:
            if not ProcessManager.is_process_running(
                    process, self.configs.saved_data.get("data_dir")):
                failed_processes.append("yb-{}".format(process))

        if not self.configs.saved_data.get("read_replica"):
            for process in ["master",]:
                if not ProcessManager.is_process_running(
                        process, self.configs.saved_data.get("data_dir")):
                    failed_processes.append("yb-{}".format(process))

        return failed_processes

    # Called after receiving certain signals or on exit. Kills all subprocesses.
    def kill_children(self, signum=None, frame=None):
        if signum:
            Output.log("Received signal: {}".format(signum), logging.DEBUG)
        Output.print_and_log("Shutting down...")
        Output.console_access = False
        self.script.daemon_success.put(-1)
        cur_pid = os.getpid()
        pgid = os.getpgid(cur_pid)
        if not pgid:
            Output.log(
                "PGID could not be found for PID {}. Is {} running?".format(cur_pid, SCRIPT_NAME))
            os._exit(os.EX_OK)
        self.set_signals(SIG_DFL)

        for p in self.processes.values():
            p.delete_pidfile()
        self.script.delete_pidfile()

        try:
            # Kill process group instead of self.processes to ensure
            # any spawned child processes are killed. Use SIGKILL because YugaWare
            # requires KILL signal to terminate and nodes currently do not gracefully terminate.
            os.killpg(pgid, SIGKILL)
            Output.log(
                "{} may not have terminated properly... "
                "Please check PGID {}.".format(SCRIPT_NAME, pgid))
        except OSError as err:
            Output.log(
                "Failed to kill PGID {}... Is {} running?\n{}".format(pgid, SCRIPT_NAME, str(err)))

        # exit no matter what
        os._exit(os.EX_OK)

    def start_first_master_tserver(self):
        self.processes.get("master").start()

        was_already_setup = self.configs.saved_data.get("cluster_member", False)

        if was_already_setup:
            Output.log("Node was a member of some cluster before. "
                    "Skipping master setup")
        elif not self.setup_master():
            # TODO(sanketh): Make these all throw exceptions instead of excns + return values
            return "Failed to start master {}".format(SCRIPT_NAME)

        self.update_tserver_master_addrs()
        self.processes.get("tserver").start()
        if was_already_setup:
            Output.log("Node was a member of some cluster before. "
                    "Skipping tserver setup")
        elif not self.wait_tserver(cluster_type="primary"):
            return "Failed to start tserver {}".format(SCRIPT_NAME)

        if not was_already_setup:
            master_addresses = self.configs.saved_data.get("current_masters")
            universe_uuid = YBAdminProxy.get_cluster_uuid(master_addresses)
            if universe_uuid and universe_uuid != self.configs.saved_data["universe_uuid"]:
                self.configs.saved_data["universe_uuid"] = universe_uuid
                self.configs.save_configs()

        return None

    def start_first_read_replica_tserver(self):
        was_already_setup = self.configs.saved_data.get("cluster_member", False)

        self.processes.get("tserver").start()
        if was_already_setup:
            Output.log("Node was a member of some cluster before. "
                    "Skipping tserver setup")
        elif not self.wait_tserver(cluster_type="read_replica"):
            return "Failed to start tserver {}".format(SCRIPT_NAME)

        if not was_already_setup:
            master_addresses = self.configs.saved_data.get("current_masters")
            universe_uuid = YBAdminProxy.get_cluster_uuid(master_addresses)
            if universe_uuid and universe_uuid != self.configs.saved_data["universe_uuid"]:
                self.configs.saved_data["universe_uuid"] = universe_uuid
                self.configs.save_configs()

        return None


    # Starts yb-master, yb-tserver, and yugaware processes.
    # After initializing, creates a callhome thread.
    def start_processes(self):
        advertise_ip = self.advertise_ip()
        master_rpc_port = self.configs.saved_data.get("master_rpc_port")
        join_ip = self.configs.saved_data.get("join")
        master_addresses  = "{}:{}".format(advertise_ip, master_rpc_port)
        if join_ip:
            master_addresses  = "{}:{},{}".format(join_ip, master_rpc_port, master_addresses)
        was_already_setup = self.configs.saved_data.get("cluster_member", False)

        common_gflags = self.get_common_flags()

        yb_master_cmd = self.get_master_cmd(common_gflags)

        yb_tserver_cmd = self.get_tserver_cmd(common_gflags)

        self.processes = {
            "master": YBProcessManager(
                "master", yb_master_cmd, self.configs.saved_data.get("log_dir"),
                self.configs.saved_data.get("data_dir")),
            "tserver": YBProcessManager(
                "tserver", yb_tserver_cmd, self.configs.saved_data.get("log_dir"),
                self.configs.saved_data.get("data_dir")),
        }

        for p in self.processes.values():
            pid = p.get_pid()
            if pid:
                Output.print_out(
                    "{} is already running... Is there an existing {} process?".format(
                        p.name, SCRIPT_NAME))
                # Clear self.processes so kill_children() doesn't kill existing processes.
                self.processes = {}
                return

        is_first_run = True
        callhome_thread = None
        masters_list_update_thread = None
        self.stop_callhome = False
        while True:
            should_callhome = False

            is_first_install = is_first_run and not self.is_yb_initialized()

            # Create data directory.
            data_dir = self.configs.saved_data.get("data_dir")
            if not os.path.exists(data_dir):
                Output.log(
                    "Creating data directory {}.".format(data_dir))
                os.makedirs(data_dir)

            # Delete corrupted data dirs left from interrupting yb-master and yb-tserver startup.
            pid_file_name = os.path.basename(self.script.pidfile)
            data_dir_files = [ x for x in os.listdir(data_dir) if x != pid_file_name ]
            if is_first_install and data_dir_files:
                Output.print_and_log(
                    ("Found files {} in data dir {} from possibly failed initialization."
                    " Removing...").format(data_dir_files, data_dir))
                rmcontents(data_dir, exclude_names=[pid_file_name])

            # Start or initialize yb-master and yb-tserver.
            if is_first_run:
                # Output.init_animation("Running system checks...")
                warnings = []
                warnings_for_ui = []
                warning_help_msg=""
                ulimits_failed = self.script.set_rlimits(print_info=True)
                if ulimits_failed:
                    msg = "Failed to meet recommended settings. Ulimits too low - {}.\n".format(
                    ", ".join(ulimits_failed))
                    ulimit_warn_msg = msg + "Note {} will still run, although it may fail for " \
                        "larger workloads. For more info, see {}".format(SCRIPT_NAME, CONFIG_LINK)
                    self.alerts.append((ALERT_WARNING, ULIMIT_ERR_CODE, ulimit_warn_msg))

                prereqs_check_result = self.prereqs_check(ulimits=ulimits_failed)
                # Output.update_animation(msg=prereqs_check_result['msg'],
                #     status=prereqs_check_result['status'])
                if prereqs_check_result['status']==Output.ANIMATION_SUCCESS:
                    Output.print_out(prereqs_check_result['msg'])
                elif prereqs_check_result['status']==Output.ANIMATION_WARNING:

                    warnings.extend(list(prereqs_check_result['msg'].values())[:-1])
                    warning_help_msg = prereqs_check_result['msg']["help_msg"]

                    prereqs_check_result['msg'].pop("help_msg")
                    warnings_for_ui = []
                    for k in prereqs_check_result['msg'].keys():
                        warnings_for_ui.extend([k])
                elif prereqs_check_result['status']==Output.ANIMATION_FAIL:
                    Output.log(msg=prereqs_check_result['msg'],
                        level=logging.ERROR)


                Output.init_animation("Starting the YugabyteDB Processes...")

                self.post_install_yb()

                ret = self.start_first_master_tserver()
                if ret:
                    Output.update_animation("Database failed to start",
                        status=Output.ANIMATION_FAIL)
                    Output.log_error_and_exit(ret)

                Output.update_animation("YugabyteDB Started")

                if join_ip:
                    Output.print_and_log(Output.ANIMATION_SUCCESS +
                            " Node joined a running cluster with UUID {}"
                            .format(self.configs.saved_data.get("universe_uuid")))

                if self.configs.saved_data.get("secure"):
                    Output.init_animation("Enabling Encryption in Transit and " +
                        "Password Authentication...")
                    if not join_ip and not was_already_setup:
                        alphadigits = string.ascii_letters + string.digits
                        new_password = ''.join(PASSWORD_GENNERATOR(alphadigits) for i in range(12))
                        self.update_db_passwords(new_password)
                        self.create_password_file()
                    Output.update_animation("Encryption in Transit and " +
                        "Password Authentication enabled")
                else:
                    insecure_msg = "Cluster started in an insecure mode without " + \
                        "authentication and encryption enabled. For non-production use only, " + \
                        "not to be used without firewalls blocking the internet traffic."
                    warnings.append(insecure_msg)
                    warnings_for_ui.extend(["insecure"])

                # Persist the config after successful start
                self.configs.save_configs()
            else:
                for name in ("master", "tserver"):
                    process = self.processes.get(name)
                    process.remove_error_logs()
                    if not process.is_running():
                        Output.log(
                            "{} died unexpectedly. Restarting...".format(process.name),
                            logging.ERROR)
                        if name == "tserver":
                            self.update_tserver_master_addrs()
                        process.start()
                        should_callhome = True

            if is_first_run:
                if self.configs.saved_data.get("ui"):
                    yugabyted_ui_path = find_binary_location("yugabyted-ui")
                    if (yugabyted_ui_path is None):
                        ui_bin_not_found = "Couldn't find yugabyted-ui binary. " + \
                            "Cluster started without UI."
                        warnings.append(ui_bin_not_found)
                        self.configs.saved_data["ui"] = False
                    else:
                        yugabyted_ui_cmd = [ yugabyted_ui_path ] + \
                            [
                                "-database_host={}".format(advertise_ip),
                                "-master_ui_port={}".format(self.configs.saved_data.
                                                            get("master_webserver_port")),
                                "-tserver_ui_port={}".format(self.configs.saved_data.
                                                            get("tserver_webserver_port")),
                                "-warnings={}".format("|".join(warnings_for_ui))
                            ]
                        if self.configs.saved_data.get("secure"):
                            yugabyted_ui_cmd.extend(["-secure=true",
                                "-database_password={}".format(
                                self.configs.saved_data.get("database_password"))])
                    if self.configs.saved_data.get("secure"):
                        yugabyted_ui_cmd.extend(["-secure=true",
                            "-database_password={}".format(
                            self.configs.saved_data.get("database_password"))])

                    self.processes["yugabyted-ui"] = ProcessManager(
                        "yugabyted-ui", yugabyted_ui_cmd,
                        self.configs.saved_data.get("log_dir"),
                        self.configs.saved_data.get("data_dir"))

            if self.configs.saved_data.get("ui"):
                (_, was_started) = self.verify_start_yugabyted_ui(is_first_run, is_first_install)
                should_callhome = should_callhome or was_started

            if is_first_install and not join_ip:
                self.first_install_init_auth()

            if warnings:
                warning_msg = "\n" + Output.make_yellow(Output.ANIMATION_WARNING +
                    " WARNINGS") + ":\n"

                for msg in warnings:
                    warning_msg += "- " + msg + "\n"
                if warning_help_msg:
                    warning_msg += warning_help_msg

            if is_first_run:
                status = self.get_status_string() + \
                    "{} YugabyteDB started successfully! To load a sample dataset, " \
                    "try '{} demo'.\n" \
                    "{} Join us on Slack at {}\n" \
                    "{} Claim your free t-shirt at {}\n".format(
                        Output.ROCKET, SCRIPT_NAME, Output.PARTY,
                        Output.make_underline(SLACK_LINK), Output.SHIRT,
                        Output.make_underline(COMMUNITY_REWARDS_LINK))

                if self.configs.saved_data.get("secure"):
                    if not join_ip:
                        status += "\n{} is saved in the {}.\n{} is stored at {}\n".format(
                            Output.make_cyan("DB_PASSWORD"),
                            Output.make_cyan("Credentials File"),
                            Output.make_cyan("Credentials File"),
                            os.path.join(self.configs.saved_data.get("data_dir"),
                                "{}_credentials.txt".format(SCRIPT_NAME)))
                    else:
                        status += "\n{} is saved in ".format(Output.make_cyan("DB_PASSWORD")) + \
                            "the {} which has been generated on the 1st node.\n".format(
                                Output.make_cyan("Credentials File"))

                if len(self.setup_env_init.get_ysql_password()) > 99:
                    status = status + Output.make_red(YSQL_PASSWORD_LENGTH_WARNING)

                if warnings:
                    Output.print_out(warning_msg)

                Output.print_out(status)

                if self.configs.temp_data.get("background"):
                    # Let original process know daemon was successful so it can exit.
                    # This is to display the initial status message.
                    self.script.daemon_success.put(1)
                    # Ignore any console output as important information will be logged.
                    with open('/dev/null', 'r+') as dev_null:
                        Output.console_access = False
                        sys.stderr.flush()
                        sys.stdout.flush()
                        os.dup2(dev_null.fileno(), sys.stdin.fileno())
                        os.dup2(dev_null.fileno(), sys.stderr.fileno())
                        os.dup2(dev_null.fileno(), sys.stdout.fileno())

                Diagnostics.first_run_secs = time.time() - start_time_sec
                Diagnostics.first_install = is_first_install

                if self.configs.saved_data.get("callhome"):
                    callhome_thread = Thread(target=self.callhome_loop)
                    callhome_thread.daemon = True
                    callhome_thread.start()

                masters_list_update_thread = Thread(target=self.update_masters_list_loop)
                masters_list_update_thread.daemon = True
                masters_list_update_thread.start()

            is_first_run = False
            if should_callhome:
                self.callhome()

            time.sleep(int(self.configs.saved_data.get("polling_interval")))

        # Stop callhome. Useful in future if we do anything after quitting.
        self.stop_callhome = True
        callhome_thread.join()

    def verify_start_yugabyted_ui(self, is_first_run, is_first_install):

        was_started = False
        err = None

        if not self.configs.saved_data.get("ui"):
            return (err, was_started)

        yugabyted_ui_process = self.processes.get("yugabyted-ui")

        try:
            if is_first_run:
                Output.init_animation("Bringing up UI...")

            # Start yugabyted UI process.
            if not yugabyted_ui_process.is_running():
                if not is_first_run:
                    Output.log(
                        "Webserver died unexpectedly. Restarting...", logging.ERROR)
                yugabyted_ui_process.start()
                was_started = True

            # After first run, do not attempt any more setup, just return.
            if not is_first_run:
                return (err, was_started)

            if is_first_run:
                Output.update_animation("UI ready")

        finally:
            if is_first_run:
                animation_status = Output.ANIMATION_FAIL if err else Output.ANIMATION_SUCCESS
                # Output.update_animation("UI status", status=animation_status)
        return (err, was_started)

    def start_rr_process(self):
        advertise_ip = self.advertise_ip()
        master_rpc_port = self.configs.saved_data.get("master_rpc_port")
        join_ip = self.configs.saved_data.get("join")
        master_addresses  = "{}:{}".format(advertise_ip, master_rpc_port)
        if join_ip:
            master_addresses  = "{}:{},{}".format(join_ip, master_rpc_port, master_addresses)
        was_already_setup = self.configs.saved_data.get("cluster_member", False)

        common_gflags = self.get_common_flags()

        yb_tserver_cmd = self.get_tserver_cmd(common_gflags)

        self.processes = {
            "tserver": YBProcessManager(
                "tserver", yb_tserver_cmd, self.configs.saved_data.get("log_dir"),
                self.configs.saved_data.get("data_dir")),
        }

        for p in self.processes.values():
            pid = p.get_pid()
            if pid:
                Output.print_out(
                    "{} is already running... Is there an existing {} process?".format(
                        p.name, SCRIPT_NAME))
                # Clear self.processes so kill_children() doesn't kill existing processes.
                self.processes = {}
                return

        is_first_run = True
        callhome_thread = None
        masters_list_update_thread = None
        self.stop_callhome = False
        while True:
            should_callhome = False

            is_first_install = is_first_run and not self.is_yb_initialized()

            # Create data directory.
            data_dir = self.configs.saved_data.get("data_dir")
            if not os.path.exists(data_dir):
                Output.log(
                    "Creating data directory {}.".format(data_dir))
                os.makedirs(data_dir)

            # Delete corrupted data dirs left from interrupting yb-master and yb-tserver startup.
            pid_file_name = os.path.basename(self.script.pidfile)
            data_dir_files = [ x for x in os.listdir(data_dir) if x != pid_file_name ]
            if is_first_install and data_dir_files:
                Output.print_and_log(
                    ("Found files {} in data dir {} from possibly failed initialization."
                    " Removing...").format(data_dir_files, data_dir))
                rmcontents(data_dir, exclude_names=[pid_file_name])

            # Start or initialize yb-master and yb-tserver.
            if is_first_run:
                warnings = []
                warning_help_msg=""
                ulimits_failed = self.script.set_rlimits(print_info=True)
                if ulimits_failed:
                    msg = "Failed to meet recommended settings. Ulimits too low - {}.\n".format(
                    ", ".join(ulimits_failed))
                    ulimit_warn_msg = msg + "Note {} will still run, although it may fail for " \
                        "larger workloads. For more info, see {}".format(SCRIPT_NAME, CONFIG_LINK)
                    self.alerts.append((ALERT_WARNING, ULIMIT_ERR_CODE, ulimit_warn_msg))

                prereqs_check_result = self.prereqs_check(ulimits=ulimits_failed)
                if prereqs_check_result['status']==Output.ANIMATION_SUCCESS:
                    Output.print_out(prereqs_check_result['msg'])
                elif prereqs_check_result['status']==Output.ANIMATION_WARNING:
                    warnings.extend(list(prereqs_check_result['msg'].values())[:-1])
                    warning_help_msg = prereqs_check_result['msg']["help_msg"]
                elif prereqs_check_result['status']==Output.ANIMATION_FAIL:
                    Output.log(msg=prereqs_check_result['msg'],
                        level=logging.ERROR)

                Output.init_animation("Starting the YugabyteDB Processes...")

                self.post_install_yb()

                ret = self.start_first_read_replica_tserver()
                if ret:
                    Output.update_animation("Database failed to start",
                        status=Output.ANIMATION_FAIL)
                    Output.log_error_and_exit(ret)

                Output.update_animation("YugabyteDB Started")

                if join_ip:
                    Output.print_and_log(Output.ANIMATION_SUCCESS +
                            " Read Replica node joined a running cluster with UUID {}"
                            .format(self.configs.saved_data.get("universe_uuid")))

                if self.configs.saved_data.get("secure"):
                    Output.init_animation("Enabling Encryption in Transit and " +
                        "Password Authentication...")
                    Output.update_animation("Encryption in Transit and " +
                        "Password Authentication enabled")
                else:
                    warnings.append("Cluster started in an insecure mode without " +
                        "authentication and encryption enabled. For non-production use only, " +
                        "not to be used without firewalls blocking the internet traffic.")

                # Persist the config after successful start
                self.configs.save_configs()
            else:
                for name in ("tserver",):
                    process = self.processes.get(name)
                    process.remove_error_logs()
                    if not process.is_running():
                        Output.log(
                            "{} died unexpectedly. Restarting...".format(process.name),
                            logging.ERROR)
                        self.update_tserver_master_addrs()
                        process.start()
                        should_callhome = True

            if warnings:
                warning_msg = "\n" + Output.make_yellow(Output.ANIMATION_WARNING +
                    " WARNINGS") + ":\n"

                for msg in warnings:
                    warning_msg += "- " + msg + "\n"
                if warning_help_msg:
                    warning_msg += warning_help_msg

            if is_first_run:
                status = self.get_status_string() + \
                    "{} YugabyteDB started successfully! To load a sample dataset, " \
                    "try '{} demo'.\n" \
                    "{} Join us on Slack at {}\n" \
                    "{} Claim your free t-shirt at {}\n".format(
                        Output.ROCKET, SCRIPT_NAME, Output.PARTY,
                        Output.make_underline(SLACK_LINK), Output.SHIRT,
                        Output.make_underline(COMMUNITY_REWARDS_LINK))

                if len(self.setup_env_init.get_ysql_password()) > 99:
                    status = status + Output.make_red(YSQL_PASSWORD_LENGTH_WARNING)

                if warnings:
                    Output.print_out(warning_msg)

                Output.print_out(status)

                if self.configs.temp_data.get("background"):
                    # Let original process know daemon was successful so it can exit.
                    # This is to display the initial status message.
                    self.script.daemon_success.put(1)
                    # Ignore any console output as important information will be logged.
                    with open('/dev/null', 'r+') as dev_null:
                        Output.console_access = False
                        sys.stderr.flush()
                        sys.stdout.flush()
                        os.dup2(dev_null.fileno(), sys.stdin.fileno())
                        os.dup2(dev_null.fileno(), sys.stderr.fileno())
                        os.dup2(dev_null.fileno(), sys.stdout.fileno())

                Diagnostics.first_run_secs = time.time() - start_time_sec
                Diagnostics.first_install = is_first_install

                if self.configs.saved_data.get("callhome"):
                    callhome_thread = Thread(target=self.callhome_loop)
                    callhome_thread.daemon = True
                    callhome_thread.start()

                masters_list_update_thread = Thread(target=self.update_masters_list_loop)
                masters_list_update_thread.daemon = True
                masters_list_update_thread.start()

            is_first_run = False
            if should_callhome:
                self.callhome()

            time.sleep(int(self.configs.saved_data.get("polling_interval")))

    def get_common_flags(self):
        advertise_ip = self.advertise_ip()
        master_rpc_port = self.configs.saved_data.get("master_rpc_port")
        join_ip = self.configs.saved_data.get("join")
        fault_tolerance = self.configs.saved_data.get("fault_tolerance")
        master_addresses  = "{}:{}".format(advertise_ip, master_rpc_port)
        if join_ip:
            master_addresses  = "{}:{},{}".format(join_ip, master_rpc_port, master_addresses)
        certs_dir = self.configs.saved_data.get("certs_dir")

        # Check if connection manager is enabled
        tserver_flags = self.configs.saved_data.get("tserver_flags")

        if tserver_flags.find("enable_ysql_conn_mgr") != -1:
            METRICS_SNAPSHOT_LIST.append("ysql_conn_mgr")

        common_gflags = [
            "--stop_on_parent_termination",
            "--undefok=stop_on_parent_termination",
            "--fs_data_dirs={}".format(self.configs.saved_data.get("data_dir")),
            "--webserver_interface={}".format(advertise_ip),
            "--metrics_snapshotter_tserver_metrics_whitelist={}".format(
                ",".join(METRICS_SNAPSHOT_LIST)),
            "--yb_num_shards_per_tserver={}".format(YB_NUM_SHARDS_PER_TSERVER),
            "--ysql_num_shards_per_tserver={}".format(YSQL_NUM_SHARDS_PER_TSERVER),
            "--placement_cloud={}".format(self.configs.saved_data.get("cloud_provider")),
            "--placement_region={}".format(self.configs.saved_data.get("cloud_region")),
            "--placement_zone={}".format(self.configs.saved_data.get("cloud_zone")),
        ]

        if fault_tolerance == "region":
            common_gflags.append("--leader_failure_max_missed_heartbeat_periods=10")

        if self.configs.saved_data.get("secure"):
            common_gflags.extend(["--certs_dir={}".format(certs_dir),
            "--allow_insecure_connections=false",
            "--use_node_to_node_encryption=true",])

        return common_gflags


    def get_master_cmd(self, common_flags):
        advertise_ip = self.advertise_ip()
        master_rpc_port = self.configs.saved_data.get("master_rpc_port")
        join_ip = self.configs.saved_data.get("join")
        master_addresses  = "{}:{}".format(advertise_ip, master_rpc_port)
        if join_ip:
            master_addresses  = "{}:{},{}".format(join_ip, master_rpc_port, master_addresses)

        yb_master_cmd = [find_binary_location("yb-master")] + \
            common_flags + \
            [
                "--rpc_bind_addresses={}:{}".format(advertise_ip, master_rpc_port),
                "--server_broadcast_addresses={}:{}".format(advertise_ip, master_rpc_port),
                "--replication_factor=1",
                "--use_initial_sys_catalog_snapshot",
                "--server_dump_info_path={}".format(
                    os.path.join(self.configs.saved_data.get("data_dir"), "master-info")),
                "--master_enable_metrics_snapshotter=true",
                "--webserver_port={}".format(self.configs.saved_data.get("master_webserver_port")),
                "--default_memory_limit_to_ram_ratio=0.35",
                "--instance_uuid_override={}".format(self.configs.saved_data.get("master_uuid")),
            ]
        # if a join ip is specified, bring up a shell mode master
        if not join_ip:
            yb_master_cmd.append("--master_addresses={}".format(master_addresses))
            yb_master_cmd.append("--cluster_uuid={}".format(
                self.configs.saved_data.get("universe_uuid")))

        if self.configs.saved_data.get("master_flags"):
            yb_master_cmd.extend(
                ["--{}".format(flag) for flag in \
                    self.configs.saved_data.get("master_flags").split(",")])

        return yb_master_cmd

    def get_tserver_cmd(self, common_flags):
        advertise_ip = self.advertise_ip()
        master_rpc_port = self.configs.saved_data.get("master_rpc_port")
        join_ip = self.configs.saved_data.get("join")
        master_addresses  = "{}:{}".format(advertise_ip, master_rpc_port)
        if join_ip:
            master_addresses  = "{}:{},{}".format(join_ip, master_rpc_port, master_addresses)
        tserver_rpc_port = self.configs.saved_data.get("tserver_rpc_port")
        certs_dir = self.configs.saved_data.get("certs_dir")

        yb_tserver_cmd = [find_binary_location("yb-tserver")] + common_flags + \
            [
                "--{}={}".format(TS_MASTER_ADDRS_FLAG, master_addresses),
                "--rpc_bind_addresses={}:{}".format(advertise_ip, tserver_rpc_port),
                "--server_broadcast_addresses={}:{}".format(advertise_ip, tserver_rpc_port),
                "--cql_proxy_bind_address={}:{}".format(
                    advertise_ip, self.configs.saved_data.get("ycql_port")),
                "--server_dump_info_path={}".format(
                    os.path.join(self.configs.saved_data.get("data_dir"), "tserver-info")),
                "--start_pgsql_proxy", "--pgsql_proxy_bind_address={}:{}".format(
                    advertise_ip, self.configs.saved_data.get("ysql_port")),
                "--tserver_enable_metrics_snapshotter=true",
                "--metrics_snapshotter_interval_ms=11000",
                "--webserver_port={}".format(self.configs.saved_data.get("tserver_webserver_port")),
                "--default_memory_limit_to_ram_ratio=0.6",
                "--instance_uuid_override={}".format(self.configs.saved_data.get("tserver_uuid")),
                "--start_redis_proxy=false",
                "--placement_uuid={}".format(self.configs.saved_data.get("placement_uuid")),
            ]

        tserver_flags = self.configs.saved_data.get("tserver_flags")

        # If ysql_hba_conf_csv is present
        # Extract the value of ysql_hba_conf_csv
        # Remove the flag present in the tserver_flags string
        if tserver_flags.find("ysql_hba_conf_csv") != -1:
            ysql_hba_conf_csv_start_index = tserver_flags.find('ysql_hba_conf_csv')
            ysql_hba_conf_csv_end_index = tserver_flags.find('}', ysql_hba_conf_csv_start_index) + 1
            ysql_hba_conf_csv_flag = tserver_flags[ysql_hba_conf_csv_start_index:\
                    ysql_hba_conf_csv_end_index]
            ysql_hba_conf_csv_flag = ysql_hba_conf_csv_flag[ysql_hba_conf_csv_flag.find("{")+1:-1]

            # If the value was given through the config file
            # remove the starting and ending quotes.
            if ysql_hba_conf_csv_flag[0] == "'" and ysql_hba_conf_csv_flag[-1] == "'":
                ysql_hba_conf_csv_flag = ysql_hba_conf_csv_flag[1:-1]
            elif ysql_hba_conf_csv_flag[0] == '"' and ysql_hba_conf_csv_flag[-1] == '"':
                ysql_hba_conf_csv_flag = ysql_hba_conf_csv_flag[1:-1]

            # Remove the flag present in tserver_flags
            if ysql_hba_conf_csv_start_index == 0:
                tserver_flags = tserver_flags[ysql_hba_conf_csv_end_index+1:]
            else:
                tserver_flags = tserver_flags[:ysql_hba_conf_csv_start_index-1] + \
                    tserver_flags[ysql_hba_conf_csv_end_index:]

            yb_tserver_cmd.extend(["--ysql_hba_conf_csv={}".format(ysql_hba_conf_csv_flag)])

        # If ysql_ident_conf_csv is present
        # Extract the value of ysql_ident_conf_csv
        # Remove the flag present in the tserver_flags string
        if tserver_flags.find("ysql_ident_conf_csv") != -1:
            ysql_ident_conf_csv_start_index = tserver_flags.find('ysql_ident_conf_csv')
            ysql_ident_conf_csv_end_index = \
                tserver_flags.find('}', ysql_ident_conf_csv_start_index) + 1
            ysql_ident_conf_csv_flag = tserver_flags[ysql_ident_conf_csv_start_index:\
                    ysql_ident_conf_csv_end_index]
            ysql_ident_conf_csv_flag = \
                ysql_ident_conf_csv_flag[ysql_ident_conf_csv_flag.find("{")+1:-1]

            # If the value was given through the config file
            # remove the starting and ending quotes.
            if ysql_ident_conf_csv_flag[0] == "'" and ysql_ident_conf_csv_flag[-1] == "'":
                ysql_ident_conf_csv_flag = ysql_ident_conf_csv_flag[1:-1]
            elif ysql_ident_conf_csv_flag[0] == '"' and ysql_ident_conf_csv_flag[-1] == '"':
                ysql_ident_conf_csv_flag = ysql_ident_conf_csv_flag[1:-1]

            # Remove the flag present in tserver_flags
            if ysql_ident_conf_csv_start_index == 0:
                tserver_flags = tserver_flags[ysql_ident_conf_csv_end_index+1:]
            else:
                tserver_flags = tserver_flags[:ysql_ident_conf_csv_start_index-1] + \
                    tserver_flags[ysql_ident_conf_csv_end_index:]

            yb_tserver_cmd.extend(["--ysql_ident_conf_csv={}".format(ysql_ident_conf_csv_flag)])

        # If ysql_pg_conf_csv is present
        # Extract the value of ysql_pg_conf_csv given through CLI
        # Remove the flag present in the tserver_flags string
        if tserver_flags.find("ysql_pg_conf_csv") != -1:
            ysql_pg_conf_csv_start_index = tserver_flags.find('ysql_pg_conf_csv')
            ysql_pg_conf_csv_end_index = tserver_flags.find('}', ysql_pg_conf_csv_start_index) + 1
            ysql_pg_conf_csv_flag = tserver_flags[ysql_pg_conf_csv_start_index:\
                    ysql_pg_conf_csv_end_index]
            ysql_pg_conf_csv_flag = ysql_pg_conf_csv_flag[ysql_pg_conf_csv_flag.find("{")+1:-1]

            # If the value was given through the config file
            # remove the starting and ending quotes.
            if ysql_pg_conf_csv_flag[0] == "'" and ysql_pg_conf_csv_flag[-1] == "'":
                ysql_pg_conf_csv_flag = ysql_pg_conf_csv_flag[1:-1]
            elif ysql_pg_conf_csv_flag[0] == '"' and ysql_pg_conf_csv_flag[-1] == '"':
                ysql_pg_conf_csv_flag = ysql_pg_conf_csv_flag[1:-1]

            # Remove the flag present in tserver_flags
            if ysql_pg_conf_csv_start_index == 0:
                tserver_flags = tserver_flags[ysql_pg_conf_csv_end_index+1:]
            else:
                tserver_flags = tserver_flags[:ysql_pg_conf_csv_start_index-1] + \
                    tserver_flags[ysql_pg_conf_csv_end_index:]

            yb_tserver_cmd.extend(["--ysql_pg_conf_csv={}".format(ysql_pg_conf_csv_flag)])

        if tserver_flags:
            yb_tserver_cmd.extend(
                ["--{}".format(flag) for flag in tserver_flags.split(",")])

        # Add authentication flags in tserver
        if self.configs.saved_data.get("secure"):
            yb_tserver_cmd.extend(["--use_client_to_server_encryption=true",
                "--certs_for_client_dir={}".format(certs_dir),])

        if self.configs.saved_data.get("ysql_enable_auth"):
            yb_tserver_cmd.extend(["--ysql_enable_auth=true"])

        if self.configs.saved_data.get("use_cassandra_authentication"):
            yb_tserver_cmd.extend(["--use_cassandra_authentication=true"])

        if self.configs.saved_data.get("dns_enabled"):
            yb_tserver_cmd.extend(["--use_node_hostname_for_local_tserver=true"])

        return yb_tserver_cmd

    # Returns (error string, yw_started).
    # yw_started is True if YW was actually started
    def maybe_start_yw(self, is_first_run, is_first_install):
        was_started = False
        err = None

        if not self.configs.saved_data.get("ui"):
            return (err, was_started)

        yw_process = self.processes.get("yugaware")
        yw_proxy = YugaWareProxy(self.advertise_ip(),
            self.configs.saved_data.get("webserver_port"))

        # Setup schema for play framework.
        if is_first_run and not self.is_yw_initialized():
            Output.log("Setting up admin console schema...")
            Output.init_animation("Preparing UI schema...")
            if not self.init_yw():
                #TODO: make this return to caller
                Output.log_error_and_exit("Failed to set up admin console schema...")

            Output.update_animation("UI schema ready")

        try:
            if is_first_run:
                Output.init_animation("Bringing up UI...")

            # Start YW process.
            if not yw_process.is_running():
                if not is_first_run:
                    Output.log(
                        "Webserver died unexpectedly. Restarting...", logging.ERROR)
                yw_process.start()
                was_started = True

            # After first run, do not attempt any more setup, just return.
            if not is_first_run:
                return (err, was_started)

            # Login with username/pwd, this tells us that YW server is up.
            err = self.wait_yw_login(yw_proxy, insecure=False)
            if err:
                return (err, was_started)

            # On first install run, always setup YW.
            # On a first run that is not first install,
            # check if setup still needs to complete.
            needs_setup = is_first_install
            if not needs_setup:
                # Login insecurely - if this fails, this likely means
                # YW wasn't fully setup the first time around.
                err = self.wait_yw_login(yw_proxy, insecure=True)
                if err:
                    Output.log("Unable to insecure login to YW: {}".format(err))
                    needs_setup = True

            if not needs_setup:
                return (err, was_started)

            # Set up login without username and password.
            err = yw_proxy.set_security("insecure")
            if err:
                return (err, was_started)

            # Verify login without username and password.
            err = yw_proxy.insecure_login()
            if err:
                return (err, was_started)
            yw_logged_in = True

            full_master_list = self.wait_get_all_masters(timeout=60)
            if not full_master_list:
                err = "Unable to find full master list for YW import"
                return (err, was_started)

            # Import the universe (or re-import it). Re-importing should be harmless.
            err = yw_proxy.import_universe(
                        ",".join(full_master_list),
                        self.master_port(),
                        self.configs.saved_data.get("universe_uuid"))
            if err:
                return (err, was_started)

            err = yw_proxy.set_landing_page(
                    self.configs.saved_data.get("universe_uuid"))
            if err:
                return (err, was_started)

            if is_first_run:
                Output.update_animation("UI ready")
            if is_first_run and yw_process.is_running() and self.alerts:
                yw_proxy.send_alerts(self.alerts)

        finally:
            if is_first_run:
                animation_status = Output.ANIMATION_FAIL if err else Output.ANIMATION_SUCCESS
                Output.update_animation("UI status", status=animation_status)
        return (err, was_started)

    # Pushes yugabyted script to background as a daemon. The process is not tied to a shell, but
    # it will not survive between machine restarts.
    def daemonize(self):
        def remove_handlers():
            if PY_VERSION < 3:
                handlers = [e for e in atexit._exithandlers if e[0] == self.kill_children]
                for handler in handlers:
                    atexit._exithandlers.remove(handler)
            else:
                atexit.unregister(self.kill_children)

        if os.fork():
            # Delete any custom exit handlers so daemon has full control.
            remove_handlers()

            # If parent is interrupted, kill the children as well. Note there is potentially a
            # window where the daemon hasn't created its pidfile yet and this will error out before
            # it can kill the daemon.
            self.set_signals(self.stop)

            # Keep the parent process alive until the daemon confirms yugabyted started properly.
            try:
                self.script.daemon_success.get(timeout=600)
            except queue.Empty as e:
                Output.print_and_log(
                    "Timed out trying to start {} daemon.".format(SCRIPT_NAME), logging.ERROR)
                self.stop()
            sys.exit()
        os.chdir(YUGABYTE_DIR)
        os.setsid()
        os.umask(0)
        if os.fork():
            remove_handlers()
            sys.exit()
        Output.log("Daemon grandchild process begins execution.")

    # Sets env variables needed for yugabyted start.
    def set_env_vars(self):
        # Sets YW metrics to use local database.
        os.environ["USE_NATIVE_METRICS"] = "true"

    # Runs post_install script for linux computers.
    def post_install_yb(self):
        if not sys.platform.startswith('linux'):
            return

        post_install_script_path = find_binary_location('post_install.sh')

        # If post_install.sh script is not found then we assume that
        # we are executing it in development mode, hence skip post_install.sh script
        # TODO(Sanket): Refactor the design to have yugabyted a way of knowing whether it
        # is an install env v/s dev
        if(post_install_script_path is None):
            return

        Output.log("Running the post-installation script {} (may be a no-op)".format(
                    post_install_script_path))
        process = subprocess.Popen(
                post_install_script_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        std_out, std_err = process.communicate()
        if process.returncode != 0:
            Output.log_error_and_exit(
                "Failed running {} (exit code: {}). Standard output:\n{}\n. "
                "Standard error:\n{}".format(
                    post_install_script_path, process.returncode, std_out, std_err))
        Output.log("Successfully ran the post-installation script.")


    # Initialize YW process. Creates all necessary tables. Returns false if init failed.
    def init_yw(self):
        # Create Play evolutions table. Required for YugaWare to start up properly.
        create_play_table = [
            os.path.join(YUGAWARE_BIN_DIR, "yugaware"),
            "-Dconfig.file=" + YUGAWARE_CONF,
            "-Dlog.override.path={}".format(self.configs.saved_data.get("log_dir"))
        ]
        Output.log("Initializing play tables...")
        run_process(create_play_table)
        Output.log("Done initializing play tables.")

        return True

    # Returns if yb-master and yb-tserver were properly initialized before.
    def is_yb_initialized(self):
        for info_file in ("master-info", "tserver-info", "tserver-info-cql"):
            if not os.path.exists(os.path.join(self.configs.saved_data.get("data_dir"), info_file)):
                return False
        return True

    # Returns if yugaware was properly initialized before.
    def is_yw_initialized(self):
        # Check Play evolutions table was created.
        Output.log("Checking play_evolutions table")
        list_tables_cmd = [find_binary_location("ysqlsh"), "-d", WEBSERVER_DB, "-c", "\d"]
        out, err, ret_code = run_process(list_tables_cmd)
        Output.log("Finished checking play evolutions table")
        return not err and not ret_code and "play_evolutions" in out

    # Returns true if this master was found in the current list of masters
    def wait_master(self, master_addr):

        if (not self.processes.get("master").is_running()):
            Output.log("Failed waiting for yb-master... process died.", logging.ERROR)
            raise RuntimeError("process died unexpectedly.")

        cur_master_uuids = [ m[0] for m in YBAdminProxy.get_masters(master_addr) ]
        master_uuid = self.configs.saved_data.get("master_uuid")
        if not cur_master_uuids:
            raise RetryableError()
        return master_uuid in cur_master_uuids

    # Returns the current list of master UUIDs
    def get_master_uuids(self, master_addr):

        if (not self.processes.get("master").is_running()):
            Output.log("Failed waiting for yb-master... process died.", logging.ERROR)
            raise RuntimeError("process died unexpectedly.")

        cur_master_uuids = [ m[0] for m in YBAdminProxy.get_masters(master_addr) ]
        if not cur_master_uuids:
            raise RetryableError()
        return cur_master_uuids

    # Use the masters we know (ourselves and the join target) to discover the full cluster.
    # Retry until timeout in case the masters we know are still coming up.
    def wait_get_all_masters(self, timeout=180):
        Output.log("Waiting to get the full master addrs list from master")
        try:
            return retry_op(self.get_all_masters, timeout)
        except RuntimeError:
            Output.log("Failed to query for all masters. Exception: {}".format(
                traceback.format_exc()))
            return False

    # Use the masters we know (ourselves and the join target) to discover the full cluster.
    def get_all_masters(self):

        current_masters = self.configs.saved_data.get("current_masters")

        if current_masters:
            all_masters = [ m[1] for m in YBAdminProxy.get_masters(current_masters) ]
            Output.log("Got all masters: {}".format(all_masters))
            if all_masters:
                return all_masters

        join_ip = self.configs.saved_data.get("join")
        advertise_ip = self.advertise_ip()
        all_masters = None
        for master_ip in (join_ip, advertise_ip):
            if not master_ip:
                continue
            master_addr = "{}:{}".format(master_ip,
                                        self.configs.saved_data.get("master_rpc_port"))
            all_masters = [ m[1] for m in YBAdminProxy.get_masters(master_addr) ]
            Output.log("Got all masters: {}".format(all_masters))
            if all_masters:
                return all_masters

        raise RetryableError()


    # Waits till the newly added master comes in the list of masters of cluster
    def wait_get_master(self, master, timeout=180):
        Output.log("Waiting for master with address {} to be added".format(master))
        try:
            return retry_op_with_argument(self.get_master_at_addrs, master, timeout)
        except RuntimeError:
            Output.log("Failed to find master {} in the masters list. Exception: {}".format(
                traceback.format_exc()))
            return False

    # Raises a Retryable error if the master is not in the list of master of the cluster
    def get_master_at_addrs(self, master):

        current_masters = self.configs.saved_data.get("current_masters")
        all_masters_and_roles = ""
        if current_masters:
            all_masters_and_roles = { m[1].split(":")[0]: m[2]
                                    for m in YBAdminProxy.get_masters(current_masters) }
        if not all_masters_and_roles:
            join_ip = self.configs.saved_data.get("join")
            advertise_ip = self.advertise_ip()
            master_addr = "{}:{}".format(join_ip if join_ip else advertise_ip,
                                            self.configs.saved_data.get("master_rpc_port"))
            all_masters_and_roles = { m[1].split(":")[0]: m[2]
                                        for m in YBAdminProxy.get_masters(master_addr) }

        if master in all_masters_and_roles and all_masters_and_roles[master] == "FOLLOWER":
            return True

        raise RetryableError()

    # Get the current list of masters known to a tserver using api/v1/masters endpoint
    # of the tserver. Use the node address given with the --join flag or use the
    # advertise address if join flag is not provided.
    def get_current_masters_from_api(self, tserverIp):
        tserver_addr = "{}:{}".format(tserverIp,
                            self.configs.saved_data.get("tserver_webserver_port"))
        try:
            tserverMastersAPI = "http://{}/api/v1/masters".format(tserver_addr)
            response = urlopen(Request(tserverMastersAPI))
            jsonResponseFromTserver = json.load(response)
            dictOfAllNodes = jsonResponseFromTserver.get("master_server_and_type")

            currentMastersCSV = ""
            lenOfMasters = len(dictOfAllNodes)
            for node in dictOfAllNodes:
                currentMastersCSV += node["master_server"]
                lenOfMasters -= 1
                if lenOfMasters > 0:
                    currentMastersCSV += ","

            Output.log("Tserver {} returned the following ".format(tserverIp) +
                "set of the current masters {}.".format(currentMastersCSV),
                    logging.DEBUG)

            return currentMastersCSV

        except HTTPError as http_err:
            Output.log('HTTP error occurred while fetching current' +
                    'masters from tserver: {}', http_err)
            return ''
        except Exception as err:
            Output.log('Other error occurred while fetching current' +
                    'masters from tserver: {}', err)
            return ''

    # Waits till the newly removed master doesn't comes in the list of masters of cluster
    def wait_remove_master(self, master, timeout=180):
        Output.log("Waiting for master with address {} to be removed".format(master))
        try:
            return retry_op_with_argument(self.remove_master_at_addrs, master, timeout)
        except RuntimeError:
            Output.log("Newly removed master {} is still present in the masters list." +
                    " Exception: {}".format(traceback.format_exc()))
            return False

    # Raises a Retryable error if the newly removed master is in the list of master of the cluster
    def remove_master_at_addrs(self, master):
        join_ip = self.configs.saved_data.get("join")
        advertise_ip = self.advertise_ip()
        master_addr = "{}:{}".format(join_ip if join_ip else advertise_ip,
                                        self.configs.saved_data.get("master_rpc_port"))

        all_masters = [ m[1].split(":")[0] for m in YBAdminProxy.get_masters(master_addr) ]
        if master not in all_masters:
            return True

        raise RetryableError()

    # Get Leader master of the cluster
    def get_leader_master(self):

        current_masters = self.configs.saved_data.get("current_masters")
        if current_masters:
            all_masters_info = YBAdminProxy.get_masters(current_masters)
            for master_info in all_masters_info:
                if master_info[2] == "LEADER":
                    return master_info[1]

        join_ip = self.configs.saved_data.get("join")
        advertise_ip = self.advertise_ip()
        master_addr = "{}:{}".format(join_ip if join_ip else advertise_ip,
                                        self.configs.saved_data.get("master_rpc_port"))
        all_masters_info = YBAdminProxy.get_masters(master_addr)
        for master_info in all_masters_info:
            if master_info[2] == "LEADER":
                return master_info[1]

        Output.log_error_and_exit("Couldn't get Leader master")

    # Get all tservers details
    def get_all_tserver_info(self, master_hostport):
        try:
            leaderMasterURL = "http://{}/api/v1/tablet-servers".format(master_hostport)
            response = urlopen(Request(leaderMasterURL))
            jsonResponseFromMaster = json.load(response)
            return jsonResponseFromMaster
        except HTTPError as http_err:
            Output.log("HTTP Error occured while hitting the api endpoint " +
                "http://{}:7000/api/v1/tablet-servers: {}".format(master_hostport, http_err))
            Output.log_error_and_exit(Output.make_red("ERROR:") + " Master node " +
                "present at {} is not reachable.".format(master_hostport))
        except Exception as err:
            Output.log("HTTP Error occured while hitting the api endpoint " +
                "http://{}:7000/api/v1/tablet-servers: {}".format(master_hostport, err))
            Output.log_error_and_exit(Output.make_red("ERROR:") + " Master node " +
                "present at {} is not reachable.".format(master_hostport))

    # Get all masters placement locations
    def get_all_nodes_locations(self, all_tserver_info, placement_uuid):
        dictOfAllNodes = all_tserver_info.get(placement_uuid)
        placementInfoOfEveryNode = {}
        for node in dictOfAllNodes:
            hostname = node.split(":")[0]
            cloudLocationOfHost = "{}.{}.{}".format(dictOfAllNodes[node]["cloud"],
                dictOfAllNodes[node]["region"], dictOfAllNodes[node]["zone"])
            placementInfoOfEveryNode[hostname] = cloudLocationOfHost

        return placementInfoOfEveryNode

    # Get the location of a master in accordance to the fault tolerance
    def get_cloud_location_per_fault_tolerance(self, master_location):
        fault_tolerance = self.configs.temp_data.get("fault_tolerance")
        master_location = master_location.split(".")
        ft_location = ""
        if fault_tolerance == "cloud":
            ft_location = master_location[0]
        elif fault_tolerance == "region":
            ft_location = ".".join(master_location[:2])
        else:
            ft_location = ".".join(master_location)

        return ft_location

    # Get new valid master addresses that does not violate fault tolerance policy
    def get_new_valid_masters(self, current_masters, leader_master,
                              master_locations):

        replication_factor = int(self.configs.temp_data.get("replication_factor"))
        # ft_locations_to_masters_map is map in the form of {ft_location: list_of_masters}.
        # Here ft_location means the fault tolerance location (cloud.region for ft=region and
        # cloud.region.zone for ft=zone).
        # list_of_masters is the list of all the masters present in that ft_location.
        # This map is used to equally distribute the nodes if the number of ft_locations
        # is less than the replication factor (implied or explicit) specified in the
        # configure command.
        ft_locations_to_masters_map = dict()
        for master, location in master_locations.items():
            ft_location = self.get_cloud_location_per_fault_tolerance(location)
            if ft_location not in ft_locations_to_masters_map.keys():
                ft_locations_to_masters_map[ft_location] = [master]
            else:
                ft_locations_to_masters_map[ft_location].append(master)

        new_master_locations = dict()
        ft_location_of_leader_master = self.get_cloud_location_per_fault_tolerance(
                                                master_locations[leader_master])
        new_master_locations[leader_master] = ft_location_of_leader_master
        ft_locations_to_masters_map[ft_location_of_leader_master].remove(leader_master)
        # check if the current master's cloud location are already
        # statisfying the fault tolerance value and replication factor (implied or explicit)
        # specified in the configure command
        for master in current_masters:
            ft_location = self.get_cloud_location_per_fault_tolerance(master_locations[master])
            if ft_location not in new_master_locations.values():
                new_master_locations[master] = ft_location
                ft_locations_to_masters_map[ft_location].remove(master)

        if len(new_master_locations) == replication_factor:
            return list(new_master_locations.keys())

        # if new masters list is not equal to replication factor,
        # determine new masters based on the distinct cloud locations
        # of nodes in the cluster
        for master, location in master_locations.items():
            ft_location = self.get_cloud_location_per_fault_tolerance(location)
            if ft_location not in new_master_locations.values():
                new_master_locations[master] = ft_location
                ft_locations_to_masters_map[ft_location].remove(master)
            if len(new_master_locations) == replication_factor:
                return list(new_master_locations.keys())

        # if new master list is not equal to replication factor after iterating
        # through the distinct cloud locations, add the existing masters back
        # to the new master's list for satisfying the replication factor.
        new_valid_masters_list = list(new_master_locations.keys())
        while len(new_valid_masters_list) != replication_factor:
            for location, master_list in ft_locations_to_masters_map.items():
                if len(master_list) != 0:
                    new_valid_masters_list.append(master_list.pop())
                if len(new_valid_masters_list) == replication_factor:
                    break

        return new_valid_masters_list

    # Replace the old master with a new one to maintain fault tolerance policy
    def replace_master(self, master_addr, old_master, new_master):
        self.add_master_for_data_placement(master_addr, new_master)
        self.remove_master_for_data_placement(master_addr, old_master)

    # add a new master computed during configure data_placement command
    def add_master_for_data_placement(self, master_addr, new_master):

        Output.log("Adding master {} to the cluster.".format(new_master))
        if not YBAdminProxy.add_master(master_addr, new_master):
            Output.log_error_and_exit("Failed to add master {} to the cluster."
                            .format(new_master))
        if not self.wait_get_master(new_master):
            Output.log_error_and_exit("Couldn't get master {} in the cluster masters list."
                            .format(new_master))
        else:
            Output.log("Added master {} to the cluster".format(new_master))

    # remove an master computed during configure data_placement command
    def remove_master_for_data_placement(self, master_addr, old_master):

        Output.log("Removing master {} from the cluster.".format(old_master))
        if not YBAdminProxy.remove_master(master_addr, old_master):
            Output.log_error_and_exit("Failed to remove master {} from the cluster."
                            .format(old_master))
        if not self.wait_remove_master(old_master):
            Output.log_error_and_exit("master {} in the cluster masters list still exists."
                            .format(old_master))
        else:
            Output.log("Removed master {} from the cluster".format(old_master))

    def parse_constraint_value(self, constraint_str):
        placement_constraints = []
        priority_constraints = []
        for item in constraint_str.split(","):
            parts = item.rsplit(":", 1)
            placement_constraints.append(parts[0])
            if len(parts) > 1:
                priority_constraints.append(item)

        placement_str = ",".join(placement_constraints)
        return placement_str, priority_constraints

    # Get status details to print after configure data_placement was run
    def get_configure_status_details(self, placement_locations, new_masters, priority_info):

        fault_tolerance = self.configs.temp_data.get("fault_tolerance")
        replication_factor = int(self.configs.temp_data.get("replication_factor"))

        # Get the number of nodes present in each AZ/region
        placement_location_map = dict()
        for master in new_masters:
            ft_location = self.get_cloud_location_per_fault_tolerance(placement_locations[master])
            if ft_location not in placement_location_map.keys():
                placement_location_map[ft_location] = 1
            else:
                placement_location_map[ft_location] += 1

        status_details = []
        status_display_info = {}

        # Case Scenario: When each AZ/region has only 1 node. (Most happy path)
        if len(placement_location_map) == replication_factor:
            final_status = ""
            ft_status = ""

            # Case Scenario: When rf = 1, the universe cannot survive the failure of the
            # AZ/region in which the master node is located.
            if replication_factor == 1:
                if fault_tolerance == "zone":
                    final_status = "Configuration successful."
                    ft_status = "Primary Cluster cannot survive even 1 availability zone failure"
                elif fault_tolerance == "region":
                    final_status = "Configuration successful. Primary data placement is " + \
                        "not geo-redundant."
                    ft_status = "Primary Cluster cannot survive even 1 region failure."

                status_details = [
                    (Output.make_yellow("Status"), final_status),
                    (Output.make_yellow("Fault Tolerance"), ft_status),
                ]
                status_display_info[ft_status] = Output.make_red

            # Case Scenario: When rf = 3, 5, 7. the universe can survive atmost (rf-1)/2
            # AZ/region failure
            else:
                if fault_tolerance == "zone":
                    final_status = "Configuration successful."
                    ft_status = "Primary Cluster can survive at most any " + \
                        "{} availability zone failure".format(replication_factor//2)
                elif fault_tolerance == "region":
                    final_status = "Configuration successful. Primary data placement is " + \
                        "geo-redundant."
                    ft_status = "Primary Cluster can survive at most any " + \
                        "{} region failure.".format(replication_factor//2)

                status_details = [
                    (Output.make_yellow("Status"), final_status),
                    (Output.make_yellow("Fault Tolerance"), ft_status),
                ]
                status_display_info[ft_status] = Output.make_green

        # Case Scenario: When all the master nodes belong to the same AZ/region
        # In this case, the universe cannot survive even 1 AZ/region failure.
        elif len(placement_location_map) == 1:
            final_status = ""
            ft_status = ""

            if fault_tolerance == "zone":
                final_status = "Configuration successful."
                ft_status = "Primary Cluster cannot survive even 1 availability zone failure."
            elif fault_tolerance == "region":
                final_status = "Configuration successful. Primary data placement is " + \
                    "not geo-redundant."
                ft_status = "Primary Cluster cannot survive even 1 region failure."

            status_details = [
                (Output.make_yellow("Status"), final_status),
                (Output.make_yellow("Fault Tolerance"), ft_status)
            ]
            status_display_info[ft_status] = Output.make_red

        # Case Scenario: When all master nodes are distributed accross 2 AZs/regions
        # In this case the universe can survive failure of only
        # 1 AZ/region (The one with lower number of master nodes).
        elif len(placement_location_map) == 2:
            number_of_failure_node_tolerance = replication_factor//2
            if fault_tolerance == "zone":
                final_status = "Configuration successful."
                ft_status = [
                    "For specified Replication factor (rf) {},".format(replication_factor) + \
                        " not enough availability zones.",
                    "Primary Cluster can survive the failure of only {} availability zone.".format(
                        [k for k, v in placement_location_map.items()
                            if v<=number_of_failure_node_tolerance][0]),
                    "Following are number of nodes in each availability zone."
                ]
            elif fault_tolerance == "region":
                final_status = "Configuration successful. Primary data placement is " + \
                    "not geo-redundant."
                ft_status = [
                    "For specified Replication factor (rf) {},".format(replication_factor) + \
                        " not enough regions.",
                    "Primary Cluster can survive the failure of only {} region.".format(
                        [k for k, v in placement_location_map.items()
                            if v<=number_of_failure_node_tolerance][0]),
                    "Following are number of nodes in each region."
                ]

            status_details = [
                (Output.make_yellow("Status"), final_status),
                (Output.make_yellow("Fault Tolerance"), ft_status[0]),
                (Output.make_yellow(""), ft_status[1]),
                (Output.make_yellow(""), ft_status[2]),
            ]
            status_display_info[ft_status[0]] = Output.make_yellow
            status_display_info[ft_status[1]] = Output.make_yellow

            for az, nodes in sorted(placement_location_map.items(), key=lambda x: x[1],
                                                                        reverse=True):
                az_node_info = "{} : {}".format(az, nodes)
                status_details += [
                    (Output.make_yellow(""), az_node_info)
                ]
                if nodes > replication_factor//2:
                    status_display_info[az_node_info] = Output.make_red
                else:
                    status_display_info[az_node_info] = Output.make_green

        # Case Scenario: When all master nodes are distributed accross more than 2 AZs/regions
        else:
            final_status = ""
            ft_status = ""
            if fault_tolerance == "zone":
                final_status = "Configuration successful."
                ft_status = [
                    "For specified Replication factor (rf) {},".format(replication_factor) + \
                        " not enough availability zones.",
                    "Primary Cluster can survive the failure of availability zones,",
                    "if the total number of down nodes in the YugabyteDB cluster doesn't " + \
                        "exceed {}.".format(replication_factor//2),
                    "Following are number of replicas in each availability zone."
                ]
            elif fault_tolerance == "region":
                final_status = "Configuration successful. Primary data placement is " + \
                    "not geo-redundant."
                ft_status = [
                    "For specified Replication factor (rf) {},".format(replication_factor) + \
                        " not enough regions.",
                    "Primary Cluster can survive the failure of regions,",
                    "if the total number of down nodes in the YugabyteDB cluster doesn't " + \
                        "exceed {}.".format(replication_factor//2),
                    "Following are number of replicas in each region."
                ]

            status_details += [
                (Output.make_yellow("Status"), final_status),
                (Output.make_yellow("Fault Tolerance"), ft_status[0]),
                (Output.make_yellow(""), ft_status[1]),
                (Output.make_yellow(""), ft_status[2]),
                (Output.make_yellow(""), ft_status[3]),
            ]
            status_display_info[ft_status[0]] = Output.make_yellow
            status_display_info[ft_status[1]] = Output.make_yellow
            status_display_info[ft_status[2]] = Output.make_yellow

            for az, nodes in sorted(placement_location_map.items(), key=lambda x: x[1],
                                    reverse=True):
                az_node_info = "{} : {}".format(az, nodes)
                status_details += [
                    (Output.make_yellow(""), az_node_info)
                ]

        if priority_info:
            preference_msg = "Successful in setting preference for zones."
            status_details.append((Output.make_yellow("Zone Preferences"), preference_msg))
            status_display_info[preference_msg] = Output.make_green
            status_details.append((Output.make_yellow(""),
                                   "Following are the preferences for zones"))
            sorted_priority_info = sorted(priority_info, key=lambda x: int(x.rsplit(':', 1)[1]))
            for item in sorted_priority_info:
                zone, priority = item.rsplit(':', 1)
                zone_preference_info = "{} : {}".format(zone, priority)
                status_details.append((Output.make_yellow(""), zone_preference_info))

        status_display_info = None if len(status_display_info) == 0 else status_display_info

        return [status_details, status_display_info]

    # This functions is used to determine if the placement constraint specified in
    # configure command is valid
    def is_placement_constraint_valid_values(self, placement_locations, placement_constraint):

        placement_constraint_list = placement_constraint.split(',')
        placement_locations_list = placement_locations.values()
        for constraint in placement_constraint_list:
            if constraint in placement_locations_list:
                is_valid = True
            else:
                is_valid = False
                break

        return is_valid

    def is_placement_constraint_valid_length(self, placement_constraint):

        replication_factor = int(self.configs.temp_data.get("replication_factor"))
        placement_constraint_list = placement_constraint.split(',')
        is_valid = True
        if len(placement_constraint_list) != replication_factor:
            is_valid = False

        return is_valid
    # az_to_num_rr_nodes_map is map in the form of {az: num_of_rr_nodes}.
    # Here az means the availability zone in form of cloud.region.zone.
    # num_of_rr_nodes is the number of all the nodes present in that az.
    def get_az_to_num_rr_nodes_map(self, placement_locations):
        az_to_num_rr_nodes_map = dict()
        for location in list(placement_locations.values()):
            if location not in az_to_num_rr_nodes_map.keys():
                az_to_num_rr_nodes_map[location] = 1
            else:
                az_to_num_rr_nodes_map[location] += 1

        return az_to_num_rr_nodes_map

    # Validates whether the new configuration provided by the user for modifying
    # rr cluster is different from the currently set configuration.
    def validate_new_config_for_rr_modify(self, cluster_config):
        rr_replication_factor = self.configs.temp_data.get("rr_replication_factor")
        rr_placement_constraint = self.configs.temp_data.get("rr_data_placement_constraint")

        old_rr_replication_factor = cluster_config["replicationInfo"] \
                                                ["readReplicas"][0].get("numReplicas")
        old_rr_placement_constraint = []
        for replica_info in cluster_config["replicationInfo"]["readReplicas"][0]. \
                                                                get("placementBlocks"):
            cloud_info = replica_info["cloudInfo"]
            old_rr_placement_constraint.append("{}.{}.{}:{}".format(cloud_info["placementCloud"],
                                                                  cloud_info["placementRegion"],
                                                                  cloud_info["placementZone"],
                                                                  replica_info["minNumReplicas"]))
        old_rr_placement_constraint = ",".join(old_rr_placement_constraint)

        def is_replication_factor_same():
            return int(rr_replication_factor) == old_rr_replication_factor

        def is_placement_constraint_same():
            rr_placement_constraint_list = rr_placement_constraint.split(',')
            old_rr_placement_constraint_list = old_rr_placement_constraint.split(',')
            placement_location_to_add = [location for location in rr_placement_constraint_list
                                            if location not in old_rr_placement_constraint_list]
            return len(placement_location_to_add) == 0

        if rr_replication_factor and is_replication_factor_same() and rr_placement_constraint \
                                                            and is_placement_constraint_same():
            Output.print_and_log(Output.make_red("ERROR") + ": You have provided the " +
                                        "same data_placement_constraint and replication_factor " +
                                        "as the ones that are currently set. Please provide " +
                                        "any new configuration to run " +
                                        "`yugabyted configure_read_replica modify` command.")
            sys.exit(1)
        elif rr_replication_factor and is_replication_factor_same():
            if rr_placement_constraint:
                msg = "You have provided the same replication_factor as the one that is " + \
                    "currently set."
            else:
                msg = "You have provided the same replication_factor as the one that is " + \
                    "currently set and no new data_placement_constraint was provided."

            msg += " Please provide any new configuration to " + \
                                    "run `yugabyted configure_read_replica modify` command."
            Output.print_and_log(Output.make_red("ERROR") + ": " + msg)
            sys.exit(1)
        elif rr_placement_constraint and is_placement_constraint_same():
            if rr_replication_factor:
                msg = "You have provided the same data_placement_constraint as the one that " + \
                    "is currently set."
            else:
                msg = "You have provided the same data_placement_constraint as the one that " + \
                    "is currently set and no new replication factor was provided."

            msg += " Please provide any new configuration to " + \
                                    "run `yugabyted configure_read_replica modify` command."
            Output.print_and_log(Output.make_red("ERROR") + ": " + msg)
            sys.exit(1)

    # Find/Validate the rr_replication_factor and data_placement_policy for read replica cluster
    def get_rr_rf_and_placement_constraint(self, placement_locations):
        rr_replication_factor = self.configs.temp_data.get("rr_replication_factor")
        rr_placement_constraint = self.configs.temp_data.get("rr_data_placement_constraint")

        az_to_num_rr_nodes_map = self.get_az_to_num_rr_nodes_map(placement_locations)

        if not rr_replication_factor:
            # CASE 1
            # When both rf and placement constraint was not provided by the user
            # rf = num of different zones
            if not rr_placement_constraint:
                rr_replication_factor = str(len(az_to_num_rr_nodes_map))
                rr_placement_constraint = self.get_rr_placement_constraint(az_to_num_rr_nodes_map)

            # CASE 2
            # When rf was not given but placement constraint was given
            # Validate the given data placement constraint against multiple checks
            # rf = total number of replicas mentioned in the constraint
            else:
                self.validate_rr_data_placement_constraint(az_to_num_rr_nodes_map,
                                                           rr_placement_constraint)

                # If all checks pass for the data plcamenet constraint, then extract rf from it.
                # rf = total number of replicas mentioned in the constraint.
                rr_replication_factor = str(self.extract_total_replicas_from_constraint(
                                                                        rr_placement_constraint))
        # CASE 3
        # When rf is given but placement constraint is not provided
        # Validate rf and extract data placement constraint
        elif not rr_placement_constraint:

                # CHECK 1
                # Check if the given rr_replication_faction is <= total num of read replica nodes.
                if len(placement_locations) < int(rr_replication_factor):
                    Output.print_and_log(msg = Output.make_red("ERROR") + ": Not enough nodes " +
                            "to set-up a RF-{} read replica cluster".format(rr_replication_factor),
                            level=logging.ERROR)
                    sys.exit(1)

                # CHECK 2
                # Check if the number of AZs which has read
                # replica nodes is <= rr_replication_factor
                if len(az_to_num_rr_nodes_map) > int(rr_replication_factor):
                    Output.print_and_log(msg = Output.make_red("ERROR") + ": There needs to be " +
                            "a replica in each AZ which contains a read replica node. There " +
                            "are {} AZs which hosts read ".format(len(az_to_num_rr_nodes_map)) +
                            "replica nodes whereas the replication factor provided " +
                            "was only {}".format(rr_replication_factor), level=logging.ERROR)
                    sys.exit(1)

                # If all checks pass for the provided replication factor,
                # then get the optimal data_placement_constraint
                rr_placement_constraint = self.get_rr_placement_constraint(az_to_num_rr_nodes_map)

        # CASE 4
        # When both rf and placemnet constraint is provided by the user
        # Validate the given data placement constraint against multiple checks
        # Validate rf with the num of replicas mentioned in the placement constraint
        else:
            self.validate_rr_data_placement_constraint(az_to_num_rr_nodes_map,
                                                       rr_placement_constraint)

            # CHECK 4
            # Check if the total number of replicas mentioned in the placement constraint is
            # equal to the replication factor provided by the user.
            if not self.is_total_num_of_replicas_valid_for_rr(rr_placement_constraint,
                                                                    rr_replication_factor):
                Output.log_error_and_exit(Output.make_red("FAILED") + ": The total number of " +
                                          "replicas mentioned in the placement constraint does " +
                                          "not match the given replication factor.")

        return [rr_replication_factor, rr_placement_constraint]

    # Validate data placement constraint provided by the user against multiple checks
    def validate_rr_data_placement_constraint(self, az_to_num_rr_nodes_map,
                                              rr_placement_constraint):

        # CHECK 1
        # Check if all the placement locations in which read replica nodes are present has
        # been provided by the user or not.
        az_not_present_in_constraint_list = self.are_all_locations_present_in_constraint(
                                        az_to_num_rr_nodes_map, rr_placement_constraint)

        if az_not_present_in_constraint_list:
            error_msg = Output.make_red("FAILED") + ": Invalid placement location " + \
                    "values provided for placement constraint. There needs to be " + \
                    "at least 1 replica in each AZ that contains a read replica node. "

            if len(az_not_present_in_constraint_list) == 1:
                error_msg += "The following AZ is not mentioned in placement constraint.\n"
            else:
                error_msg += "The following AZs are not mentioned in placement " + \
                    "constraint.\n"


            for az in az_not_present_in_constraint_list:
                error_msg += Output.make_red("* " + az) + "\n"

            Output.log_error_and_exit(error_msg)

        # CHECK 2
        # Check if the placement locations mentioned in the constraint mentioned by the
        # user has read replica nodes in them.
        az_in_constraint_not_present_list = self.is_placement_constraint_valid_values_for_rr(
                                            az_to_num_rr_nodes_map, rr_placement_constraint)

        if az_in_constraint_not_present_list:
            error_msg = Output.make_red("FAILED") + ": Invalid placement location " + \
                "values provided for placement constraint. Cloud locations mentioned " + \
                "in the placement constraint should contain read replica nodes."

            if len(az_in_constraint_not_present_list) == 1:
                error_msg += "The following AZ mentioned in placement constraint does " + \
                                                "not contain any read replica nodes.\n"
            else:
                error_msg += "The following AZs mentioned in placement constraint " + \
                                            "does not contain any read replica nodes.\n"


            for az in az_in_constraint_not_present_list:
                error_msg += Output.make_red("* " + az) + "\n"

            Output.log_error_and_exit(error_msg)

        # CHECK 3
        # Check if the nodes present for each cloud location is >=
        # number of replicas mentioned for that cloud location.
        invalid_num_replica_locations = self.is_num_replicas_for_each_az_valid_for_rr(
                                        az_to_num_rr_nodes_map, rr_placement_constraint)

        # print(invalid_num_replica_locations)
        if invalid_num_replica_locations:
            msg = Output.make_red("FAILED") + ": Invalid number of replicas mentioned.\n"


            for invalid_num_replica_location in invalid_num_replica_locations:

                if int(invalid_num_replica_location[1]) == 1:
                    msg += "* There is only 1 node in the " + \
                            "{} ".format(invalid_num_replica_location[0]) + \
                            "but {} ".format(invalid_num_replica_location[2]) + \
                            "replicas were mentioned.\n"
                else:
                    msg += "* There are only {} nodes".format(invalid_num_replica_location[1]) + \
                            " in the {}".format(invalid_num_replica_location[0]) + \
                            " but {} ".format(invalid_num_replica_location[2]) + \
                            "replicas were mentioned.\n"

            Output.log_error_and_exit(msg)

    # Deduce the optimal placement constraint for read replica cluster
    def get_rr_placement_constraint(self, az_to_num_rr_nodes_map):

        rr_replication_factor = self.configs.temp_data.get("rr_replication_factor")
        if not rr_replication_factor:
            rr_replication_factor = len(az_to_num_rr_nodes_map)
        else:
            rr_replication_factor = int(rr_replication_factor)

        az_to_num_replicas_map = dict()
        total_replicas = 0
        while total_replicas < rr_replication_factor:
            for location, num_nodes in az_to_num_rr_nodes_map.items():
                if num_nodes != 0:
                    if location not in az_to_num_replicas_map.keys():
                        az_to_num_replicas_map[location] = 1
                    else:
                        az_to_num_replicas_map[location] += 1

                    az_to_num_rr_nodes_map[location] -= 1
                    total_replicas += 1

                if total_replicas == rr_replication_factor:
                    break

        rr_placement_constraint = [ "{}:{}".format(location, num_of_replicas)
                          for location, num_of_replicas in
                          az_to_num_replicas_map.items()]

        rr_placement_constraint = ",".join(rr_placement_constraint)

        return rr_placement_constraint

    # Extracts rr_replication_factor from the data_placement_constraint mentioned by the user.
    def extract_total_replicas_from_constraint(self, placement_constraint):

        placement_constraint_list = placement_constraint.split(',')
        num_replicas_list = [ int(location_with_replicas.split(':')[1]) for
                             location_with_replicas in placement_constraint_list ]
        total_replicas = sum(num_replicas_list)

        return total_replicas

    # This functions is used to determine if the placement locations in which read replica
    # nodes are present has been provided by the user or not.
    def are_all_locations_present_in_constraint(self, az_to_num_rr_nodes_map,
                                                    rr_placement_constraint):

        placement_constraint_list = rr_placement_constraint.split(',')
        placement_constraint_list = [ location_with_replicas.split(':')[0] for
                                     location_with_replicas in placement_constraint_list ]

        placement_locations_list = list(az_to_num_rr_nodes_map.keys())

        az_not_present_list = list()

        for location in placement_locations_list:
            if location not in placement_constraint_list:
                az_not_present_list.append(location)

        return None if len(az_not_present_list) == 0 else \
                                        az_not_present_list

    # This functions is used to determine if the placement locations mentioned in the constraint
    # mentioned by the user has read replica nodes in them.
    def is_placement_constraint_valid_values_for_rr(self, az_to_num_rr_nodes_map,
                                                    rr_placement_constraint):
        placement_constraint_list = rr_placement_constraint.split(',')
        placement_constraint_list = [ location_with_replicas.split(':')[0] for
                                     location_with_replicas in placement_constraint_list ]

        placement_locations_list = list(az_to_num_rr_nodes_map.keys())

        az_not_present_list = list()

        for location in placement_constraint_list:
            if location not in placement_locations_list:
                az_not_present_list.append(location)

        return None if len(az_not_present_list) == 0 else \
                                        az_not_present_list

    # Checks if the total number of replicas mentioned in the placement constraint is
    # equal to the replication factor provided by the user.
    def is_total_num_of_replicas_valid_for_rr(self, placement_constraint,
                                                      rr_replication_factor):
        total_replicas = self.extract_total_replicas_from_constraint(placement_constraint)
        is_valid = True
        if total_replicas != int(rr_replication_factor):
            is_valid = False

        return is_valid

    # Checks if the nodes present for each cloud location is at least equal or more
    # than the number of replicas mentioned for that cloud location.
    def is_num_replicas_for_each_az_valid_for_rr(self, az_to_num_rr_nodes_map,
                                                 placement_constraint):
        placement_constraint_list = placement_constraint.split(',')

        invalid_num_replica_locations = list()
        for location_with_replicas in placement_constraint_list:
            location_with_replicas = location_with_replicas.split(':')
            location, replicas = location_with_replicas[0], location_with_replicas[1]
            # print(location, replicas)
            if az_to_num_rr_nodes_map.get(location) < int(replicas):
                invalid_num_replica_locations.append([location,
                                                str(az_to_num_rr_nodes_map.get(location)),
                                                replicas])
        # print(invalid_num_replica_locations)
        return invalid_num_replica_locations if \
                        len(invalid_num_replica_locations) > 0 else None



    # Updating the database user passwords for postgres and cassandra
    def update_db_passwords(self, new_password):
        ysql_proxy = YsqlProxy(self.advertise_ip(), self.configs.saved_data.get("ysql_port"))
        Output.log("Attempting to update Postgres password.")
        try:
            retry_op_with_argument(ysql_proxy.try_update_password, new_password, timeout=60)
        except RuntimeError:
            Output.log_error_and_exit("Could not update Postgress user password." +
                " Exception: {}".format(traceback.format_exc()))

        self.setup_env_init.setup_cert_file_path(self.configs.saved_data.get("ca_cert_file_path"))
        ycql_proxy = YcqlProxy(self.advertise_ip(), self.configs.saved_data.get("ycql_port"),
                        secure=self.configs.saved_data.get("secure"))
        Output.log("Attempting to update Cassandra password.")
        try:
            retry_op_with_argument(ycql_proxy.try_update_password, new_password, timeout=60)
        except RuntimeError:
            Output.log_error_and_exit("Could not update Cassandra user password." +
                " Exception: {}".format(traceback.format_exc()))

        self.setup_env_init.update_passwords(new_password)
        self.configs.saved_data["database_password"] = new_password

    # Create a new file in base directory and put Ysql login credentials in it.
    def create_password_file(self):
        password_file = os.path.join(self.configs.saved_data.get("data_dir"),
                "{}_credentials.txt".format(SCRIPT_NAME))

        ysql_username, ysql_password, ysql_db = self.setup_env_init.get_ysql_credentials()
        ycql_username, ycql_password, ycql_keyspace = self.setup_env_init.get_ycql_credentials()

        content = ""
        content += "YSQL Credentials:\nUsername: {}\nPassword: {}\n" \
            "Database: {}\n\n".format(ysql_username, ysql_password, ysql_db)
        content += "YCQL Credentials:\nUsername: {}\nPassword: {}\n" \
            "Keyspace: {}\n".format(ycql_username, ycql_password, ycql_keyspace)

        with open(password_file, "w+") as f:
            f.write(content)

    # When the 3rd node joins, this method will set the placement location and rf to 3
    def setup_first_cluster_config(self):
        placement_uuid = self.configs.saved_data.get("placement_uuid")
        leader_master = self.get_leader_master().split(':')[0]
        leader_master_http_endpoint = "{}:{}".format(leader_master,
                        self.configs.saved_data.get("master_webserver_port"))

        all_tserver_info = self.get_all_tserver_info(leader_master_http_endpoint)
        placement_locations = self.get_all_nodes_locations(all_tserver_info, placement_uuid)
        placement_info = list(placement_locations.values())
        placement_info.append("{}.{}.{}".format(self.configs.saved_data.get("cloud_provider"),
                                                self.configs.saved_data.get("cloud_region"),
                                                self.configs.saved_data.get("cloud_zone")))
        placement_info = ",".join(placement_info)

        master_addresses = list(placement_locations.keys())
        master_addresses.append(self.configs.saved_data.get("advertise_address"))
        master_addresses = ["{}:{}".format(master, self.configs.saved_data.get("master_rpc_port")) \
                                           for master in master_addresses]
        master_addresses = ",".join(master_addresses)

        if not YBAdminProxy.modify_placement_info(master_addresses,
                                                  placement_info, placement_uuid):
            Output.log_error_and_exit(Output.make_red("FAILED") + ": Setting of default " +
                                      "data placement policy on the cluster.")

    # Verify that the master is in the current list of masters.
    # If not, set it up appropriately.
    def setup_master(self, timeout=180):
        Output.log("Waiting for master")
        join_ip = self.configs.saved_data.get("join")
        master_addrs = self.get_current_masters_from_api(join_ip)
        # API didn't return any masters addresses, use join or advertise address
        if not master_addrs:
            join_ip = self.configs.saved_data.get("join")
            master_ip = join_ip if join_ip else self.advertise_ip()
            master_addrs = "{}:{}".format(master_ip,
                                        self.configs.saved_data.get("master_rpc_port"))

        # save the current known masters received from the get_current_masters_from_api()
        self.configs.saved_data["current_masters"]= master_addrs

        try:
            master_uuids = retry_op_with_argument(self.get_master_uuids, master_addrs)
            current_node_master_uuid = self.configs.saved_data.get("master_uuid")
            # If False, it means the master is
            # not part of the current set of masters. If we have a
            # join_ip, let's try to add ourselves to it, otherwise
            # it is a hard failure.
            if current_node_master_uuid in master_uuids:
                self.configs.saved_data["cluster_member"] = True
                if not join_ip:
                    placement_uuid = self.configs.saved_data.get("placement_uuid")
                    placement_info = list()
                    placement_info = "{}.{}.{}".format(
                                                self.configs.saved_data.get("cloud_provider"),
                                                self.configs.saved_data.get("cloud_region"),
                                                self.configs.saved_data.get("cloud_zone"))

                    master_addresses = "{}:{}".format(
                                                self.configs.saved_data.get("advertise_address"),
                                                self.configs.saved_data.get("master_rpc_port"))

                    if not YBAdminProxy.modify_placement_info(master_addresses, placement_info,
                                                        placement_uuid, replication_factor="1"):
                        Output.log_error_and_exit(Output.make_red("FAILED") + ": Setting of " +
                                                "default data placement policy on the cluster.")
                    return True
            if not join_ip:
                return False
        except RuntimeError:
            Output.log_error_and_exit("Failed to setup master. Exception: {}".format(
                traceback.format_exc()))
            return False

        # The master was not in the current list of masters
        # and we have a valid join_ip
        advertise_ip = self.advertise_ip()

        if len(master_uuids) >= 3:
            # this is going to be a standalone shell master
            return True
        if not YBAdminProxy.add_master(master_addrs, advertise_ip):
            Output.log_error_and_exit("Unable to add master {} to existing cluster at {}.".format(
                advertise_ip, join_ip))
            return False

        # If we are the third master, set replication factor to 3. This makes the cluster
        # automatically expand to rf3 when the third node is added.
        if len(master_uuids) == 2:
            self.setup_first_cluster_config()

        try:
            if retry_op_with_argument(self.get_master_uuids, master_addrs, timeout):
                self.configs.saved_data["cluster_member"] = True
                Output.log("Completed setup and wait for master.")
                return True
        except RuntimeError:
            Output.log("setup_master: exception: {}".format(traceback.format_exc()))

        Output.log("Failed to setup master, did add_master succeed?")
        return False

    def is_tserver_up(self, cluster_type):
        join_ip = self.configs.saved_data.get("join")

        if cluster_type == "primary":
            master_addr = self.configs.saved_data.get("current_masters")
        else:
            master_addr = self.get_current_masters_from_api(join_ip)

        if not master_addr:
            master_ip = join_ip if join_ip else self.advertise_ip()
            master_addr = "{}:{}".format(master_ip,
                                     self.configs.saved_data.get("master_rpc_port"))
        self.configs.saved_data["current_masters"] = master_addr

        if (cluster_type == "primary" and not self.processes.get("master").is_running()):
            Output.log("Failed waiting for yb-master... process died.", logging.ERROR)
            raise RuntimeError("process not running")

        if not self.processes.get("tserver").is_running():
            Output.log("Failed waiting for yb-tserver... process died.", logging.ERROR)
            raise RuntimeError("process not running")

        cur_tservers = YBAdminProxy.get_tservers(master_addr)
        tserver_uuid = self.configs.saved_data.get("tserver_uuid")
        if not cur_tservers or tserver_uuid not in cur_tservers:
            raise RetryableError()

        Output.log("Found tserver uuid {} in list of tserver uuids {}.".format(
            tserver_uuid, cur_tservers))
        return True

    def wait_tserver(self, cluster_type, timeout=180):
        Output.log("Waiting for tserver ...")
        try:
            if retry_op_with_argument(self.is_tserver_up, cluster_type, timeout):
                Output.log("Completed waiting for tserver.")
                return True
        except RuntimeError:
            Output.log("wait_tserver: exception: {}".format(traceback.format_exc()))

        Output.log("Failed to wait for tserver.")
        return False

    # In a multi-node cluster, the tserver initially knows just about its own master and the
    # master it is joining. After the cluster is formed, this method will attempt to
    # refresh the full list of masters so that the tserver can become aware of other masters.
    def update_tserver_master_addrs(self):
        tserver_cmd = self.processes["tserver"].cmd
        master_flag = [ flag for flag in tserver_cmd if flag.find(TS_MASTER_ADDRS_FLAG) >= 0 ]
        full_master_list = self.configs.saved_data.get("current_masters").split(",")

        if not full_master_list:
            Output.log("Unable to get all masters list, keeping tserver flag: {}".format(
                master_flag))
            return

        if len(full_master_list) < 3:
            my_master_ip = "{}:{}".format(
                self.advertise_ip(), self.configs.saved_data.get("master_rpc_port"))
            full_master_list = list(set(full_master_list + [ my_master_ip ]))
            Output.log("Got full master addrs list: {}".format(full_master_list))

        new_master_flag = "--{}={}".format(TS_MASTER_ADDRS_FLAG, ",".join(full_master_list))
        Output.log("Old master flag is: {} and new master flag is: {}".format(
            master_flag, new_master_flag))
        if master_flag:
            tserver_cmd.remove(master_flag[0])
        tserver_cmd += [new_master_flag]
        self.configs.saved_data["current_masters"] = ",".join(full_master_list)

    def wait_yw_login(self, yw_proxy, timeout=180, insecure=False):
        Output.log("Attempting to log in...")
        start_time = time.time()
        # Only the last error message is recorded if timed out.
        err = ""
        while time.time() - start_time < timeout:
            err = yw_proxy.insecure_login() if insecure else yw_proxy.login()
            if not err:
                Output.log("Login succeeded.")
                return ""
            time.sleep(.5)
        Output.log("Failed to login: {}".format(err))
        return "Timeout: " + err

    # Returns pretty output table.
    def get_status_string(self):

        join_ip = self.configs.saved_data.get("join")
        advertise_ip = self.advertise_ip()
        was_already_setup = self.configs.saved_data.get("cluster_member", False)

        # It is possible that masters have changed, try to find the latest
        # masters before computing the status string.
        master_web_addrs_ip = join_ip
        master_addrs = self.get_current_masters_from_api(join_ip)
        if not master_addrs:
            master_web_addrs_ip = advertise_ip
            master_addrs = self.get_current_masters_from_api(advertise_ip)
        self.configs.saved_data["current_masters"] = master_addrs
        Output.log("Master address list updated, new list: {}".format(master_addrs))

        master_web_addrs = "{}:{}".format(master_web_addrs_ip,
                                      self.configs.saved_data.get("master_webserver_port"))

        ycql_port = self.configs.saved_data.get("ycql_port")
        cql_hostname_param = ""
        cql_port_param = ""
        if advertise_ip != IP_LOCALHOST or ycql_port != DEFAULT_YCQL_PORT:
            cql_hostname_param =  advertise_ip
            cql_port_param = str(ycql_port)
        ycql_username, ycql_password, ycql_keyspace = self.setup_env_init.get_ycql_credentials()

        ysql_hostname_param = "-h {}".format(advertise_ip) if advertise_ip != IP_LOCALHOST else ""
        ysql_port = self.configs.saved_data.get("ysql_port")
        ysql_port_param = "-p {}".format(ysql_port) if ysql_port != DEFAULT_YSQL_PORT else ""
        ysql_username, ysql_password, ysql_db = self.setup_env_init.get_ysql_credentials()

        status_info = []
        status_display_info = dict()
        # Make sure ascii escape characters for color encoding do not count towards char limit.
        if self.get_failed_node_processes():
            title = Output.make_bold(Output.make_red(SCRIPT_NAME))
            extra_len = len(Output.make_bold(Output.make_red("")))
            status = "Stopped"
            status_info = [
                (Output.make_yellow("Status"), status),
            ]
        else:
            title = Output.make_bold(Output.make_green(SCRIPT_NAME))
            extra_len = len(Output.make_bold(Output.make_green("")))
            # Check if there is a leader yet.
            # We can use a smaller timeout here instead of the regular 60 secs.
            # In case of starting up via yugabyted, we have already given enough time to
            # the leader election
            # In case of manual start or some other route, we can have a smaller timeout
            status = ""
            if was_already_setup:
                if master_addrs:
                    status = "Running."
            else:
                if self.wait_get_all_masters(timeout=10):
                    status = "Running."
                else:
                    status = "Status command timed out as YugabyteDB \"yb-master\" " + \
                                "process is not responding."

            enabled_security_features = []
            if self.configs.temp_data.get("yugabyted_cmd") == "status":
                encryption_enabled = self.is_encryption_at_rest_enabled()
                if encryption_enabled is None:
                    Output.log("Error checking status of " +
                        "encryption-at-rest.")
                elif encryption_enabled:
                        enabled_security_features.append("Encryption-at-rest")

            is_secure = self.configs.saved_data.get("secure")
            if is_secure:
                if self.is_leader_master_secure(master_web_addrs):
                    enabled_security_features.append("Encryption-in-transit")
                if self.configs.saved_data.get("ysql_enable_auth") and \
                    self.configs.saved_data.get("use_cassandra_authentication"):
                    enabled_security_features.append("Password Authentication")

            rf = self.configs.temp_data.get("replication_factor")
            if not rf and self.configs.temp_data.get("yugabyted_cmd") == "start":
                Output.init_animation("Verifying data placement constraint " +
                            "on the YugabyteDB Cluster...")
                rf = YBAdminProxy.get_cluster_rf(master_addrs)
                Output.update_animation("Data placement constraint successfully verified")
            else:
                rf = YBAdminProxy.get_cluster_rf(master_addrs)
            status_info = [
                (Output.make_yellow("Status"), status),
                (Output.make_yellow("Replication Factor"), rf),
            ]

            if enabled_security_features:
                status_info += [
                    (Output.make_yellow("Security Features"), ", ".join(enabled_security_features))
                ]


        ysql_flags = " {} {} -U {} -d {}".format(ysql_hostname_param, ysql_port_param,
                        ysql_username, ysql_db)
        ycql_flags = " {} {} -u {}".format(cql_hostname_param, cql_port_param,
                        ycql_username)
        if ycql_keyspace is not None:
            ycql_flags = ycql_flags + " -k {}".format(ycql_keyspace)
        if self.configs.saved_data.get("secure"):
            ycql_flags = ycql_flags + " --ssl"

        # TODO: Check if YW is disabled. This could be from saving --ui flag, checking PID,
        # or some other method. The first is not preferred because it would deviate from how the
        # other saved data works in that they persist between runs but --ui shoudln't.

        yw_status = "http://{}:{}".format(advertise_ip,
                                          self.configs.saved_data.get("webserver_port"))
        yugabyted_ui_status = "http://{}:{}".format(advertise_ip,
                                          self.configs.saved_data.get("yugabyted_ui_port"))
        yb_master_status = "http://{}:{}".format(advertise_ip,
                                          self.configs.saved_data.get("master_webserver_port"))
        console_desc = "Web console"
        if self.configs.saved_data.get("ui"):
            try:
                response = urlopen(Request(yugabyted_ui_status))
                if response.code == 200:
                    console_desc = "YugabyteDB UI"
                    status_info += [ (Output.make_yellow(console_desc), yugabyted_ui_status), ]
                else:
                    status_info += [ (Output.make_yellow(console_desc), yb_master_status), ]
            except HTTPError as http_err:
                Output.log('HTTP error occurred while fetching YugabyteDB UI {}', http_err)
                status_info += [ (Output.make_yellow(console_desc), yb_master_status), ]
            except Exception as err:
                Output.log('HTTP error occurred while fetching YugabyteDB UI {}', err)
                status_info += [ (Output.make_yellow(console_desc), yb_master_status), ]
        else:
            status_info += [ (Output.make_yellow(console_desc), yb_master_status), ]

        jdbc_string = "jdbc:postgresql://{}:{}/{}?user={}&password=".format(advertise_ip,
                                                                            ysql_port,
                                                                            ysql_db,
                                                                            ysql_username)
        if self.configs.saved_data.get("secure"):
            jdbc_string += Output.make_cyan("<DB_PASSWORD>")
            status_display_info[jdbc_string] = Output.make_cyan
        else:
            jdbc_string += "yugabyte"

        status_info += [
            (Output.make_yellow("JDBC"), jdbc_string),
            (Output.make_yellow("YSQL"), "bin/ysqlsh{}".format(ysql_flags)),
            (Output.make_yellow("YCQL"), "bin/ycqlsh{}".format(ycql_flags)),
            (Output.make_yellow("Data Dir"), self.configs.saved_data.get("data_dir")),
            (Output.make_yellow("Log Dir"), self.configs.saved_data.get("log_dir")),
            (Output.make_yellow("Universe UUID"), self.configs.saved_data.get("universe_uuid"))
        ]
        max_key_len = 0
        max_value_len = 0
        for k, v in status_info:
            if status_display_info is not None and v in status_display_info.keys():
                func = status_display_info.get(v)
                value_len = len(str(v)) - len(func(""))
            else:
                value_len = len(str(v))

            max_key_len = max(max_key_len, len(str(k)))
            max_value_len = max(max_value_len, value_len)

        div_line = "+" + "-" * (max_key_len + max_value_len + 4) + "+" + "\n"
        status = "\n" + div_line
        status += ("| {:^" + str(max_key_len + max_value_len + 2
            + extra_len) + "} |\n").format(title)
        status += div_line
        for k, v in status_info:
            func = None
            if status_display_info is not None and v in status_display_info.keys():
                func = status_display_info.get(v)

            extra_len = len(Output.make_yellow(""))
            status += ("| {:" + str(max_key_len + extra_len - 7) + "}: ").format(k)
            if func is not None:
                status += ("{:<" + str(max_value_len + 7 + len(func(""))) + "} |\n").format(v)
            else:
                status += ("{:<" + str(max_value_len + 7) + "} |\n"). \
                    format(v if v is not None else "None")

        status += div_line
        return status

    # Returns pretty output table
    def get_status_string_common(self, status_info, status_display_info = None):
        title = Output.make_bold(Output.make_green(SCRIPT_NAME))
        extra_len = len(Output.make_bold(Output.make_green("")))

        max_key_len = 0
        max_value_len = 0
        for k, v in status_info:
            max_key_len = max(max_key_len, len(str(k)))
            max_value_len = max(max_value_len, len(str(v)))

        div_line = "+" + "-" * (max_value_len + max_key_len + 5) + "+" + "\n"
        status = "\n" + div_line
        status += ("| {:^" + str(max_value_len + max_key_len + 3
                                 + extra_len) + "} |\n").format(title)
        status += div_line
        for k, v in status_info:
            func = None
            if status_display_info is not None and v in status_display_info.keys():
                func = status_display_info.get(v)
            extra_len = len(Output.make_yellow(""))
            status += ("| {:" + str(max_key_len + extra_len - 1) + "}: ").format(k)
            if func is not None:
                status += ("{:<" + str(max_value_len + 2 + len(func(""))) + "} |\n").format(func(v))
            else:
                status += ("{:<" + str(max_value_len + 2) + "} |\n").format(v)

        status += div_line
        return status

    # Callhome loop. Sends data every minute for the first hour, then every hour after.
    def callhome_loop(self):
        num_times_called = 0
        initial_interval = 60
        final_interval = 3600
        while not self.stop_callhome:
            self.callhome()
            num_times_called += 1
            # Send callhome data more often in initial hour.
            time.sleep(initial_interval if num_times_called < 60 else final_interval)

    # Collects callhome data and sends it.
    def callhome(self):
        if self.configs.saved_data.get("callhome"):
            try:
                url = "http://diagnostics.yugabyte.com"
                headers = {
                    "Content-Type": "application/json",
                    "User-Agent": "Mozilla",
                }
                data = Diagnostics(self.configs).get_data(self.processes)
                req = Request(url, headers=headers, data=data.encode('utf8'))
                resp = urlopen(req)
            except Exception as e:
                Output.log("Callhome failed: " + str(e))
                pass

    # update the master address list every min for the first half hour, then every hour after.
    def update_masters_list_loop(self):
        num_times_cmd_executed = 0
        initial_interval = 60
        final_interval = 3600
        try:
            while True:
                self.update_master_list_on_background_thread()
                num_times_cmd_executed += 1
                time.sleep(initial_interval
                        if num_times_cmd_executed < 30 else final_interval)
        except Exception as e:
                Output.log("update_masters_list: " + str(e))
                pass

    def update_master_list_on_background_thread(self):

        # global _global_current_list_of_masters
        current_master_list = self.configs.saved_data.get("current_masters").split(",")
        Output.log("thread-uml: current masters {}".format(current_master_list))

        # thread for updating the master address list in the background
        # First attempt for /masters using the --join flag
        masters_csv = self.get_current_masters_from_api(self.configs.saved_data.get("join"))
        if not masters_csv:
            # second attempt for /masters using the --advertise_address flag
            masters_csv = self.get_current_masters_from_api(self.advertise_ip())
            if not masters_csv:
                Output.log("thread-uml: Unable to query for all masters list, " +
                    "keeping masters list: {}".format(current_master_list))
                return

        full_master_list = masters_csv.split(",")
        # check if masters have changed since last refresh, if not return
        if_any_new_masters = set(full_master_list).difference(current_master_list)
        if if_any_new_masters:
            self.configs.saved_data["current_masters"] = masters_csv
            Output.log("thread-uml: master list updated, old list: {} ".format(
                current_master_list) + "new list: {}".format(full_master_list))

            # Persist the config if masters have changed
            self.configs.save_configs()


    # Calls func after receiving certain exit signals.
    def set_signals(self, func):
        for sig in EXIT_SIGNALS:
            signal(sig, func)

    # Returns true if Java is installed. While Java 8+ is necessary for YW, we don't explicitly
    # check for it so that future version format changes (e.g. Java 1.4 to Java 5) won't give
    # wrong error messages.
    def java_installed(self):
        try:
            cmd = ["java", "-version"]
            java_info = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
            return java_info is not None
        except (OSError, subprocess.CalledProcessError) as e:
            Output.log("Failed to find java: {}".format(e), logging.ERROR)
            return False

    # Check whether the hostnames or IP provided is in correct format.
    def validate_hostname_ip(self, ip):
        # Regex expression for validating IPv4
        ipv4_regex = "^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.){3}(25[0-5]|2[0-4]"\
            "[0-9]|1[0-9][0-9]|[1-9]?[0-9])$"

        # Regex expression for validating IPv6
        ipv6_regex = "((([0-9a-fA-F]){1,4})\\:){7}([0-9a-fA-F]){1,4}"

        # Regex expression for validating DNS names
        dns_regex = "^(?=.{1,255}$)(?:(?!-)[a-zA-Z0-9-]{1,63}(?<!-)\.?)+[a-zA-Z]{2,}$"

        # Regex expression for validating hostname
        hostname_regex = "^[a-zA-Z0-9.-]+$"

        # Regex expression for validating following pattern - 127.0.0.1.
        ip_address_edge_case_regex = "^[0-9.-]+$"

        ipv4_pattern = re.compile(ipv4_regex)
        ipv6_pattern = re.compile(ipv6_regex)
        dns_pattern = re.compile(dns_regex)
        hostname_pattern = re.compile(hostname_regex)
        may_be_ipaddress = re.compile(ip_address_edge_case_regex)

        # Checking if it is a valid IPv4
        if re.search(ipv4_pattern, ip):
            return True
        elif re.search(may_be_ipaddress, ip):
            return False

        # Checking if it is a valid IPv6
        if re.search(ipv6_pattern, ip):
            return True

        # Checking if it is a valid FQDN or hostname
        if (re.search(dns_pattern, ip) or
                re.search(hostname_pattern, ip)):
            self.configs.saved_data["dns_enabled"] = True
            return True

        return False

    # Returns a list of invalid IPs from the list of IPs provided.
    def get_invalid_ips(self, ips):
        invalid_ips = []
        for ip in ips:
            if not self.validate_hostname_ip(ip):
                invalid_ips.append(ip)

        return invalid_ips

    def find_ip_address_of_node(self):

        host_name = self.find_hostname_of_node()
        host_ip = socket.gethostbyname(host_name)

        return host_ip

    def find_hostname_of_node(self):
        fqdn = socket.getfqdn()

        if "localhost" in fqdn:
            fqdn = socket.getaddrinfo(socket.gethostname(), None, socket.AF_INET,
                                  socket.SOCK_DGRAM, socket.IPPROTO_IP,
                                  socket.AI_CANONNAME)[0][3]
            if "localhost" in fqdn:
                fqdn = socket.gethostname()

        return fqdn

    # Check whether the leader master is secure.
    def is_leader_master_secure(self, master_hostport):
        try:
            leaderMasterURL = "http://{}/api/v1/varz".format(master_hostport)
            response = urlopen(Request(leaderMasterURL))
            jsonResponseFromMaster = json.load(response)
            listOfAllFlags = jsonResponseFromMaster.get("flags")

            encryptionFlag = [flag for flag in listOfAllFlags if
                flag.get("name") == "use_node_to_node_encryption"]

            return encryptionFlag[0].get("value") == "true"

        except HTTPError as http_err:
            Output.log_error_and_exit("HTTP error occurred while checking for security of " +
                                    "leader master: {}".format(http_err))
        except Exception as err:
            Output.log_error_and_exit("Other error occurred while checking for security of " +
                                    "leader master: {}".format(err))

    # Parse the config file and input parent flags to validate and set them.
    # Parent flags: --config, --data_dir, --log_dir
    def validate_and_set_parent_configs(self, args):

        home_dir = os.path.expanduser("~")
        default_base_dir = os.path.join(home_dir, "var")
        default_conf_path = os.path.join(default_base_dir, "conf", "{}.conf".format(SCRIPT_NAME))
        base_dir = os.path.abspath(os.path.expanduser(args.base_dir)) if args.base_dir \
            else default_base_dir
        base_dir_conf = ''
        if args.base_dir:
            base_dir_conf = os.path.join(base_dir, "conf", "{}.conf".format(SCRIPT_NAME))

        has_errors = False

        self.conf_file = args.config or base_dir_conf or \
                         Configs.get_brew_config() or default_conf_path
        self.conf_file = os.path.realpath(os.path.expanduser(self.conf_file))

        conf_dir = os.path.dirname(self.conf_file)
        if not os.path.isdir(conf_dir):
            os.makedirs(conf_dir)

        self.configs = Configs.parse_config_file(self.conf_file, base_dir)

        for path_args in ("data_dir", "log_dir"):
            path = getattr(args, path_args, None)
            if path:
                setattr(args, path_args, os.path.abspath(os.path.realpath(path)))

        if args.data_dir is not None:
            args.data_dir = os.path.expanduser(args.data_dir)

            config_data_dir = self.configs.saved_data.get("data_dir")
            if (config_data_dir and os.path.exists(config_data_dir) and
                    config_data_dir != args.data_dir):
                has_errors = True
                # TODO: Gradefully handle this case... User should be able to override config.
                Output.print_out(
                    "Data directory already exists at {}.".format(config_data_dir))

        if args.log_dir is not None:
            args.log_dir = os.path.expanduser(args.log_dir)

            config_log_dir = self.configs.saved_data.get("log_dir")
            if (config_log_dir and os.path.exists(config_log_dir) and
                    config_log_dir != args.log_dir):
                Output.print_out(
                    "Old log directory already exists at {}. New logs will go to {}".format(
                        config_log_dir, args.log_dir))

        if has_errors:
            sys.exit(1)

        parent_flags = ["config", "log_dir", "data_dir"]
        # Override configs and defaults with user specified variables
        for k, v in get_kv(args.__dict__):
            if (v is not None and k in parent_flags and k in self.configs.saved_data
                    and v != self.configs.saved_data.get(k)):
                self.configs.saved_data[k] = v

        data_dir = self.configs.saved_data["data_dir"]
        log_dir = self.configs.saved_data["log_dir"]
        self.configs.save_configs()

        if not os.path.isdir(data_dir):
            os.makedirs(data_dir)
        if not os.path.isdir(log_dir):
            os.makedirs(log_dir)

        self.script = ScriptProcessManager(log_dir, data_dir)

    # Check and validate the flags regarding the secure deployment of the node.
    # Check whether the certs files are present or not.
    # Create certs if it is 1-node cluster throw error otherwise.
    def validate_security_configs(self, args):
        home_dir = os.path.expanduser("~")
        default_base_dir = os.path.join(home_dir, "var")
        base_dir = os.path.abspath(os.path.expanduser(args.base_dir)) if args.base_dir \
            else default_base_dir

        default_certs_dir = os.path.join(base_dir, "certs")

        certs_dir = self.configs.saved_data.get("certs_dir") or \
                    default_certs_dir

        has_errors = False

        certs_files = ["ca.crt", "node."+args.advertise_address+".crt",
                    "node."+args.advertise_address+".key"]

        if args.certs_dir is not None:
            args.certs_dir = os.path.expanduser(args.certs_dir)

            if not os.path.exists(args.certs_dir):
                # Case Scenario: When the directory provided by the user through
                # --certs_dir flag in CLI does not exists.
                has_errors = True
                Output.print_out(Output.make_red("ERROR") +
                    ": Trying to use {} as the certs directory. ".format(args.certs_dir) +
                    "No such directory found. Please provide the correct directory " +
                    "where the certificates are through --certs_dir flag.")
            else:
                if not check_files_in_path(args.certs_dir, certs_files):
                    # Case Scenario: When the directory provided by the user through --certs_dir
                    # flag in CLI exists but doesn't have the correct certs.
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") +
                        ": Trying to use {} as the certs ".format(args.certs_dir) +
                        "directory. Correct certs not found in this directory. Please " +
                        "either use 'cert generate_server_certs' command to create " +
                        "certificates or follow this doc to create your own " +
                        "certificates: {}".format(GENERATE_SERVER_CERTS))
                else:
                    # Case Scenario: When the directory provided by the user through
                    # --certs_dir flag in CLI exists and have the correct certs.
                    Output.log("Found certs at {}.".format(args.certs_dir))
        else:
            if os.path.exists(certs_dir):
                if not check_files_in_path(certs_dir, certs_files):
                    # Case Scenario: When the default certs directory or the certs directory saved
                    # in conf file exists but doesn't have the correct certs.
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") +
                        ": Trying to use {} as the certs ".format(certs_dir) +
                        "directory. Correct certs not found in this directory. Please " +
                        "either use 'cert generate_server_certs' command to create " +
                        "certificates or follow this doc to create your own " +
                        "certificates: {}".format(GENERATE_SERVER_CERTS))
                else:
                    # Case Scenario: When the default certs directory or the certs directory saved
                    # in conf file exists and has the correct certs.
                    Output.log("Found certs at {}.".format(certs_dir))
            else:
                if certs_dir != default_certs_dir:
                    # Case Scenario: When the certs directory is mentioned in the configs
                    # file but the directory doesn't exist.
                    # Example: When the user starts a secure cluster, then stops it.
                    # Then due to some reason the certs directory that was being used has
                    # been deleted. Now when the user tries to restart the node,
                    # yugabyted will try to use the same dir as it was stored in the config file,
                    # but the directory doesn't exist.
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") +
                        ": The directory mentioned in the --cert_dir value of the configs file " +
                        "doen't exist. Please create the directory and copy the certs into that " +
                        "directory.")
                elif args.join:
                    # Case Scenario: When the user is trying to start and join a node
                    # to a secure cluster but the generated certs are neither present
                    # in the default directory nor given through --certs_dir flag.
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") +
                        ": Trying to start a secure node but couldn't find the certs at " +
                        "default directory. Cannot create new certs as the --join flag " +
                        "was used. Please generate the node certificates in 1 node and " +
                        "copy them to other nodes.")
                else:
                    # Case Scenario: When the user is trying to start a secure node but
                    # the generated certs are neither present in default directory nor
                    # given through --certs_dir flag. Create new certs in this case.
                    Output.log("Couldn't find the certs so creating new certificates.")
                    if self.check_openssl():
                        has_errors = True
                        Output.print_out(Output.make_red("Error") + ": openssl " +
                            "not installed. Can't create certificates.")

                    if not has_errors:
                        gen_certs_dir = self.configs.saved_data.get("gen_certs_dir")
                        root_certs_dir = os.path.join(gen_certs_dir, "root-ca")

                        if not self.generate_ca_certs(root_certs_dir=root_certs_dir):
                            has_errors = True
                            Output.print_out(Output.make_red("Error") +
                                "Cert generation failed. Please check the logs.")

                        hostnames = [args.advertise_address]
                        generated_certs_hostnames = self.generate_node_server_certs(
                                            hostnames=hostnames, gen_certs_dir=gen_certs_dir)
                        if not has_errors and not len(generated_certs_hostnames):
                            has_errors = True
                            Output.print_out(Output.make_red("Error") +
                                "Cert generation failed. Please check the logs.")

                        if not has_errors:
                            os.makedirs(certs_dir)

                            for file in certs_files:
                                shutil.copy2(os.path.join(gen_certs_dir, args.advertise_address,
                                                            file), certs_dir)

            args.certs_dir = certs_dir



        if has_errors:
            sys.exit(1)

    # Find the correct security nature of the deployment of the node by checking
    # secure or insecure flags passed by the user and secure and insecure options
    # present in the config file
    def find_security_nature_of_deployment(self, args):
        if args.secure and not args.insecure:
            # If --secure flag is passed in the CLI and --insecure is not. Node should be
            # deployed in secure mode.
            args.secure = True
            args.insecure = False
            return

        if args.insecure and not args.secure:
            # If --insecure flag is passed in the CLI and --secure is not. Node should be
            # deployed in insecure mode.
            args.secure = False
            args.insecure = True
            return

        if args.secure and args.insecure:
            # If both --secure and --insecure flags are passed in the CLI, show an error.
            Output.log_error_and_exit(Output.make_red("ERROR") +
                ": --secure flag can't be used together with --insecure flag")

        if not self.configs.saved_data.get("secure"):
            # If neither --secure nor --insecure flag is passed in the CLI and the value
            # of --secure in conf file is False then start the node in insecure mode
            args.secure = False
            args.insecure = True
            return

        if not self.configs.saved_data.get("insecure"):
            # If neither --secure nor --insecure flag is passed in the CLI and value of
            # --insecure in conf file is False and --secure is True then start
            # the node in secure mode
            args.secure = True
            args.insecure = False
            return

        # If neither --secure nor --insecure flag and both --secure and --insecure have
        # values in conf file have value as True, show an error.
        Output.log_error_and_exit(Output.make_red("ERROR") +
            ": config file {} has both secure and ".format(self.conf_file) +
            "insecure set to True. Please change one of them to False")

    # Get the current master leader known to a tserver using api/v1/masters endpoint
    # of the tserver.
    def get_current_master_leader_from_api(self, tserverIP, timeout=60):
        try:
            tserverMastersAPI = "http://{}:9000/api/v1/masters".format(tserverIP)
            Output.log("Trying to get masters information from {}".format(tserverMastersAPI) +
                        " (Timeout={})".format(timeout))
            response = urlopen(Request(tserverMastersAPI), timeout=timeout)
            jsonResponseFromTserver = json.load(response)
            dictOfAllNodes = jsonResponseFromTserver.get("master_server_and_type")

            masterLeader = ""
            for node in dictOfAllNodes:
                if node["is_leader"]:
                    masterLeader = node["master_server"].split(':')[0]
                    break

            Output.log("Tserver {} returned the following".format(tserverIP) +
                        "master leader {}.".format(masterLeader),
                            logging.DEBUG)

            return masterLeader

        except HTTPError as http_err:
            Output.log("HTTP Error occured while hitting the api endpoint " +
                "http://{}:9000/api/v1/masters: {}".format(tserverIP, http_err))
            Output.log_error_and_exit(Output.make_red("ERROR:") + " Node at the join ip " +
                "provided is not reachable.")
        except URLError as url_error:
            Output.log("Some error occured while hitting the api endpoint " +
                "http://{}:9000/api/v1/masters: {}".format(tserverIP, url_error))
            Output.log_error_and_exit(Output.make_red("ERROR:") + " Node at the join ip " +
                "provided is not reachable.")
        except Exception as err:
            Output.log("Error occured while hitting the api endpoint " +
                "http://{}:9000/api/v1/masters: {}".format(tserverIP, err))
            Output.log_error_and_exit(Output.make_red("ERROR:") + " Node at the join ip " +
                "provided is not reachable.")


    # Parse config file and input args. Validate them and save any new configs.
    def validate_and_set_configs(self, args):

        has_errors = False

        if self.configs.saved_data.get("database_password") is not None:
            self.setup_env_init.update_passwords(self.configs.saved_data.get("database_password"))

        if args.parser == "collect_logs":
            self.configs.temp_data["collect_logs_stdout"] = args.stdout
            if args.collect_at_dir is not None:
                args.collect_at_dir = \
                os.path.realpath(
                os.path.abspath(
                os.path.expanduser(args.collect_at_dir)))
                self.configs.temp_data["collect_at_dir"]=args.collect_at_dir

            collect_at_dir=self.configs.temp_data.get("collect_at_dir")
            if not os.path.isdir(collect_at_dir):
                os.makedirs(collect_at_dir)

        if args.parser == "admin_operation":
            if not args.command:
                has_errors = True
                Output.print_and_log(Output.make_red("Error:") + " --command flag is empty. " +
                                 "Please specify a yb-admin command to execute.")
                # Output.print_out("--command flag is empty. " +
                #                  "Specify a yb-admin command to execute.")
            else:
                self.configs.temp_data["admin_command"] = args.command

            if args.master_addresses is not None:
                self.configs.temp_data[
                    "admin_operation_master_addresses"] = args.master_addresses

        if args.parser == "data_placement":
            if args.fault_tolerance is not None:
                if args.fault_tolerance.lower() in FAULT_TOLERANCE_CHOICES:
                    self.configs.temp_data["fault_tolerance"] = args.fault_tolerance.lower()
                else:
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") + ": " +
                        "Incorrect fault_tolerance value specified. " +
                        "Please specify one of the following - zone, region or cloud ")

            if args.constraint_value is not None:
                placement_info,priority_info = self.parse_constraint_value(args.constraint_value)
                placement_info = placement_info.split(",")

                # Initializing a map to store different types of errors
                constraint_value_error_map = {
                    'placement_error': [],
                    'invalid_priority_error': [],
                    'inconsistent_priority_error': [],
                    'missing_priority_error': False,
                }
                has_errors = False
                for constraint in placement_info:
                    is_valid_placement_info = True
                    cloud_info = constraint.split(".")
                    # Each cloud location should have exactly 3 parts:
                    # cloudprovider, region and zone
                    if (len(cloud_info) == 3):
                        for value in cloud_info:
                            if not value:
                                is_valid_placement_info = False
                    else:
                        is_valid_placement_info = False

                    if not is_valid_placement_info:
                        error_info = " * {}".format(constraint)
                        constraint_value_error_map['placement_error'].append(error_info)

                # Validate the format and consistency of the priority info
                if priority_info:
                    priority_numbers = []
                    zones = {}
                    for preference in priority_info:
                        zone, priority = preference.rsplit(":", 1)
                        # Ensure the priority value is a valid integer
                        if not priority.isdigit():
                            if priority == '':
                                error_info = " * Empty priority value found for {}".format(zone)
                            else:
                                error_info = " * {} in {}".format(priority, zone)
                            constraint_value_error_map['invalid_priority_error'].append(error_info)
                            continue

                        priority = int(priority)
                        priority_numbers.append(priority)
                        # Check that same zone must be specified with consistent priority values
                        if zone in list(zones.keys()):
                            if priority not in zones[zone]:
                                zones[zone].append(priority)
                        else:
                            zones[zone] = [priority]

                    for zone, priorities in zones.items():
                        # Condition to check if more than 2 priorities
                        # are associated with a specified zone
                        if len(priorities) > 1:
                            error_info = " * {} in {}". \
                            format(', '.join(map(str, sorted(priorities))), zone)
                            constraint_value_error_map['inconsistent_priority_error']. \
                            append(error_info)

                    # Ensure there are no missing priorities in the sequence
                    priority_set = set(priority_numbers)
                    max_priority = max(priority_numbers)
                    for i in range(1, max_priority + 1):
                        if i not in priority_set:
                            constraint_value_error_map['missing_priority_error'] = True
                            break

                errors = []

                # Add placement errors to the errors list
                if constraint_value_error_map.get('placement_error'):
                    errors.append("- Incorrect value specified for cloud location. " \
                                  "Please follow the format - cloudprovider.region.zone" \
                                  " for the following cloud locations:\n" + \
                        "\n".join(constraint_value_error_map['placement_error'])
                    )
                    has_errors = True
                # Add invalid priority errors to the errors list
                if constraint_value_error_map.get('invalid_priority_error'):
                    errors.append("- Incorrect priority value specified. " \
                        "Please specify a valid integer for the following cloud locations:\n" +
                        "\n".join(constraint_value_error_map['invalid_priority_error'])
                    )
                    has_errors = True
                # Add inconsistent priority errors to the errors list
                if constraint_value_error_map.get('inconsistent_priority_error'):
                    errors.append("- Same zone specified with different priority values. Please " \
                    "specify a consistent priority value for the following cloud locations:\n" +
                    "\n".join(constraint_value_error_map['inconsistent_priority_error'])
                    )
                    has_errors = True
                # Add missing priority errors to the errors list
                if constraint_value_error_map.get('missing_priority_error'):
                    errors.append("- Priority values should not skip numbers." \
                            " Please specify consecutive priority numbers."
                    )
                    has_errors = True

                if has_errors:
                    Output.print_out(Output.make_red("Error") + ": Following errors found for" \
                                        " --constraint_value flag:\n" + "\n".join(errors))
                 # If no errors are found, set the valid constraint value
                else:
                    self.configs.temp_data[
                        "constraint_value"] = args.constraint_value

            if args.rf is not None:
                self.configs.temp_data["replication_factor"] = str(args.rf)
            else:
                self.configs.temp_data["replication_factor"] = str("3")

        if args.parser == "generate_server_certs":
            if args.hostnames is None:
                has_errors = True
                Output.print_and_log(Output.make_red("Error") + ": Please provide the " +
                    "--hostnames along with \'yugabyted cert generate_server_certs\' command.")
            else:
                hostnames = args.hostnames.split(",")
                invalid_hostnames = self.get_invalid_ips(hostnames)
                if invalid_hostnames:
                    if len(invalid_hostnames) > 1:
                        msg = Output.make_red("ERROR") + ": Hostnames {} are not valid "\
                            "addresses. Please try again with a valid IPV4, IPV6 or DNS. "\
                            "hostnames.".format(",".join(invalid_hostnames))
                    else:
                        msg = Output.make_red("ERROR") + ": Hostname {} are not valid "\
                            "address. Please try again with a valid IPV4, IPV6 or DNS. "\
                            "hostnames.".format(",".join(invalid_hostnames))

                    has_errors = True
                    Output.print_and_log(msg)

            self.configs.temp_data["hostnames"] = args.hostnames

        if args.parser == "encrypt_at_rest":
            if not args.enable and not args.disable:
                has_errors = True
                Output.print_out(Output.make_red("ERROR") +
                    ": Either --enable or --disable flag has to be provided.")

            if args.enable and args.disable:
                has_errors = True
                Output.print_out(Output.make_red("ERROR") +
                    ": --enable and --disable flags cannot be used together.")
            elif args.enable:
                self.configs.temp_data["enable_encrypt_at_rest"] = args.enable
            elif args.disable:
                self.configs.temp_data["disable_encrypt_at_rest"] = args.disable

        if args.parser == "new":
            if args.data_placement_constraint is not None:
                data_constraints = args.data_placement_constraint.split(",")
                has_errors = False
                for constraint in data_constraints:
                    cloud_info = constraint.split(":")[0].split(".")
                    if (len(cloud_info) == 3):
                        for value in cloud_info:
                            if not value:
                                has_errors=True
                    else:
                        has_errors = True

                if has_errors:
                    Output.print_out(Output.make_red("ERROR") +
                        ": Incorrect value specified for --data_placement_constraint. " +
                        "Please specify comma sperated value with format " +
                        "- cloudprovider.region.zone")
                else:
                    self.configs.temp_data[
                        "rr_data_placement_constraint"] = args.data_placement_constraint

            if args.rf is not None:
                if int(args.rf) > 0:
                    self.configs.temp_data["rr_replication_factor"] = str(args.rf)
                else:
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") + ": Please provide a valid " +
                                 "--rf flag. Replication factor cannot be less than 1.")

        if args.parser == "modify":
            if args.data_placement_constraint is None and args.rf is None:
                has_errors = True
                Output.print_out(Output.make_red("ERROR") + ": Please provide " +
                                 "either --data_placement_constraint or --rf flag " +
                                 "for modifying the read replica cluster.")

            if args.data_placement_constraint is not None:
                data_constraints = args.data_placement_constraint.split(",")
                has_errors = False
                for constraint in data_constraints:
                    cloud_info = constraint.split(".")
                    if (len(cloud_info) == 3):
                        for value in cloud_info:
                            if not value:
                                has_errors=True
                    else:
                        has_errors = True

                if has_errors:
                    Output.print_out(
                        "Incorrect value specified for --data_placement_constraint. " +
                        "Please specify comma sperated value with format " +
                        "- cloudprovider.region.zone".format(
                            args.data_placement_constraint))
                else:
                    self.configs.temp_data[
                        "rr_data_placement_constraint"] = args.data_placement_constraint

            if args.rf is not None:
                if int(args.rf) > 0:
                    self.configs.temp_data["rr_replication_factor"] = str(args.rf)
                else:
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") + ": Please provide a valid " +
                                 "--rf flag. Replication factor cannot be less than 1.")

        if args.parser == "ysql":
            if args.username is not None:
                self.setup_env_init.set_ysql_user(args.username)
            if args.password is not None:
                self.setup_env_init.set_ysql_password(args.password)
            if args.database is not None:
                self.setup_env_init.set_ysql_db(args.database)

        if args.parser == "ycql":
            if args.username is not None:
                self.setup_env_init.set_ycql_user(args.username)
            if args.password is not None:
                self.setup_env_init.set_ycql_password(args.password)
            if args.keyspace is not None:
                self.setup_env_init.set_ycql_keyspace(args.keyspace)

        if args.parser == "start":
            if args.read_replica and not args.join:
                Output.print_out(Output.make_red("ERROR") + ": read_replica neds to be " +
                                 "started with --join flag.")

            if args.cloud_location is not None:
                cloud_location = args.cloud_location.split(".")
                if len(cloud_location) == 3:
                    self.configs.saved_data["cloud_provider"] = cloud_location[0]
                    self.configs.saved_data["cloud_region"] = cloud_location[1]
                    self.configs.saved_data["cloud_zone"] = cloud_location[2]
                else:
                    Output.print_out(Output.make_red("ERROR") +
                        ": Incorrect format used for flag --cloud_location. " +
                        "Please use cloud.region.zone format.")
                    has_errors = True

            if args.fault_tolerance is not None:
                if args.fault_tolerance == "cloud":
                    Output.log_error_and_exit("Cloud based fault tolerance is not supported yet.")
                if args.fault_tolerance.lower() in START_FAULT_TOLERANCE_CHOICES:
                    self.configs.saved_data["fault_tolerance"] = args.fault_tolerance.lower()
                else:
                    has_errors = True
                    Output.print_out(Output.make_red("ERROR") + ": " +
                        "Incorrect fault_tolerance value specified. " +
                        "Please specify one of the following - zone, region or cloud ")

            if args.listen is not None:
                if args.advertise_address is not None and args.listen != args.advertise_address:
                    Output.print_out(Output.make_red("ERROR") +
                        ": --listen and --advertise_address flags " +
                        "are same. --listen is depricated. Can't have different values for them.")
                    has_errors = True
                args.advertise_address = args.listen

            if args.advertise_address is None:
                if not self.configs.saved_data.get("advertise_address"):
                    if OS_NAME == "Linux":
                        # Case Scenario: When advertise_address has no value in conf file
                        # and the OS is Linux, set the advertise address to the private IP
                        # of the machine.
                        args.advertise_address = self.find_ip_address_of_node()
                    else:
                        # Case Scenario: When advertise_address has no value in conf file
                        # and the OS is Mac, set the advertise address to 127.0.0.1.
                        args.advertise_address = IP_LOCALHOST
                else:
                    # Case Scenario: When advertise_address has a value in conf file and then
                    # use that regardless of the OS.
                    args.advertise_address = self.configs.saved_data.get("advertise_address")
            else:
                if not self.validate_hostname_ip(args.advertise_address):
                    has_errors = True
                    Output.print_and_log(Output.make_red("ERROR") + ": --advertise_address " +
                        "provided is not a valid address. Please try again with a valid IPV4, " +
                        "IPV6 or DNS.")

            args.advertise_address = IP_LOCALHOST if args.advertise_address == IP_ANY \
                else args.advertise_address

            if args.daemon is not None:
                if args.background is not None and \
                        self.parse_bool(args.daemon) != self.parse_bool(args.background):
                    Output.print_out(Output.make_red("ERROR") +
                        ": --daemon and --background flags " +
                        "are same. --daemon is depricated. Can't have different values for them.")
                    has_errors = True
                args.background = args.daemon

            if args.background is None:
                args.background = "True"

            if args.join is not None:
                if not self.validate_hostname_ip(args.join):
                    Output.log_error_and_exit(Output.make_red("ERROR") + ": --join" +
                        " provided is not a valid address. Please try again with a valid IPV4, " +
                        "IPV6 or DNS.")

                Output.print_and_log("Fetching configs from join IP...")
                # Check if tserver webserver at join_IP is reachable or not
                # Also get the leader master(used to get the info of all tservers)
                master_leader = self.get_current_master_leader_from_api(args.join)
                args.join = master_leader

                # Get info on all tservers
                master_leader_hostport = "{}:7000".format(master_leader)
                tservers_info = dict(self.get_all_tserver_info(master_leader_hostport))

                # Check if any existing node has the same IP as advertise address
                for uuid, nodes in tservers_info.items():
                    for node in [node.split(":")[0] for node in list(nodes.keys())]:
                        if args.advertise_address == node:
                            Output.log_error_and_exit(Output.make_red("ERROR:") + " A node is " +
                                        "already running on {}, please ".format(args.join) +
                                        "specify a valid address.")

                is_placement_uuid_set = False

                # Set placement UUID for the node according to it's properties(rr or primary)
                if args.read_replica:
                    # When the 1st read replica node is started use a new uuid
                    if len(tservers_info) == 1:
                        is_placement_uuid_set = True
                        Output.log("Starting first read replica node. " +
                                    "Using {} as placement_uuid".format(
                                        self.configs.saved_data.get("placement_uuid")))
                    # When a read replica cluster exists use the existing placement UUID
                    else:
                        for uuid, nodes in tservers_info.items():
                            nodes_list = [node.split(":")[0] for node in list(nodes.keys())]
                            if master_leader not in nodes_list and len(nodes) != 0:
                                self.configs.saved_data["placement_uuid"] = uuid
                                Output.log("Using placement_uuid {} from ".format(uuid) +
                                            "existing read replica cluster.")
                                is_placement_uuid_set = True
                else:
                    # Use placement uuid set for the primary cluster when 1st node was started.
                    for uuid, nodes in tservers_info.items():
                        nodes_list = [node.split(":")[0] for node in list(nodes.keys())]
                        if master_leader in nodes_list:
                            self.configs.saved_data["placement_uuid"] = uuid
                            Output.log("Using placement_uuid {} from ".format(uuid) +
                                            "existing primary cluster.")
                            is_placement_uuid_set = True

                # If placement UUID could not be set for some reason, throw an error
                if not is_placement_uuid_set:
                    Output.log("Cannot find placement UUID for the node. " +
                            "Leader Master node: {}. ".format(master_leader) +
                            "Response from tablet-servers API: {}".format(
                                str(tservers_info)))
                    Output.log_error_and_exit(Output.make_red("ERROR:") +
                            " Unable to start the node.")

            # If no --join is passed then start a new cluster with a new placement_uuid
            else:
                Output.log("Starting first primary node. Using {} as placement_uuid".format(
                                           self.configs.saved_data.get("placement_uuid")))

            self.find_security_nature_of_deployment(args)

            if (args.certs_dir is not None) and (not args.secure):
                # Case Sceneario: When certs_dir flag is passed without secure flag.
                has_errors = True
                Output.print_out(Output.make_red("ERROR") +
                    ": --certs_dir flag needs to be accompanied with the --secure flag.")

            if args.insecure:
                if args.join:
                    master_hostport = "{}:7000".format(args.join)
                    if self.is_leader_master_secure(master_hostport):
                        # Case Scenario: When a User starts the 1st node in secure mode and tries
                        # to start the second node in insecure mode
                        has_errors = True
                        Output.print_out(Output.make_red("ERROR") + ": The node whose " +
                            "IP was provided in --join flag has SSL/TLS enabled. Cannot join a " +
                            "secure and an insecure node.")
            elif args.secure:
                if args.join:
                    master_hostport = "{}:7000".format(args.join)
                    if not self.is_leader_master_secure(master_hostport):
                        # Case Scenario: When the user starts the 1st node in insecure mode and
                        # tries to start the second node in secure mode.
                        has_errors = True
                        Output.print_out(Output.make_red("ERROR") + ": The node whose " +
                            "IP was provided --join flag does not have SSL/TLS enabled. Cannot " +
                            "join a secure and an insecure node.")

                if not has_errors:
                    self.validate_security_configs(args)

                    args.ysql_enable_auth="true"
                    args.use_cassandra_authentication="true"

                    self.configs.saved_data["ca_cert_file_path"] = os.path.join(args.certs_dir,
                                                                                    "ca.crt")

            args.background = self.parse_bool(args.background)
            if args.ui is not None:
                args.ui = self.parse_bool(args.ui)
            else:
                args.ui = self.configs.saved_data.get("ui")

            if args.callhome is not None:
                args.callhome = self.parse_bool(args.callhome)
            elif os.environ.get("YB_DISABLE_CALLHOME") is not None:
                args.callhome = os.environ.get("YB_DISABLE_CALLHOME") not in TRUE_CHOICES
            else:
                args.callhome = DEFAULT_CALLHOME

            # Set authentication flags same as provided in the command-line flags.
            if args.ysql_enable_auth:
                args.ysql_enable_auth = self.parse_bool(args.ysql_enable_auth)

            if args.use_cassandra_authentication:
                args.use_cassandra_authentication = self.parse_bool(args.use_cassandra_authentication)

            # Set authentication flags to True, if it is first-run and
            # have required environment variables to enforce the authentication.
            if not self.is_yb_initialized():
                if self.setup_env_init.is_exists('YSQL_PASSWORD'):
                    args.ysql_enable_auth = True

                # Add use_cassandra_authentication flag to enforce authentication for YCQL
                if self.setup_env_init.is_exists('YCQL_USER') or \
                        self.setup_env_init.is_exists('YCQL_PASSWORD'):
                    args.use_cassandra_authentication = True

            self.configs.temp_data["background"] = args.background
            self.configs.saved_data["ui"] = args.ui
            self.configs.temp_data["initial_scripts_dir"] = args.initial_scripts_dir

        if has_errors:
            sys.exit(1)

        parent_flags = ["config", "log_dir", "data_dir"]
        # Override configs and defaults with user specified variables
        for k, v in get_kv(args.__dict__):
            if (v is not None and k not in parent_flags and k in self.configs.saved_data
                    and v != self.configs.saved_data.get(k)):
                self.configs.saved_data[k] = v


    def parse_bool(self, config):
        return config in TRUE_CHOICES

    def run(self):
        # Parent subparser for common args
        common_parser = argparse.ArgumentParser(add_help=False)
        common_parser.add_argument(
            "--config", help="{} configuration file path".format(
                              SCRIPT_NAME), metavar="")
        # TODO: Refactor data_dir to be a list for multi-node. How should the config file and
        # data dir be set for local mulit-node setups? Note: daemon mode may be affected.
        common_parser.add_argument(
            "--data_dir", help=argparse.SUPPRESS)
        common_parser.add_argument(
            "--base_dir", help="Directory under which {} will store data, conf and logs".format(
                SCRIPT_NAME), metavar="")
        common_parser.add_argument(
            "--log_dir", help=argparse.SUPPRESS)

        start_msg = "To start YugabyteDB cluster, run '{}'.\n\n".format(
            Output.make_green("{} start".format(SCRIPT_NAME)))
        start_msg += "Find more information at: {}".format(Output.make_underline(YUGABYTED_LINK))
        parser = PrettyArgParser(description=start_msg)
        all_parsers = {"default": parser}

        # Top Level Commands: start, stop, destroy, status, version, collect_logs
        subparsers = parser.add_subparsers(dest="parser", metavar="")
        subparsers.required = True
        for cmd, description in (
                ("start", "Start YugabyteDB cluster."),
                ("stop", "Stop running YugabyteDB cluster."),
                ("destroy", "Destroy YugabyteDB cluster and remove data."),
                ("status", "Print status of YugabyteDB cluster."),
                ("version", "Release version of YugabyteDB cluster."),
                ("collect_logs", "Collect and package logs for troubleshooting.")):
            example = ""
            if EXAMPLE.get(cmd):
                example = Output.make_yellow("Examples") + ": \n" + EXAMPLE[cmd]
            subparser = subparsers.add_parser(cmd, description=example,
                                                help=description, parents=[common_parser])
            subparser.epilog = EPILOG_SPECIFIC[cmd] + EPILOG_COMMON
            func = getattr(self, cmd, None)
            subparser.set_defaults(func=func)
            all_parsers[cmd] = subparser

        # Top Level command: connect
        connect = subparsers.add_parser("connect",
                    help="Connect to YugabyteDB cluster through the CLI.")
        all_parsers["connect"] = connect

        # Sub-commands for top level `connect` command: ysql, ycql
        connect_subparser = connect.add_subparsers(dest="parser", metavar="")
        connect_subparser.required = True
        for api in YUGABYTE_API_CLIENT_PROGRAMS:
            cur_parser = connect_subparser.add_parser(
                api, help="Use {} through the CLI.".format(api.upper()), parents=[common_parser])
            func = getattr(self, "connect_{}".format(api), None)
            cur_parser.set_defaults(func=func)
            all_parsers[api] = cur_parser

        # Flags for sub command: ysql
        for cmd in ("ysql",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--username", help="YSQL username to connect to" \
                " the YugabyteDB", metavar="")
            cur_parser.add_argument(
                "--password", help="The password for YSQL username to connect" \
                " to the YugabyteDB", metavar="")
            cur_parser.add_argument(
                "--database", help="Name of the YSQL database to connect" \
                " to the YugabyteDB", metavar="")

        # Flags for sub command: ycql
        for cmd in ("ycql",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--username", help="YCQL username to connect to" \
                " the YugabyteDB", metavar="")
            cur_parser.add_argument(
                "--password", help="The password for YCQL username to connect" \
                " to the YugabyteDB", metavar="")
            cur_parser.add_argument(
                "--keyspace", help="Name of the YCQL keyspace to connect" \
                " to the YugabyteDB", metavar="")

        # Top Level command: demo
        demo_parser = subparsers.add_parser("demo", help="Load and interact with preset demo data.")
        all_parsers["demo"] = demo_parser

        # Sub-commands for top level `demo` command: connect, destroy
        demo_subparsers = demo_parser.add_subparsers(dest="parser", metavar="")
        demo_subparsers.required = True
        for cmd, description in (
                ("connect", "Connect to the demo database."),
                ("destroy", "Destroy the demo database.")):
            subparser = demo_subparsers.add_parser(cmd, help=description, parents=[common_parser])
            parser_name = cmd + "_demo"
            func = getattr(self, parser_name, None)
            subparser.set_defaults(func=func)
            all_parsers[parser_name] = subparser

        # Top Level command: cert
        example = Output.make_yellow("Examples") + ": \n" + EXAMPLE["cert"]
        cert_parser = subparsers.add_parser("cert", description=example,
                                                help="Generate SSL certificates")
        all_parsers["cert"] = cert_parser
        cert_subparsers = cert_parser.add_subparsers(dest="parser", metavar="")
        cert_subparsers.required = True
        for cmd, description in (
                ("generate_server_certs", "Generate node server certificates."),):
            subparser = cert_subparsers.add_parser(cmd, help=description, parents=[common_parser])
            parser_name = "cert_" + cmd
            func = getattr(self, parser_name, None)
            subparser.set_defaults(func=func)
            all_parsers[parser_name] = subparser

        # Sub-commands for top level `cert` command: generate_server_certs
        for cmd in ("cert_generate_server_certs",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--hostnames", help="Hostnames of the nodes to be added in the cluster. " +
                "Mandatory flag.", metavar="")

        # Flags for top level `collect_logs` command
        # Docker: Redirect the logs.tar.gz to stdout
        for cmd in ("collect_logs",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--stdout", help="Redirect the logs.tar.gz file's content to stdout. Ex: "+
                "docker exec <container-id> bin/yugabyted collect_logs --stdout > yugabyted.tar.gz",
                action="store_true", default=False)
            cur_parser.add_argument(
                "--collect_at_dir", help="Directory under which {} will store logs.tar.gz file"
                .format(SCRIPT_NAME),metavar="")

        # Top Level command: configure
        example = Output.make_yellow("Examples") + ": \n" + EXAMPLE["configure"]
        configure_parser = subparsers.add_parser("configure", description=example,
                                                help="Configure data placement or " +
                                                "toggle encryption at rest for " +
                                                "the cluster.", parents=[common_parser])

        # Sub-commands for top level `configure` command: data_placement,
        # encrypt_at_rest, admin_operation
        all_parsers["configure"] = configure_parser
        configure_subparsers = configure_parser.add_subparsers(dest="parser", metavar="")
        configure_subparsers.required = True
        for cmd, description in (
                ("data_placement", "Configure multi-zone/multi-region cluster."),
                ("encrypt_at_rest", "Enable or disable encryption at rest."),
                ("admin_operation", "Run yb-admin command on the YugabyteDB cluster.")):
            example = ""
            if EXAMPLE.get(cmd):
                example = Output.make_yellow("Examples") + ": \n" + EXAMPLE[cmd]
            subparser = configure_subparsers.add_parser(cmd, help=description, description=example,
                                parents=[common_parser])
            parser_name = "configure_" + cmd
            func = getattr(self, parser_name, None)
            subparser.set_defaults(func=func)
            all_parsers[parser_name] = subparser

        # Flags for sub-command: data_placement
        # Flags for muti-zone/multi-region configuration.
        for cmd in ("configure_data_placement",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--fault_tolerance", help="No more than 1 node in the same " +
                "fault tolerance can be a master",
                metavar="")
            cur_parser.add_argument(
                "--constraint_value", help="Data placement constriant to be applied " +
                "on the YugabyteDB Cluster",
                metavar="")
            cur_parser.add_argument(
                "--rf", help="Set the replication factor for each tablet",
                metavar="")

        # Flags for sub-command: encrypt_at_rest
        for cmd in ("configure_encrypt_at_rest",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--enable", help="Enable encryption at rest for the cluster. No need to set a " +
                "value for the flag. Use --enable or --disable flag to toggle encryption " +
                "features on a YugabyteDB cluster.",
                action="store_true")
            cur_parser.add_argument(
                "--disable", help="Disable encryption at rest for the cluster. No need to set a " +
                "value for the flag. Use --enable or --disable flag to toggle encryption " +
                "features on a YugabyteDB cluster.",
                action="store_true")

        # Flags for sub-command: admin_operation
        for cmd in ("configure_admin_operation",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--command", help="specify the yb-admin command to be executed " +
                                        "on the YugabyteDB Cluster",
                metavar="")
            cur_parser.add_argument(
                "--master_addresses", help="specify the comma seperated values of current " +
                "masters of the cluster.",
                metavar=""
            )

        # Top Level command: configure_read_replica
        example = Output.make_yellow("Examples") + ": \n" + EXAMPLE["configure_read_replica"]
        configure_read_replica_parser = subparsers.add_parser("configure_read_replica",
                                                description=example, help="Configure/Modify/" +
                                                "Delete a read replica cluster.",
                                                parents=[common_parser])

        # Sub-commands for top level `configure_read_replica` command: new, modify, delete
        all_parsers["configure_read_replica"] = configure_read_replica_parser
        configure_read_replica_subparsers = configure_read_replica_parser.add_subparsers(
                                                                    dest="parser", metavar="")
        configure_read_replica_subparsers.required = True
        for cmd, description in (
                ("new", "Configure a new read replica cluster."),
                ("modify", "Modify an existing new read replica cluster."),
                ("delete", "Delete an existing new read replica cluster.")):
            example = ""
            if EXAMPLE.get(cmd):
                example = Output.make_yellow("Examples") + ": \n" + EXAMPLE[cmd]
            subparser = configure_read_replica_subparsers.add_parser(cmd, help=description,
                                description=example, parents=[common_parser])
            parser_name = "configure_read_replica_" + cmd
            func = getattr(self, parser_name, None)
            subparser.set_defaults(func=func)
            all_parsers[parser_name] = subparser

        # Flags for sub-command: new
        for cmd in ("configure_read_replica_new",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--rf", help="Replication factor for read replica cluster.",
                metavar="")
            cur_parser.add_argument(
                "--data_placement_constraint", help="Placement policy for the read replica " +
                "cluster.", metavar="")

        # Flags for sub-command: modify
        for cmd in ("configure_read_replica_modify",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--rf", help="Replication factor for read replica cluster.",
                metavar="")
            cur_parser.add_argument(
                "--data_placement_constraint", help="Placement policy for the read replica " +
                "cluster.", metavar="")

        # Commands that can alter configuration file.
        for cmd in ("start",):
            cur_parser = all_parsers[cmd]
            cur_parser.add_argument(
                "--ycql_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--ysql_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--master_rpc_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--tserver_rpc_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--master_webserver_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--tserver_webserver_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--webserver_port", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--listen", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--advertise_address",
                help="IP address or local hostname on which {} will listen.".format(SCRIPT_NAME),
                metavar="")
            cur_parser.add_argument(
                "--join", help="IP address to which this process will join",
                metavar="")
            cur_parser.add_argument(
                "--read_replica", help="Use this flag to start a read replica node.",
                action="store_true", default=None)
            cur_parser.add_argument(
                "--secure", help="Start a YugabyteDB cluster in secure mode with encryption in " +
                "transit and password authentication enabled. No need to set a value for the " +
                "flag. Use --secure or --insecure flag to toggle security features on a " +
                "YugabyteDB cluster.",
                action="store_true", default=None)
            cur_parser.add_argument(
                "--insecure", help="Start a YugabyteDB cluster in an insecure mode without " +
                "encryption in transit and password authentication enabled. For non-production " +
                "use only, not to be used without firewalls blocking the internet traffic. No " +
                "need to set a value for the flag. Use --secure or --insecure flag to toggle " +
                "security features on a YugabyteDB cluster.", action="store_true", default=None)
            cur_parser.add_argument(
                "--certs_dir", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--daemon", choices=BOOL_CHOICES, help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--background", choices=BOOL_CHOICES, help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--callhome", choices=BOOL_CHOICES,
                help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--ui", choices=BOOL_CHOICES, metavar="",
                help="Toggle enabling or disabling webserver UI. Default true.")
            cur_parser.add_argument(
                "--ysql_enable_auth", choices=BOOL_CHOICES,
                help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--use_cassandra_authentication", choices=BOOL_CHOICES,
                help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--initial_scripts_dir", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--cloud_location", metavar="",
                help="Cloud location of the node in form of cloudprovider.region.zone")
            cur_parser.add_argument(
                "--fault_tolerance", metavar="",
                help="Determines the type of deployment of the cluster. Default is None.")

            # Hidden commands for development/advanced users
            cur_parser.add_argument(
                "--polling_interval", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--master_flags", help=argparse.SUPPRESS)
            cur_parser.add_argument(
                "--tserver_flags", help=argparse.SUPPRESS)


        if not sys.argv[1:]:
            parser.print_help()
            return

        args = parser.parse_args()
        self.validate_and_set_parent_configs(args)
        # Yugabyted command currently being processed is required for
        # generating the status string.
        self.configs.temp_data["yugabyted_cmd"] = args.parser

        log_dir = self.configs.saved_data.get("log_dir")
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)

        Output.log_dir = log_dir
        logging.basicConfig(
            level=logging.DEBUG, filemode="a",
            filename=os.path.join(log_dir, "{}.log".format(SCRIPT_NAME)),
            format="[%(filename)s " + args.parser + "] %(asctime)s %(levelname)s: %(message)s")

        Output.log("Running yugabyted command: '{}'".format(' '.join(sys.argv)))

        Output.log("cmd = {} using config file: {} (args.config={})".format(
            args.parser, self.conf_file, args.config))

        # Initialize the script path of openssl_proxy.sh
        OpenSSLProxy.init()

        # Initialize the binary path of ybadmin
        YBAdminProxy.init()

        self.validate_and_set_configs(args)

        # Initialize the binary path of ybadmin
        # TODO(Sanket): Clean up and refactor this file
        YBAdminProxy.set_certs_dir(self.configs.saved_data.get("master_flags"), \
            self.configs.saved_data.get("secure"), self.configs.saved_data.get("certs_dir"))

        try:
            args.func()
        except Exception as e:
            Output.print_out(
                "{} crashed. For troubleshooting, contact us on {} or check our FAQ at {}".format(
                    SCRIPT_NAME, Output.make_underline(SLACK_LINK),
                    Output.make_underline(HELP_LINK)))
            Output.log_error_and_exit(traceback.format_exc())

    def advertise_ip(self):
        bind_ip = self.configs.saved_data.get("advertise_address")
        return bind_ip if bind_ip != IP_ANY else IP_LOCALHOST

    def master_port(self):
       self.configs.saved_data.get("master_rpc_port")

    def first_install_init_auth(self):
        if self.get_failed_node_processes():
            Output.log_error_and_exit(
                "{} is not running.".format(SCRIPT_NAME))

        if self.setup_env_init.is_exists('YSQL_USER') or \
                self.setup_env_init.is_exists('YSQL_PASSWORD') or \
                self.setup_env_init.is_exists('YSQL_DB'):
            ysql_proxy = YsqlProxy(ip=self.advertise_ip(),
                            port=self.configs.saved_data.get("ysql_port"),
                            get_default_credentials=True)
            if retry_op(ysql_proxy.is_ysql_up):
                Output.log("Setting up custom credentials for YSQL...")
                self.setup_env_init.setup_ysql_credentials(ysql_proxy)

        if self.setup_env_init.is_exists('YCQL_USER') or \
                self.setup_env_init.is_exists('YCQL_PASSWORD') or \
                self.setup_env_init.is_exists('YCQL_KEYSPACE'):
            ycql_proxy = YcqlProxy(ip=self.advertise_ip(),
                            port=self.configs.saved_data.get("ycql_port"),
                            get_default_credentials=True)
            if retry_op(ycql_proxy.is_ycql_up):
                Output.log("Setting up custom credentials for YCQL...")
                self.setup_env_init.setup_ycql_credentials(ycql_proxy)

        if self.configs.temp_data.get("initial_scripts_dir"):
            init_scripts = os.path.abspath(self.configs.temp_data.get("initial_scripts_dir"))

            if os.path.exists(init_scripts):
                Output.log("Initialization scripts from the {} directory".format(init_scripts))

                sql_files = sorted([sql_file for sql_file in os.listdir(init_scripts) if (
                                sql_file.endswith('.sql'))])
                cql_files = sorted([cql_file for cql_file in os.listdir(init_scripts) if (
                                cql_file.endswith('.cql'))])

                ysql_proxy = YsqlProxy(ip=self.advertise_ip(),
                                port=self.configs.saved_data.get("ysql_port"))
                if sql_files and retry_op(ysql_proxy.is_ysql_up):
                    self.load_init_scripts(ysql_proxy, init_scripts, sql_files)

                ycql_proxy = YcqlProxy(ip=self.advertise_ip(),
                                port=self.configs.saved_data.get("ycql_port"))
                if cql_files and retry_op(ycql_proxy.is_ycql_up):
                    self.load_init_scripts(ycql_proxy, init_scripts, cql_files)

    def load_init_scripts(self, proxy_class, init_scripts_dir, files):
        files_path = []
        for name in files:
            files_path.append(os.path.join(init_scripts_dir, name))

        proxy_class.load_files(files_path)

class Configs(object):
    def __init__(self, config_file, base_dir):
        self.saved_data = {
            "data_dir": os.path.join(base_dir, "data"),
            "log_dir": os.path.join(base_dir, "logs"),
            "gen_certs_dir": os.path.join(base_dir, "generated_certs"),
            "master_rpc_port": DEFAULT_MASTER_RPC_PORT,
            "tserver_rpc_port": DEFAULT_TSERVER_RPC_PORT,
            "master_webserver_port": DEFAULT_MASTER_WEBSERVER_PORT,
            "tserver_webserver_port": DEFAULT_TSERVER_WEBSERVER_PORT,
            "ysql_port": DEFAULT_YSQL_PORT,
            "ycql_port": DEFAULT_YCQL_PORT,
            "advertise_address": "",
            "webserver_port": DEFAULT_WEBSERVER_PORT,
            "yugabyted_ui_port": DEFAULT_YUGABYTED_UI_PORT,
            "universe_uuid": str(uuid.uuid4()),
            "node_uuid": str(uuid.uuid4()),
            "tserver_uuid": str(uuid.uuid4()).replace("-", ""),
            "master_uuid": str(uuid.uuid4()).replace("-", ""),
            "placement_uuid": str(uuid.uuid4()),
            "polling_interval": "5",
            "callhome": DEFAULT_CALLHOME,
            "master_flags": "",
            "tserver_flags": "",
            "join": "",
            "ysql_enable_auth": False,
            "use_cassandra_authentication": False,
            "cloud_provider": DEFAULT_CLOUD_PROVIDER,
            "cloud_region": DEFAULT_CLOUD_REGION,
            "cloud_zone": DEFAULT_CLOUD_ZONE,
            "fault_tolerance": DEFAULT_START_FAULT_TOLERANCE,
            "secure": False,
            "insecure": True,
            "certs_dir": os.path.join(base_dir, "certs"),
            "ca_cert_file_path": "",
            "database_password": None,
            "current_masters": "",
            "ui": True,
            "dns_enabled": False,
            "read_replica": False,
        }
        # Used to store data specific to certain functions that we don't want to save.
        self.temp_data = {
            "demo_db": DEFAULT_DEMO_DATABASE,
            "background": True,
            "initial_scripts_dir": "",
            "collect_logs_stdout": False,
            "constraint_value": "",
            "replication_factor":"",
            "fault_tolerance": DEFAULT_FAULT_TOLERANCE,
            "hostnames": "",
            "enable_encrypt_at_rest": False,
            "disable_encrypt_at_rest": False,
            "admin_command": "",
            "admin_operation_master_addresses":"",
            "rr_data_placement_constraint": "",
            "rr_replication_factor": "",
            "collect_at_dir": os.path.join(os.path.expanduser("~"), "yugabyte_collected_logs")
        }
        self.config_file = config_file

    # Saves current configs to config file.
    def save_configs(self):
        with open(self.config_file, "w+") as f:
            json.dump(self.saved_data, f, indent=4)

    # Custom parser for reading config file.
    @staticmethod
    def parse_config_file(config_file, base_dir):
        configs = Configs(config_file, base_dir)
        if os.path.isfile(config_file):
            try:
                with open(config_file) as f:
                   configs.saved_data.update(json.load(f))
            except ValueError as e:
                Output.log_error_and_exit(
                    "Failed to read config file {}: {}".format(config_file, str(e)))

        paths = ["data_dir", "log_dir", "certs_dir", "gen_certs_dir", "ca_cert_file_path"]
        for k, v in configs.saved_data.items():
            if v is not None and k in paths:
                if "$" in v:
                    configs.saved_data[k] = os.path.expanduser(os.path.expandvars(v))
                else:
                    configs.saved_data[k] = os.path.expanduser(v)
        return configs

    @staticmethod
    def get_brew_config():
        # hack alert: we are using the cellar dir name to identify a brew install
        if ("darwin" == sys.platform and
            os.path.realpath(sys.argv[0]).lower().find('cellar') >= 0 and
            os.path.exists(BREW_CONF_FILE)):
            return BREW_CONF_FILE

        return None

    # Returns information about demo databases.
    @staticmethod
    def get_demo_info():
        return {
            "retail": {
                "files": ("schema.sql", "products.sql", "users.sql", "reviews.sql", "orders.sql"),
                "output": "    Database: yb_demo_retail\n"
                        "    |_ users\n"
                        "    |_ products\n"
                        "    |_ orders\n"
                        "    |_ reviews\n\n",
                "examples": "# JOINS (find user details of orders):\n"
                        "    %s users.id, users.name, users.email, orders.id, orders.total\n"
                        "        %s orders %s users %s orders.user_id=users.id\n"
                        "        %s 10;\n\n" % tuple([Output.make_cyan(kw) for kw in (
                            "SELECT", "FROM", "INNER JOIN", "ON", "LIMIT")])
            },
            "northwind": {
                "files": ("northwind_ddl.sql", "northwind_data.sql"),
                "output": "",
                "examples": "# JOINS (find customer details for orders):\n"
                        "   %s c.customer_id, c.company_name, o.order_id, o.order_date\n"
                        "       %s orders o %s customers c %s o.customer_id=c.customer_id\n"
                        "       %s 10;\n\n" % tuple([Output.make_cyan(kw) for kw in (
                            "SELECT", "FROM", "INNER JOIN", "ON", "LIMIT")])
            },
            "club": {
                "files": ("clubdata_ddl.sql", "clubdata_data.sql"),
                "output": "",
                "examples": ""
            },
            "sports": {
                "files": (
                    "sportsdb_tables.sql", "sportsdb_fks.sql",
                    "sportsdb_indexes.sql", "sportsdb_inserts.sql"),
                "output": "",
                "examples": ""
            }
        }


class ProcessManager(object):
    def __init__(self, name, cmd, log_dir, data_dir, process_log_dir=""):
        self.name = name
        self.cmd = cmd
        self.log_dir = log_dir
        self.data_dir = data_dir
        self.pidfile = os.path.join(self.data_dir, "{}.pid".format(name))
        self.process = None
        self.start_time = None
        self.process_log_dir = process_log_dir

    # Start process. Creates pidfile and corresponding output logs.
    def start(self):
        Output.log("About to start {} with cmd {}".format(self.name, " ".join(self.cmd)))
        out_log = os.path.join(self.log_dir, "{}.out".format(self.name))
        err_log = os.path.join(self.log_dir, "{}.err".format(self.name))
        with open(out_log, "a") as out_log, open(err_log, "a") as err_log:
            self.process = subprocess.Popen(
                self.cmd, stdout=out_log, stderr=err_log, preexec_fn=self.set_rlimits)
            self.start_time = time.time()
        self.write_pid(self.process.pid)

        # Add symlink to the logs from log directory.
        log_path = os.path.join(self.log_dir, self.name)
        if self.process_log_dir and not os.path.exists(log_path):
            try:
                os.symlink(self.process_log_dir, log_path)
            except OSError as e:
                Output.log(
                    "Failed to create symlink from {} to {}".format(self.process_log_dir, log_path),
                    logging.ERROR)

    # Records given pid in pidfile.
    # TODO: Redirect YW logs to yugabyte-logs
    def write_pid(self, pid):
        with open(self.pidfile, "w+") as pid_file:
            pid_file.write(str(pid))
            Output.log("{} started running with PID {}.".format(self.name, pid))

    # Returns pid of this process if it's running.
    def get_pid(self):
        if os.path.exists(self.pidfile):
            if self.process:
                return self.process.pid
            else:
                with open(self.pidfile, "r") as f:
                    try:
                        pid = int(f.readline())
                    except ValueError as e:
                        Output.log(
                            "Could not parse int PID from {}. Deleting file.".format(self.pidfile),
                            logging.DEBUG)
                        self.delete_pidfile()
                        return None
                command = ProcessManager.get_command(pid)
                if command and self.name.encode('utf8') in command:
                    return pid

            Output.log(
                "Pidfile {} was not properly deleted."
                "Contained PID {}. Deleting file.".format(self.pidfile, pid), logging.DEBUG)
            self.delete_pidfile()
        return None

    # Kills self.process if it exists.
    def kill(self):
        err = None
        pid = None
        if self.process:
            self.process.kill()
        else:
            pid = self.get_pid()
            if pid:
                try:
                    os.kill(pid, SIGTERM)
                except OSError as e:
                    return (e, pid)
        self.delete_pidfile()
        return (err, pid)

    # Raise RetryableError if pidfile still exists.
    def is_proc_running(self, pid):
        if (os.path.exists(self.pidfile) or ProcessManager.get_command(pid) != ""):
            raise RetryableError()
        else:
            return True

    # Function that waits until pidfile no longer exists.
    def wait_until_stop(self, pid):
        def temp_func():
            return self.is_proc_running(pid)
        retry_op(temp_func, timeout=60)
        return

    # Delete corresponding pidfile for this process.
    def delete_pidfile(self):
        if os.path.exists(self.pidfile):
            try:
                os.remove(self.pidfile)
            except OSError as e:
                if os.path.exists(self.pidfile):
                    Output.log(
                        "Failed to delete {}.".format(self.pidfile), level=logging.ERROR)
        self.start_time = None

    # Check fatal errors in fatal/error logs, if any. Overwritten in YBProcessManager
    def check_fatals(self):
        pass

    # Returns process status.
    def is_running(self):
        self.check_fatals()
        return self.get_pid() and self.process and self.process.poll() is None

    # Checks resource settings for current shell. Prints warning if requirements aren't met.
    def set_rlimits(self, print_info=False):
        rlim_max = resource.RLIM_INFINITY
        # TODO: Figure out what specs are recommended. max_user_processes is problematic.
        # https://github.com/yugabyte/yugabyte-db/issues/2818
        recommended_resources = {
            # "RLIMIT_FSIZE": (rlim_max, "file_size"),
            # "RLIMIT_MEMLOCK": (rlim_max, "max_locked_memory"),
            # "RLIMIT_AS": (rlim_max, "max_memory_size"),
            "RLIMIT_NOFILE": (1048576, "open_files"),
            # "RLIMIT_CPU": (rlim_max, "cpu_time"),
            "RLIMIT_NPROC": (MAX_PROC[OS_NAME], "max_user_processes"),
            # "RLIMIT_VMEM": (rlim_max, "virtual_memory"),
        }

        # If the current platform does not support the resource,
        # it won't be defined in the resource module.
        failed = []
        for res, (min_val, ulimit) in recommended_resources.items():
            if not hasattr(resource, res):
                continue
            # Check soft limit, not hard limit.
            key = getattr(resource, res)
            soft_lim, hard_lim = resource.getrlimit(key)
            if soft_lim != rlim_max and (soft_lim < min_val or min_val == rlim_max):
                try:
                    resource.setrlimit(key, (min_val, hard_lim))
                    if print_info:
                        Output.log("Changed {} from {} to {}".format(res, soft_lim, min_val))
                except ValueError as e:
                    failed.append((ulimit, soft_lim, min_val))
                    if print_info:
                        Output.log(
                            "Error changing {} from {} to {}: {}".format(
                                res, soft_lim, min_val, e),
                            logging.ERROR)

        if failed and print_info:
            return list(zip(*failed))[0]

    # Returns the command that was run with the input pid.
    @staticmethod
    def get_command(pid):
        try:
            return subprocess.check_output(["ps", "-p", str(pid), "-o", "command="])
        except subprocess.CalledProcessError as e:
            return ""

    # Returns if process called name is running.
    @staticmethod
    def is_process_running(name, pid_dir):
        return ProcessManager(name, cmd="", log_dir="", data_dir=pid_dir).get_pid() is not None


# Class for managing yugabyted process components - e.g. pidfiles and lockfiles.
# Maybe this class can take over the ControlScript.start_processes functionality?
class ScriptProcessManager(ProcessManager):
    def __init__(self, log_dir, data_dir):
        super(ScriptProcessManager, self).__init__(SCRIPT_NAME, "", log_dir, data_dir)
        # Used to retrieve status of daemon process. When daemon successfully initializes,
        # it will put a value here which can be checked on.
        self.daemon_success = multiprocessing.Queue()

    def start(self):
        return

    def is_running(self):
        pid = self.get_pid()
        # In certain scenarios like docker, the pid of a previously crashed docker run
        # is going to be 1, which is the same as ours and looks like the previous run is
        # still ongoing. The getpid check below covers that case.
        # In the long run, the plan is to move to something like flock instead.
        return pid is not None and pid != os.getpid()


class YBProcessManager(ProcessManager):
    def __init__(self, name, cmd, log_dir, data_dir):
        data_log_path = "{}/yb-data/{}/logs".format(data_dir, name)
        super(YBProcessManager, self).__init__(name, cmd, log_dir, data_dir, data_log_path)
        self.error_log = "{}/yb-{}.ERROR".format(data_log_path, name)


    def start(self):
        # Remove old logs as timestamped logs should have already been created.
        self.remove_error_logs()

        super(YBProcessManager, self).start()

    def remove_error_logs(self):
        if os.path.isfile(self.error_log):
            os.remove(self.error_log)

    def check_fatals(self):
        # Error logs contain port information, but fatal logs don't.
        address_error_1 = "Could not start on address"
        address_error_2 = "Error binding socket to "
        address_error_3 = "Is another postmaster already running on port "
        if os.path.isfile(self.error_log):
            with open(self.error_log) as log:
                for line in log.readlines():
                    if address_error_1 in line:
                        err_msg = line.split(address_error_1)
                        # Try to find address, otherwise log entire error message.
                        if len(err_msg) == 2:
                            err_msg = err_msg[1]
                        else:
                            err_msg = line
                        Output.log_error_and_exit(
                            "Failed to bind to address: {}".format(err_msg))
                    elif address_error_2 in line:
                        err_msg = line.split(address_error_2)[1]
                        address = err_msg.split()[0]
                        Output.log_error_and_exit(
                            "Failed to bind to address: {}".format(address))
                    elif address_error_3 in line:
                        err_msg = line.split(address_error_3)
                        # Try to find address, otherwise log entire error message.
                        if len(err_msg) == 2:
                            err_msg = err_msg[1].split()[0]
                            Output.log_error_and_exit(
                                "Failed to bind to port: {}.".format(err_msg))
                        else:
                            Output.log_error_and_exit(
                                "Failed to bind to address: {}".format(err_msg))


class Diagnostics(object):
    first_install = None
    first_run_secs = None

    def __init__(self, configs):
        self.configs = configs

    # Collects data.
    def get_data(self, processes):
        payload = {
            "data_dir_size": self.get_dir_size(self.configs.saved_data.get("data_dir")),
            "num_cpus": multiprocessing.cpu_count(),
            "master_flags": self.configs.saved_data.get("master_flags"),
            "tserver_flags": self.configs.saved_data.get("tserver_flags"),
            "is_docker" : str(os.path.exists("/.dockerenv"))
        }
        if Diagnostics.first_install is not None:
            payload['first_install'] = str(Diagnostics.first_install)
            Diagnostics.first_install = None
        if Diagnostics.first_run_secs is not None:
            payload['first_run_secs'] = str(int(Diagnostics.first_run_secs))
            Diagnostics.first_run_secs = None
        for p in processes.values():
            payload["{}_status".format(p.name)] = "UP" if p.is_running() else "DOWN"
            if p.start_time:
                payload["{}_start_time".format(p.name)] = p.start_time

        bind_ip = self.configs.saved_data.get("advertise_address")
        advertise_ip = bind_ip if bind_ip != IP_ANY else IP_LOCALHOST

        master_addrs = "{}:{}".format(
            advertise_ip, self.configs.saved_data.get("master_rpc_port"))
        # TODO: This is going to change for multi-node.
        cur_master_addr = master_addrs
        data = {
            "cluster_uuid": self.configs.saved_data.get("universe_uuid"),
            "node_uuid": self.configs.saved_data.get("node_uuid"),
            "server_type": SCRIPT_NAME,
            "timestamp": int(time.time()),
            "payload": payload
        }
        return json.dumps(data)

    def get_dir_size(self, dirname):
        size = 0
        for path, _, files in os.walk(dirname):
            for f in files:
                filepath = os.path.join(path, f)
                # Check that the file is not a symlink
                if os.path.isfile(filepath):
                    size += os.path.getsize(filepath)
        return size


# Proxy for parsing output from yb-admin commands.
class YBAdminProxy(object):
    cmd_args = []

    @staticmethod
    def init():
        YBAdminProxy.cmd_args.append(find_binary_location("yb-admin"))

    @staticmethod
    def set_certs_dir(master_flags, secure, certs_dir):
        # If the user is attempting to use TLS, let's point yb-admin to
        # the same certs dir as the master
        if secure:
            YBAdminProxy.cmd_args.append("--certs_dir_name={}".format(certs_dir))
        elif master_flags:
            flags_list = master_flags.split(",")
            if 'use_node_to_node_encryption=true' not in flags_list:
                return
            certs_dir_name = [y for y in
                [re.match('certs_dir=(.*)', x) for x in flags_list]
                if y is not None]
            if not certs_dir_name[0]:
                raise RuntimeError("use_node_to_node_encryption=true must "
                                "be accompanied by a certs_dir setting")
            YBAdminProxy.cmd_args.append('--certs_dir_name={}'.format(certs_dir_name[0].group(1)))

    @staticmethod
    def add_master(master_addrs, new_master_ip, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--init_master_addrs", master_addrs,
              "change_master_config", "ADD_SERVER", new_master_ip, "7100"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (0 == ret_code)

    @staticmethod
    def remove_master(master_addrs, old_master, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--init_master_addrs", master_addrs,
              "change_master_config", "REMOVE_SERVER", old_master, "7100"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (0 == ret_code)

    @staticmethod
    def set_rf(master_addrs, rf, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--init_master_addrs", master_addrs,
                "modify_placement_info", "cloud1.datacenter1.rack1", str(rf) ]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (0 == ret_code)

    @staticmethod
    # Returns [ (uuid, ip:port, role) ] for each master
    def get_masters(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--master_addresses", master_addrs, "list_all_masters"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        if 0 != ret_code or len(out.splitlines()) <= 1:
            return []
        masters = [ line.split() for line in out.splitlines()[1:] ]
        return [ (master[0], master[1], master[3]) for master in masters ]

    # Returns list[tserver uuid] reported by yb-master.
    @staticmethod
    def get_tservers(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--master_addresses", master_addrs,
                "list_all_tablet_servers"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        if 0 != ret_code or len(out.splitlines()) <= 1:
            return None
        return [ line.split()[0] for line in out.splitlines()[1:] ]

    # Returns the cluster_uuid for this universe
    @staticmethod
    def get_cluster_uuid(master_addrs):
        cluster_config = YBAdminProxy.get_cluster_config(master_addrs)
        if cluster_config:
            return cluster_config['clusterUuid']
        else:
            return None

    # Returns the rf for this universe
    @staticmethod
    def get_cluster_rf(master_addrs):
        cluster_config = YBAdminProxy.get_cluster_config(master_addrs)
        if cluster_config:
            if "replicationInfo" in cluster_config:
                return cluster_config["replicationInfo"].get("liveReplicas").get("numReplicas")
            else:
                return 1
        else:
            return None

    # Returns the cluster config for this universe
    @staticmethod
    def get_cluster_config(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["--master_addresses", master_addrs, "get_universe_config"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        if ret_code:
            return None
        try:
            return json.loads(out)
        except Exception:
            return None

    # Returns node_uuid by finding the UUID corresponding to current master's IP
    @staticmethod
    def get_node_uuid(master_addrs, cur_master_addr):
        cmd = YBAdminProxy.cmd_args + ["--init_master_addrs", master_addrs, "list_all_masters"]
        out, err, ret_code = run_process(cmd)
        if ret_code:
            return None
        for line in out.splitlines()[1:]:
            master_uuid, rpc_addr, _, _ = line.split()
            if rpc_addr == cur_master_addr:
                return master_uuid
        return None

    # Sets placement info
    @staticmethod
    def modify_placement_info(master_addrs, placement_locations, placement_uuid,
                              replication_factor = "3", timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs, "modify_placement_info",
                placement_locations, replication_factor, placement_uuid]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    @staticmethod
    def set_preferred_zones(master_addrs, priority_info, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses",
                                       master_addrs, "set_preferred_zones"] + priority_info
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return ret_code == 0

    # Sets placement info of read replicas
    @staticmethod
    def set_read_replica_placement(master_addrs, placement_locations, placement_uuid,
                              replication_factor, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
                        "add_read_replica_placement_info",placement_locations,
                        replication_factor, placement_uuid]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Modify placement info of read replicas
    @staticmethod
    def modify_read_replica_placement(master_addrs, placement_locations, placement_uuid,
                              replication_factor, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
                        "modify_read_replica_placement_info",placement_locations,
                        replication_factor, placement_uuid]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Delete placement info of read replicas
    @staticmethod
    def delete_read_replica_placement(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
                        "delete_read_replica_placement_info"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Copy universe key to all masters for encryption at rest
    @staticmethod
    def copy_key_to_masters(master_addrs, key_id, key_path, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
            "add_universe_key_to_all_masters", key_id, key_path]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Check if universe key has been copied to all masters for encryption at rest
    @staticmethod
    def check_key_in_masters(master_addrs, key_id, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
            "all_masters_have_universe_key_in_memory", key_id]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        if 0 != ret_code:
            return []
        out = out.splitlines()
        return out

    # Enable encryption at rest by start using the key in masters
    @staticmethod
    def enable_encryption_using_key(master_addrs, key_id, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs,
            "rotate_universe_key_in_memory", key_id]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Disable encryption at rest
    @staticmethod
    def disable_encryption(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs, "disable_encryption"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        return (ret_code == 0)

    # Check if universe key has been copied to all masters for encryption at rest
    @staticmethod
    def check_encryption(master_addrs, timeout=10):
        cmd = YBAdminProxy.cmd_args + ["-master_addresses", master_addrs, "is_encryption_enabled"]
        out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
        if 0 != ret_code:
            return None
        return out

    # Passthrough method for all the yb-admin commands
    # @staticmethod
    # def call_yb_admin_command(master_addrss, command, timeout=10):
    #     cmd =  YBAdminProxy.cmd_args + ["-master_addresses", master_addrss, command]
    #     out, err, ret_code = run_process(cmd, timeout=timeout, log_cmd=True)
    #     if 0 != ret_code:
    #         return None
    #     return out


# Proxy for ysqlsh commands.
class YsqlProxy(object):
    def __init__(self, ip, port, path=None,
                    get_default_credentials=False):
        if path is None:
            path = find_binary_location(YUGABYTE_API_CLIENT_PROGRAMS["ysql"])
        self.setup_env_init = EnvBasedCredentials()
        self.username, self.password, self.db = self.setup_env_init.get_ysql_credentials(
                                                    get_default_credentials)
        self.cmd_with_password = [path, "postgresql://{}:{}@{}:{}".format(self.username,
                self.password, ip, port)]
        self.cmd_without_password = [path, "-h", str(ip), "-p", str(port)]
        env_var = os.environ.copy()
        env_var["PGUSER"] = self.username
        env_var["PGPASSWORD"] = self.password
        env_var["PGDATABASE"] = self.db

        self.env = env_var

    # Starts interactive YSQL shell.
    def connect(self, cmd):
        env_var = os.environ.copy()
        env_var["PGUSER"] = self.username
        Output.log("Connecting to the YSQL database using command: {}" \
                             .format(cmd))
        shell = subprocess.Popen(cmd, env=env_var)
        while True:
            try:
                shell.communicate()
            except KeyboardInterrupt:
                continue
            break

    def connect_without_password(self, db=None):
        if db is None:
            db = self.db

        cmd = self.cmd_without_password + ["-d", db]
        self.connect(cmd)

    def connect_with_password(self, db=None):
        if db is None:
            db = self.db

        cmd=list(self.cmd_with_password)
        cmd[1] +="/"+db
        self.connect(cmd)

    # Checks if db exists.
    # Note that this will return false if ysqlsh can't connect, even if db exists.
    def db_exists(self, db):
        cmd = self.cmd_with_password + ["-q", "-c", "\\t", "-c",
            "select datname from pg_catalog.pg_database where datname='{}'".format(db)]
        return run_process_checked(cmd=cmd, env_vars=self.env).strip() == db

    # Creates specified db.
    def create_db(self, db):
        cmd = self.cmd_with_password + ["-c", "create database \"{}\"".format(db)]
        run_process_checked(cmd=cmd, env_vars=self.env)

    # Deletes specified db.
    def drop_db(self, db):
        cmd = self.cmd_with_password + ["-c", "drop database {}".format(db)]
        run_process_checked(cmd=cmd, env_vars=self.env)

    # Runs ysqlsh with specified files.
    def load_files(self, filepaths, db=None):
        cmd = list(self.cmd_with_password)
        env = self.env
        if db:
            env['PGDATABASE'] = db
        else:
            env['PGDATABASE'] = self.db
        for path in filepaths:
            cmd.extend(["-f", path])
        run_process_checked(cmd=cmd, log_cmd=False, env_vars=env)

    # Check user exists
    # Note that this will return false if ysqlsh can't connect, even if user exists.
    def user_exists(self, ysql_username):
        cmd = self.cmd_with_password + \
                ["-c", "select rolname from pg_catalog.pg_roles where rolname='{}'".
                format(ysql_username)]
        run_process_checked(cmd=cmd, env_vars=self.env).strip() == ysql_username

    # Create specified superuser
    def create_user(self, ysql_username, ysql_password):
        cmd = self.cmd_with_password + \
                ["-c", "create role \"{}\" with LOGIN SUPERUSER password '{}';".
                format(ysql_username, ysql_password)]
        run_process_checked(cmd=cmd, log_cmd=False, env_vars=self.env)

    # Delete specified user
    def delete_user(self, username, password, user_to_delete=DEFAULT_YSQL_USER):
        cmd = self.cmd_with_password + ["-c", "drop role {};".format(user_to_delete)]
        env = self.env
        env['PGUSER'] = username
        env['PGPASSWORD'] = password
        run_process_checked(cmd=cmd, env_vars=env)

    # Update specified user's password
    def update_password(self, new_password, user_to_update=DEFAULT_YSQL_USER):
        cmd = self.cmd_with_password + \
            ["-c", "alter role {} password '{}';".format(user_to_update, new_password)]
        run_process_checked(cmd=cmd, log_cmd=False, env_vars=self.env)

    # Try to update specified user's password otherwise throw Retryable Error
    def try_update_password(self, new_password, user_to_update=DEFAULT_YSQL_USER):
        cmd = self.cmd_with_password + \
            ["-c", "alter role {} password '{}';".format(user_to_update, new_password)]

        encrypted_password = "*" * len(new_password)
        encrypted_cmd = self.cmd_with_password + \
            ["-c", "alter role {} password '{}';".format(user_to_update, encrypted_password)]

        out, err, retcode = run_process(cmd=cmd, encrypted_cmd=encrypted_cmd, log_cmd=True,
                env_vars=self.env)
        if (retcode != 0):
            raise RetryableError

    # Change specified DB's owner
    def db_owner(self, db, new_owner):
        cmd = self.cmd_with_password + \
            ["-c", "alter database \"{}\" owner to \"{}\";".format(db, new_owner)]
        run_process_checked(cmd=cmd, env_vars=self.env)

    # Check YSQL is UP
    def is_ysql_up(self):
        cmd = self.cmd_with_password + ["-c", "\\conninfo"]
        out, err, _ = run_process(cmd=cmd, log_cmd=True, env_vars=self.env)
        if err:
            Output.log("Unable to connect using YSQL. Error: {}".format(err))
            raise RetryableError()
        else:
            Output.log("YSQL Connection Info - {}".format(out))
            return True

# Proxy for ycqlsh commands.
class YcqlProxy(object):
    def __init__(self, ip, port, path=None,
                    get_default_credentials=False, secure=False):
        if path is None:
            path = find_binary_location(YUGABYTE_API_CLIENT_PROGRAMS["ycql"])
        self.cmd = [path, str(ip), str(port)]
        self.setup_env_init = EnvBasedCredentials()
        self.username, self.password, self.keyspace = self.setup_env_init.get_ycql_credentials(
                                                        get_default_credentials)
        self.password_authentication=secure

        if secure:
            self.cmd.append("--ssl")

    # Starts interactive YCQL shell.
    def connect(self, cmd):
        Output.log("Connecting to the YCQL keyspace using command: {}" \
                             .format(cmd))
        shell = subprocess.Popen(cmd)
        while True:
            try:
                shell.communicate()
            except KeyboardInterrupt:
                continue
            break

    def connect_without_password(self):
        cmd = list(self.cmd)
        if self.password_authentication:
            cmd.extend(["-u", self.username])
        if self.keyspace is not None:
            cmd.extend(["-k", self.keyspace])
        self.connect(cmd)

    def connect_with_password(self):
        cmd = list(self.cmd)
        if self.username:
            cmd.extend(["-u", self.username])
        if self.keyspace is not None:
            cmd.extend(["-k", self.keyspace])
        cmd.extend(["-p", self.password])
        self.connect(cmd)

    # Check user exists
    # Note that this will return false if ycqlsh can't connect, even if user exists.
    def user_exists(self, ycql_username):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
            "SELECT role FROM system_auth.roles WHERE role='{}';".format(ycql_username)]
        return run_process_checked(cmd).strip() == ycql_username

    # Create specified superuser
    def create_user(self, ycql_username, ycql_password):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
                "CREATE ROLE \"{}\" WITH PASSWORD = '{}' AND LOGIN = true AND SUPERUSER = true;".
                format(ycql_username, ycql_password)]
        run_process_checked(cmd, log_cmd=False)

    # Delete specified user
    def delete_user(self, username, password, user_to_delete=DEFAULT_YCQL_USER):
        cmd = self.cmd + ["-u", username, "-p", password, "-e",
                "DROP ROLE IF EXISTS {};".format(user_to_delete)]
        run_process_checked(cmd)

    # Update specified user's password
    def update_password(self, new_password, user_to_update=DEFAULT_YCQL_USER):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
                "ALTER ROLE {} WITH PASSWORD = '{}';".format(user_to_update, new_password)]
        run_process_checked(cmd, log_cmd=False)

    # Try to update specified user's password otherwise throw Retryable Error
    def try_update_password(self, new_password, user_to_update=DEFAULT_YCQL_USER):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
                "ALTER ROLE {} WITH PASSWORD = '{}';".format(user_to_update, new_password)]

        encrypted_password = "*" * len(new_password)
        encrypted_cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
                "ALTER ROLE {} WITH PASSWORD = '{}';".format(user_to_update, encrypted_password)]

        out, err, retcode = run_process(cmd=cmd, encrypted_cmd=encrypted_cmd, log_cmd=True,
                env_vars=os.environ.copy())
        if (retcode != 0):
            raise RetryableError

    # Check keyspace exists
    # Note that this will return false if ycqlsh can't connect, even if keyspace exists.
    def keyspace_exists(self, keyspace):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
            "SELECT keyspace_name FROM system_schema.keyspaces WHERE keyspace_name='{}';".
            format(keyspace)]
        return run_process_checked(cmd).strip() == keyspace

    # Create specified keyspace
    def create_keyspace(self, keyspace):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e",
                "CREATE KEYSPACE \"{}\";".format(keyspace)]
        run_process_checked(cmd)

    # Runs ycqlsh with specified files.
    # Example:
    # 1. bin/ycqlsh -f directory/a.ycql
    # 2. If environment variables exists: bin/ycqlsh -u user -p password -f directory/b.ycql
    def load_files(self, filepaths):
        cmd = self.cmd
        cmd.extend(["-u", self.username, "-p", self.password])
        for path in filepaths:
            cmd.extend(["-f", path])
        run_process_checked(cmd=cmd, log_cmd=False)

    # Check YCQL is UP
    def is_ycql_up(self):
        cmd = self.cmd + ["-u", self.username, "-p", self.password, "-e", "SHOW HOST"]
        out, err, _ = run_process(cmd, log_cmd=True)
        if err:
            Output.log("Unable to connect using YCQL. Error: {}".format(err))
            raise RetryableError()
        else:
            Output.log("YCQL Connection Info - {}".format(out))
            return True


# Proxy for creating ssl certificates and keys using openssl
class OpenSSLProxy(object):
    cmd_args = []

    @staticmethod
    def init(path=None):
        if path is None:
            path = find_binary_location("openssl_proxy.sh")

        OpenSSLProxy.cmd_args = [path]

    # Generate root ca certificates
    @staticmethod
    def generate_root_ca_certs(root_certs_dir, timeout=60):
        cmd = OpenSSLProxy.cmd_args + ['generate-ca', '-rcp', root_certs_dir]
        out, err, ret = run_process(cmd, timeout=timeout, log_cmd=True)

        return (0 == ret)

    # Generate node server certificates
    @staticmethod
    def generate_node_server_certs(root_certs_dir, server_cert_dir, hostname, timeout=60):
        cmd = OpenSSLProxy.cmd_args + ['generate-server-cert', '-rcp', root_certs_dir,
            '-scp', server_cert_dir, '-hn', hostname]
        out, err, ret = run_process(cmd, timeout=timeout, log_cmd=True)

        return (0 == ret)

    # Generate universe key for encryption-at-rest
    @staticmethod
    def generate_key(key_dir, keyname, timeout=60):
        cmd = OpenSSLProxy.cmd_args + ['generate-key', '-kp', key_dir, '-kn', keyname]
        out, err, ret = run_process(cmd, timeout=timeout, log_cmd=True)

        return (0 == ret)


# Currently unused. Useful for getting diagnostics that are available only through logs.
class LogAnalyzer(object):
    unsupported_error = "not supported yet"
    def __init__(self, logfile):
        self.logfile = logfile
        # Flag to stop tailing the logfile.
        self.kill_thread = False
        self.unsupported_commands = []

    def analyze(self):
        lines = self.tail()
        for line in lines:
            if LogAnalyzer.unsupported_error in line:
                # Get the command logged right before error message
                cmd = line.split("not supported yet")[0].split()[-1]
                self.unsupported_commands.append(cmd)

    # Generator that continually returns last line of logfile.
    def tail(self):
        with open(self.logfile) as open_file:
            open_file.seek(0, 2)
            while not self.kill_thread:
                line = open_file.readline()
                if not line:
                    time.sleep(0.1)
                    continue
                yield line

# Manages API calls to YW.
class YugaWareProxy(object):
    def __init__(self, ip, webserver_port, univ_name="local-universe"):
        self.univ_name = univ_name
        self.api_token_secure = ""
        self.api_token_insecure = ""
        self.cust_uuid = ""
        self.url = "http://{}:{}/api/v1".format(ip, webserver_port)

    # Retrieves permanent api_token. Returns error, if any.
    def login(self):
        try:
            target = "{}/login".format(self.url)
            headers = {
                "Content-Type": "application/json",
            }
            data = urlencode({"email": "admin", "password": "admin"})
            req = Request(target, data=data.encode('utf8'))
            resp = urlopen(req)
            session_data = json.loads(resp.read())
            auth_token = session_data["authToken"]
            self.cust_uuid = session_data["customerUUID"]
            # Auth token will expire, so get API token instead.
            target = "{}/customers/{}/api_token".format(self.url, self.cust_uuid)
            headers = {
                "X-Auth-Token": auth_token,
            }
            req = Request(target, headers=headers)
            req.get_method = lambda: "PUT"
            resp = urlopen(req)
            api_data = json.loads(resp.read())
            self.api_token_secure = api_data["apiToken"]
            return None
        except (ValueError, HTTPError, URLError, KeyError) as e:
            return "Failed to log into webserver. {}".format(e)

    # Attempts insecure login. Returns error, if any.
    def insecure_login(self):
        try:
            target = "{}/insecure_login".format(self.url)
            req = Request(target)
            resp = urlopen(req)
            session_data = json.loads(resp.read())
            self.api_token_insecure = session_data["apiToken"]
            self.cust_uuid = session_data["customerUUID"]
            return None
        except (ValueError, HTTPError, URLError, KeyError) as e:
            return "Failed to log into webserver. {}".format(e)

    # Import local universe into YW. Returns error, if any.
    def import_universe(self, master_address, master_rpc_port, universeUUID):
        target = "{}/customers/{}/universes/import".format(self.url, self.cust_uuid)
        headers = {
            "X-AUTH-YW-API-TOKEN": self.api_token_secure,
            "Content-Type": "application/json",
        }
        data = {
            "cloudProviderType": "other",
            "currentState": "BEGIN",
            "masterAddresses": "{}".format(master_address),
            "universeName": self.univ_name,
            "universeUUID": universeUUID,
            "singleStep": "true",
        }

        Output.log("Importing Yugabyte into webserver...")
        try:
            Output.log("Importing master.", logging.DEBUG)
            req = Request(target, headers=headers, data=json.dumps(data).encode('utf8'))
            resp = json.loads(urlopen(req).read())
            checks = resp.get("checks")
            Output.log("Import master payload: req: {}, resp: {}".format(req, resp))
            if universeUUID != resp.get("universeUUID"):
                Output.log(
                    "Failed to import local universe into webserver: invalid uuid: {}".format(resp),
                    logging.ERROR)
            # Node exporter does not matter for local universes and will fail on import.
            if "node_exporter_ip_error_map" in checks:
                del[checks["node_exporter_ip_error_map"]]
            if checks and not all(check == 'OK' for check in checks.values()):
                Output.log(
                    ("Failed to import local universe into webserver, "
                     "checks failed: {}").format(resp))

        except (ValueError, HTTPError, URLError, KeyError) as e:
            return "Failed to import local universe into YW with payload: {}.\n" \
                "Got error: {}".format(data, e)

        Output.log("Import completed.")
        return None

    # Disables/hides paid services on UI.
    def set_landing_page(self, universe_uuid):
        target = "{}/customers/{}/features".format(self.url, self.cust_uuid)
        headers = {
            "X-AUTH-YW-API-TOKEN": self.api_token_secure,
            "Content-Type": "application/json",
        }
        data = {
            "features": {
                "main": {
                    "landing_page": "universes/{}".format(universe_uuid),
                    "universe_list": "disabled"
                }
            }
        }

        err_msg = None
        Output.log("Setting UI landing page...")
        try:
            req = Request(target, headers=headers, data=json.dumps(data).encode('utf8'))
            req.get_method = lambda: "PUT"
            resp = urlopen(req)
            if resp.code != 200:
                err_msg = "Failed setting landing page with error code: {}.".format(resp.code)
        except (ValueError, HTTPError, URLError, KeyError) as e:
            err_msg = "Failed to set landing page: {}".format(e)

        if not err_msg:
            Output.log("Successfully set landing page.")
        else:
            Output.log(err_msg, logging.ERROR)
        return err_msg

    # Sets YugaWare to input security level.
    def set_security(self, level):
        target = "{}/customers/{}/security".format(self.url, self.cust_uuid)
        headers = {
            "X-AUTH-YW-API-TOKEN": self.api_token_secure,
            "Content-Type": "application/json",
        }
        data = {"level": level}
        err_msg = None
        Output.log("Updating YW security to {}...".format(level))
        try:
            req = Request(target, headers=headers, data=json.dumps(data).encode('utf8'))
            req.get_method = lambda: "PUT"
            resp = urlopen(req)
            if resp.code != 200:
                err_msg = "Failed to set security level. YW returned code: " + resp.code
        except (ValueError, HTTPError, URLError, KeyError) as e:
            err_msg = "Failed to set security level: {}".format(e)

        if not err_msg:
            Output.log("Sucesssfully set YW security to {}".format(level))
        else:
            Output.log(err_msg, logging.ERROR)
        return err_msg

    # Add alerts to YugaWare.
    def send_alerts(self, alerts):
        target = "{}/customers/{}/alerts".format(self.url, self.cust_uuid)
        headers = {
            "X-AUTH-YW-API-TOKEN": self.api_token_secure,
            "Content-Type": "application/json",
        }
        Output.log("Adding alerts: {}".format(alerts))
        for alert in alerts:
            data = {
                "type": alert[0],
                "errCode": alert[1],
                "message": alert[2]
            }
            try:
                req = Request(target, headers=headers, data=json.dumps(data).encode('utf8'))
                req.get_method = lambda: "PUT"
                resp = urlopen(req)
                if resp.code != 200:
                    Output.log(
                        "Got error code {} when adding alert: {}".format(resp.code, alert),
                        logging.ERROR)
            except (ValueError, HTTPError, URLError, KeyError) as e:
                Output.log("Failed adding alert {} with error: {}".format(alert, e), logging.ERROR)
        del alerts[:]


# Class that handles any output operations. Use print for what users should see.
# Use log for logging. ANSI escape characters should not be used for logging.
class Output(object):
    supports_color = (sys.platform != 'win32' or 'ANSICON' in os.environ) and \
        hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()
    END = "\u001b[0m"
    BOLD = "\u001b[1m"
    UNDERLINE = "\u001b[4m"
    RED = "\u001b[31m"
    GREEN = "\u001b[32m"
    YELLOW = "\u001b[33m"
    BLUE = "\u001b[34m"
    MAGENTA = "\u001b[35m"
    CYAN = "\u001b[36m"
    # Transform to an "extended ASCII" library to parse the string then print it as unicode
    ROCKET = "\xf0\x9f\x9a\x80".encode('latin1').decode('utf8')
    PARTY = "\xf0\x9f\x8e\x89".encode('latin1').decode('utf8')
    SHIRT = "\xf0\x9f\x91\x95".encode('latin1').decode('utf8')
    log_dir = None
    script_exit_func = None
    # Only attempt to write to stdout while we have access to console.
    console_access = True

    ANIMATION_SUCCESS = '\u2705'
    ANIMATION_FAIL = '\u274C'
    ANIMATION_RUNNING = 1
    ANIMATION_WARNING = '\u26A0'
    # Tuple of (animation_status, animation_message) to ensure atomicity.
    animation_status = (ANIMATION_SUCCESS, "")
    animation_thread = None

    @staticmethod
    def print_out(msg):
        if not Output.console_access:
            return

        try:
            try:
                if PY_VERSION < 3:
                    print(msg.encode('utf8'))
                else:
                    print(msg)
            except UnicodeEncodeError:
                print(msg.encode('ascii', 'ignore').decode())
        except Exception as e:
            # Ignore any print errors as they are not critical to the application.
            Output.log("Failed to print with error: {}".format(traceback.format_exc()))

    # Writes one line that may be replaced with the update_animation method.
    # Note - ONLY one line can be replaced. E.g. only characters after a newline can be replaced.
    @staticmethod
    def init_animation(msg):
        if not Output.console_access:
            return

        def animate():
            loading_symbols = ['/', '-', '\\', '|']
            line_len = 0
            i = 0
            running = True
            while running and Output.console_access:
                status, msg = Output.animation_status
                if status == Output.ANIMATION_RUNNING:
                    symbol = loading_symbols[i]
                else:
                    symbol = status
                    running = False

                line = "\r{} {}".format(symbol, msg)
                line_len = max(len(line), line_len)
                line_to_write = "{:<{}}".format(line, line_len)
                if not running:
                    line_to_write += "\n"

                try:
                    try:
                        if PY_VERSION < 3:
                            sys.stdout.write(line_to_write.encode('utf-8'))
                        else:
                            sys.stdout.write(line_to_write)
                    except UnicodeEncodeError:
                        sys.stdout.write(line_to_write.encode('ascii', 'ignore').decode())

                    try:
                        sys.stdout.flush()
                    except IOError as e:
                        Output.log("Errored when flushing stdout: {}".format(e), logging.ERROR)
                    i = (i + 1) % len(loading_symbols)
                except Exception as e:
                    # Ignore stdout write errors as they are not critical to application.
                    Output.log("Failed stdout write with error: {}".format(traceback.format_exc()))

                time.sleep(.05)

        Output.animation_status = (Output.ANIMATION_RUNNING, msg)
        Output.animation_thread = Thread(target=animate)
        Output.animation_thread.start()

    @staticmethod
    def update_animation(msg, status=ANIMATION_SUCCESS):
        if not Output.console_access:
            return

        if not Output.animation_thread:
            Output.print_out(msg)

        Output.animation_status = (status, msg)
        if status != Output.ANIMATION_RUNNING and Output.animation_thread:
            Output.animation_thread.join()
            Output.animation_thread = None

    @staticmethod
    def log(msg, level=logging.INFO):
        if '' == msg or msg is None:
            return

        full_msg = msg
        time_since_sec = time.time() - start_time_sec
        if time_since_sec < 1000:
            # add time since start to make it easier to debug startup perf
            full_msg = " | {:.1f}s | {}".format(time_since_sec, msg)
        try:
            logging.log(level, full_msg)
        except:
            pass

    @staticmethod
    def print_and_log(msg, level=logging.INFO):
        Output.log(msg, level=level)
        Output.print_out(msg)

    @staticmethod
    def log_error_and_exit(msg):
        if Output.log_dir:
            msg += "\nFor more information, check the logs in {}".format(Output.log_dir)
        Output.print_and_log(msg, logging.ERROR)
        Output.console_access = False
        if Output.script_exit_func:
            Output.script_exit_func()
        sys.exit(1)

    @staticmethod
    def make_bold(msg):
        return Output.BOLD + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_underline(msg):
        return Output.UNDERLINE + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_red(msg):
        return Output.RED + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_green(msg):
        return Output.GREEN + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_yellow(msg):
        return Output.YELLOW + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_blue(msg):
        return Output.BLUE + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_magenta(msg):
        return Output.MAGENTA + msg + Output.END if Output.supports_color else msg

    @staticmethod
    def make_cyan(msg):
        return Output.CYAN + msg + Output.END if Output.supports_color else msg


# Class to customize argparse output.
class PrettyArgParser(argparse.ArgumentParser):
    def __init__(self, **kwargs):
        kwargs["formatter_class"] = PrettyHelpFormatter
        kwargs["epilog"] = EPILOG_COMMON
        super(PrettyArgParser, self).__init__(**kwargs)
        self._positionals.title = Output.make_yellow("Commands")
        self._optionals.title = Output.make_yellow("Flags")

    # Add epilog help message to errors.
    def error(self, message):
        Output.print_out("{} {}.".format(Output.make_red("Error:"), message))
        self.print_help(sys.stderr)
        self.exit(2)


# Class that capitalizes argparse help message.
class PrettyHelpFormatter(argparse.RawTextHelpFormatter):
    # Add prefix of cli title and change "Usage" to yellow
    # Add usage according to the command from which help is called
    def add_usage(self, usage, actions, groups, prefix=None):
        cmd = ('%(prog)s' % dict(prog=self._prog)).split()
        for cmds in ['[command]', '[flags]']:
            if cmds in cmd:
                cmd.remove(cmds)
        if len(cmd) > 2:
            temp = ""
            for cmds in cmd[1:]:
                temp += cmds + " "
            cmd = temp.strip()
        else:
            cmd = cmd.pop()
        if prefix is None:
            prefix = get_cli_title()
            prefix += PREFIX[cmd] + "\n\n"
            prefix += Output.make_yellow('Usage') + ": "
        if usage is None:
            usage = USAGE[cmd]
        super(PrettyHelpFormatter, self).add_usage(
            usage, actions, groups, prefix)

    # Sort the flags in alphabetical order for the help message
    def add_arguments(self, actions):
        actions = sorted(actions, key=operator.attrgetter('option_strings'))
        super(PrettyHelpFormatter, self).add_arguments(actions)

    # Remove the help under "Command" section which had all command inside curly braces
    def _format_action(self, action):
        self._max_help_position = self._action_max_length + 2
        result = super(PrettyHelpFormatter,
                       self)._format_action(action)
        if isinstance(action, argparse._SubParsersAction):
            return "%*s%s" % (self._current_indent, "", result.lstrip())
        return result

    # Remove the help under "Command" section which had all command inside curly braces
    def _iter_indented_subactions(self, action):
        if isinstance(action, argparse._SubParsersAction):
            try:
                get_subactions = action._get_subactions
            except AttributeError:
                pass
            else:
                for subaction in get_subactions():
                    yield subaction
        else:
            for subaction in super(PrettyHelpFormatter,
                                   self)._iter_indented_subactions(action):
                yield subaction

# Returns key-value pairs of input dict. Independent of python version.
def get_kv(map):
    if PY_VERSION < 3:
        return map.iteritems()
    else:
        return map.items()

def run_process(cmd, encrypted_cmd=None, timeout=None, log_cmd=False, env_vars=None, shell=False):
    if log_cmd:
        if encrypted_cmd:
            Output.log("run_process: cmd: {}".format(str(encrypted_cmd)))
        else:
            Output.log("run_process: cmd: {}".format(str(cmd)))

    proc = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE,
                            env=env_vars, shell=shell)
    if PY_VERSION >= 3:
        try:
            out, err = proc.communicate(timeout=timeout)
        except subprocess.TimeoutExpired as e:
            if encrypted_cmd:
                Output.log("run_process: {} timeout expired for command: ".format(encrypted_cmd))
            else:
                Output.log("run_process: {} timeout expired for command: ".format(cmd))
            return None, str(e), -1
    else:
        out, err = proc.communicate()
    (ret_out, ret_err, retcode) = (out.decode('utf-8'), err.decode('utf-8'), proc.returncode)
    if log_cmd:
        Output.log("run_process returned {}: \nOUT >>\n{}\n<< ERR >>\n{}\n<<".format(
            retcode, ret_out, ret_err))
    return (ret_out, ret_err, retcode)

def run_process_checked(cmd, timeout=None, log_cmd=True, env_vars=None):
    out, err, retcode = run_process(cmd=cmd, timeout=timeout, log_cmd=log_cmd, env_vars=env_vars)
    if retcode:
        Output.log_error_and_exit("Error: {}".format(err))
    return out

def rmcontents(dirname, exclude_names=[]):
    for f in os.listdir(dirname):
        if f in exclude_names:
            continue
        fullpath = os.path.join(dirname, f)
        if os.path.islink(fullpath) or os.path.isfile(fullpath):
            os.unlink(fullpath)
            continue
        if os.path.isdir(fullpath):
            shutil.rmtree(fullpath)
            continue
        Output.log("Unexpected type of file : [ {} ]".format(fullpath))


class RetryableError(Exception):
    pass

# Retry as long as func throws RetryableError.
def retry_op(func, timeout=180):
    start_time = time.time()
    now = start_time
    while True:
        try:
            return func()
        except RetryableError:
            pass
        now = time.time()
        if now - start_time > timeout:
            break
        time.sleep(0.5)

    raise RuntimeError("Failed after retrying operation for {} secs.".format(
        now - start_time))

# Retry the function with argument as long as func throws RetryableError.
def retry_op_with_argument(func, argument, timeout=180):
    start_time = time.time()
    now = start_time
    while True:
        try:
            return func(argument)
        except RetryableError:
            pass
        now = time.time()
        if now - start_time > timeout:
            break
        time.sleep(0.5)

    raise RuntimeError("Failed after retrying operation for {} secs.".format(
        now - start_time))


class EnvBasedCredentials(object):
    _ysql_user = os.environ.get('YSQL_USER')
    _ysql_password = os.environ.get('YSQL_PASSWORD')
    _ysql_db = os.environ.get('YSQL_DB')
    _ycql_user = os.environ.get('YCQL_USER')
    _ycql_password = os.environ.get('YCQL_PASSWORD')
    _ycql_keyspace = os.environ.get('YCQL_KEYSPACE')
    _cert_file_path = os.environ.get('SSL_CERTFILE')

    def attrsetter(attr):
        def set_any(self, value):
            setattr(type(self), attr, value)
        return set_any

    def attrgetter(attr):
        def get_any(self):
            return getattr(self, attr)
        return get_any

    ysql_user = property(attrgetter('_ysql_user'), attrsetter('_ysql_user'))
    ysql_password = property(attrgetter('_ysql_password'), attrsetter('_ysql_password'))
    ysql_db = property(attrgetter('_ysql_db'), attrsetter('_ysql_db'))
    ycql_user = property(attrgetter('_ycql_user'), attrsetter('_ycql_user'))
    ycql_password = property(attrgetter('_ycql_password'), attrsetter('_ycql_password'))
    ycql_keyspace = property(attrgetter('_ycql_keyspace'), attrsetter('_ycql_keyspace'))
    cert_file_path = property(attrgetter('_cert_file_path'), attrsetter('_cert_file_path'))

    def get_ysql_user(self):
        return self.ysql_user or DEFAULT_YSQL_USER

    def get_ysql_password(self):
        return self.ysql_password or DEFAULT_YSQL_PASSWORD

    def get_ysql_db(self):
        return self.ysql_db or DEFAULT_YSQL_DB

    def get_ycql_user(self):
        return self.ycql_user or DEFAULT_YCQL_USER

    def get_ycql_password(self):
        return self.ycql_password or DEFAULT_YCQL_PASSWORD

    def get_ycql_keyspace(self):
        return self.ycql_keyspace or DEFAULT_YCQL_KEYSPACE

    def get_cert_file_path(self):
        return self.cert_file_path

    def get_ysql_credentials(self, get_default_credentials=False):
        if get_default_credentials:
            return DEFAULT_YSQL_USER, DEFAULT_YSQL_PASSWORD, DEFAULT_YSQL_DB
        else:
            return self.get_ysql_user(), self.get_ysql_password(), self.get_ysql_db()

    def get_ycql_credentials(self, get_default_credentials=False):
        if get_default_credentials:
            return DEFAULT_YCQL_USER, DEFAULT_YCQL_PASSWORD, DEFAULT_YCQL_KEYSPACE
        else:
            return self.get_ycql_user(), self.get_ycql_password(), self.get_ycql_keyspace()

    def is_exists(self, var_to_check):
        return True if var_to_check in os.environ and os.environ.get(var_to_check) else False

    def set_ysql_password(self, password):
        self.ysql_password = password

    def set_ycql_password(self, password):
        self.ycql_password = password

    def update_passwords(self, new_password):
        self.set_ysql_password(new_password)
        self.set_ycql_password(new_password)

    def set_ysql_user(self, user):
        self.ysql_user = user

    def set_ysql_db(self, db):
        self.ysql_db = db

    def set_ycql_user(self, user):
        self.ycql_user = user

    def set_ycql_keyspace(self, keyspace):
        self.ycql_keyspace = keyspace

    def setup_ysql_credentials(self, proxy_class):

        # Create DB
        if self.get_ysql_db() != DEFAULT_YSQL_DB and not proxy_class.db_exists(self.get_ysql_db()):
            proxy_class.create_db(self.get_ysql_db())

        # Update password for default user
        if self.get_ysql_user() == DEFAULT_YSQL_USER and self.get_ysql_password() != DEFAULT_YSQL_PASSWORD:
            proxy_class.update_password(self.get_ysql_password())

        # Create User
        if self.get_ysql_user() != DEFAULT_YSQL_USER and not proxy_class.user_exists(self.get_ysql_user()):
            proxy_class.create_user(self.get_ysql_user(), self.get_ysql_password())

            if self.get_ysql_db() != DEFAULT_YSQL_DB:
                proxy_class.db_owner(self.get_ysql_db(), self.get_ysql_user())

            # Note: Following lines will be commented till we can decide on default user deletion.
            #proxy_class.delete_user(self.get_ysql_user(), self.get_ysql_password())

    def setup_ycql_credentials(self, proxy_class):

        # Create YCQL Keyspace
        if self.get_ycql_keyspace() and not proxy_class.keyspace_exists(self.get_ycql_keyspace()):
            proxy_class.create_keyspace(self.get_ycql_keyspace())

        # Update password for default user
        if self.get_ycql_user() == DEFAULT_YCQL_USER and self.get_ycql_password() != DEFAULT_YCQL_PASSWORD:
            proxy_class.update_password(self.get_ycql_password())

        # Create user
        if self.get_ycql_user() != DEFAULT_YCQL_USER and not proxy_class.user_exists(self.get_ycql_user()):
            proxy_class.create_user(self.get_ycql_user(), self.get_ycql_password())

            # Note: Following lines will be commented till we can decide on default user deletion.
            #proxy_class.delete_user(self.get_ycql_user(), self.get_ycql_password())

    def setup_cert_file_path(self, cert_file_path):
        if self.get_cert_file_path() is None or self.get_cert_file_path != cert_file_path:
            os.environ['SSL_CERTFILE'] = cert_file_path
            self.cert_file_path = cert_file_path


if __name__ == '__main__':
    ControlScript().run()
