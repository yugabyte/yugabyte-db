---
title: How to Develop AI Apps Locally with Ollama and YugabyteDB
headerTitle: Similarity search using Ollama
linkTitle: Similarity search - Ollama
description: Learn to build LLM applications using Ollama.
image: /images/tutorials/ai/icons/ollama-icon.svg
headcontent: Use YugabyteDB as the database backend for LLM applications
menu:
  preview_tutorials:
    identifier: tutorials-ai-ollama
    parent: tutorials-ai-vector
    weight: 60
type: docs
---

This tutorial shows how you can use [Ollama](https://ollama.com/) to generate text embeddings. It features a Node.js application that uses a locally-running LLM to generate text-embeddings. This LLM generates embeddings for news article headlines and descriptions, which are stored in a YugabyteDB database using the [pgvector extension](../../../explore/ysql-language-features/pg-extensions/extension-pgvector/).

## Prerequisites

- [YugabyteDB v2.19+](https://download.yugabyte.com/)
- [Ollama](https://ollama.com/)
- Node.js V18+
- Docker

## Set up the application

Download the application and provide settings specific to your deployment:

1. Clone the repository.

   ```sh
   git clone https://github.com/YugabyteDB-Samples/ollama-news-archive.git
   ```

1. Install the application dependencies.

   ```sh
   git lfs fetch --all
   npm install
   cd backend/ && npm install
   cd news-app-ui/ && npm install
   ```

1. Configure the database connection parameters in `{project_directory/backend/index.js}`.

## Set up YugabyteDB

Start a 3-node YugabyteDB cluster in Docker (or feel free to use another deployment option):

```sh
# NOTE: if the ~/yb_docker_data already exists on your machine, delete and re-create it
mkdir ~/yb_docker_data

docker network create custom-network

docker run -d --name yugabytedb-node1 --net custom-network \
    -p 15433:15433 -p 7001:7000 -p 9001:9000 -p 5433:5433 \
    -v ~/yb_docker_data/node1:/home/yugabyte/yb_data --restart unless-stopped \
    yugabytedb/yugabyte:{{< yb-version version="preview" format="build">}} \
    bin/yugabyted start \
    --base_dir=/home/yugabyte/yb_data --background=false

docker run -d --name yugabytedb-node2 --net custom-network \
    -p 15434:15433 -p 7002:7000 -p 9002:9000 -p 5434:5433 \
    -v ~/yb_docker_data/node2:/home/yugabyte/yb_data --restart unless-stopped \
    yugabytedb/yugabyte:{{< yb-version version="preview" format="build">}} \
    bin/yugabyted start --join=yugabytedb-node1 \
    --base_dir=/home/yugabyte/yb_data --background=false

docker run -d --name yugabytedb-node3 --net custom-network \
    -p 15435:15433 -p 7003:7000 -p 9003:9000 -p 5435:5433 \
    -v ~/yb_docker_data/node3:/home/yugabyte/yb_data --restart unless-stopped \
    yugabytedb/yugabyte:{{< yb-version version="preview" format="build">}} \
    bin/yugabyted start --join=yugabytedb-node1 \
    --base_dir=/home/yugabyte/yb_data --background=false
```

Navigate to the YugabyteDB UI to confirm that the database is up and running, at <http://127.0.0.1:15433>.

## Get started with Ollama

Ollama provides installers for a variety of platforms, and the [Ollama models library](https://ollama.com/library) provides numerous models for a variety of use cases. This sample application uses `nomic-embed-text` to generate text embeddings. Unlike some models, such as Llama3, which need to be run using the Ollama CLI, embeddings can be generated by supplying the desired embedding model in a REST endpoint.

1. Pull the model using the Ollama CLI.

    ```sh
    ollama pull nomic-embed-text:latest
    ```

1. With Ollama up and running on your machine, run the following command to verify the installation:

    ```sh
    curl http://localhost:11434/api/embeddings -d '{
      "model": "nomic-embed-text",
      "prompt": "goalkeeper"
    }'
    ```

The following output is generated, providing a 768-dimensional embedding that can be stored in the database and used in similarity searches:

```output
{"embedding":[-0.6447112560272217,0.7907757759094238,-5.213506698608398,-0.3068113327026367,1.0435500144958496,-1.005386233329773,0.09141742438077927,0.4835842549800873,-1.3404604196548462,-0.2027662694454193,-1.247795581817627,1.249923586845398,1.9664828777313232,-0.4091946482658386,0.3923419713973999,...]}
```

## Load the schema and seed data

This application requires a database table with information about news stories. This schema includes a `news_stories` table.

1. Copy the schema to the first node's Docker container as follows:

    ```sh
    docker cp {project_dir}/database/schema.sql yugabytedb-node1:/home/database
    ```

1. Copy the seed data file to the Docker container as follows:

    ```sh
    docker cp {project_dir}/sql/data.csv yugabytedb-node1:/home/database
    ```

1. Execute the SQL files against the database:

    ```sh
    docker exec -it yugabytedb-node1 bin/ysqlsh -h yugabytedb-node1 -f /home/database/schema.sql
    docker exec -it yugabytedb-node1 bin/ysqlsh -h yugabytedb-node1 -c "\COPY news_stories(link,headline,category,short_description,authors,date,embeddings) from '/home/database/data.csv' DELIMITER ',' CSV HEADER;"
    ```

## Start the application

This Node.js application uses a locally-running LLM to produce text embeddings. It takes an input in natural language, as well as a news category, and returns a response from YugabyteDB. By converting text to embeddings, a similarity search is executed using `pgvector`.

1. Start the API server:

    ```sh
    node {project_dir}/backend/index.js
    ```

    ```output
    Server running at http://localhost:3000/
    ```

1. Query the `/search` endpoint with a relevant prompt and category. For instance:

    ```sh
    curl "http://localhost:3000/api/search?q=olympic%20gold%20medal&category=SPORTS"
    ```

    ```output
    {
        "data": [
            {
                "headline": "17-Year-Old Snowboarder Wins United Statesâ€™ First Gold Medal In Pyeongchang",
                "short_description": "â€œI havenâ€™t had time for it to sink in yet,\" Redmond Gerard said following his victory.",
                "link": "https://www.huffingtonpost.com/entry/red-gerard-gold-olympics_us_5a7fac94e4b0c6726e141850"
            },
            {
                "headline": "Brazil Finally Wins Olympic Soccer Gold And Everybody Is In Tears",
                "short_description": "Neymar cried. And then so did everyone else.",
                "link": "https://www.huffingtonpost.com/entry/brazil-tears-olympic-soccer-rio-2016_us_57b960b4e4b00d9c3a180858"
            },
            {
                "headline": "Simone Manuel And Simone Biles Pose For Ultimate Olympic Selfie",
                "short_description": "The gold medalists are feeling the love. ðŸ’›",
                "link": "https://www.huffingtonpost.com/entry/simone-biles-simone-manuel-selfie_us_57ae111de4b069e7e5052acf"
            },
            {
                "headline": "United States Wins 1,000th Olympic Gold Medal",
                "short_description": "That's a lot of victories.",
                "link": "https://www.huffingtonpost.com/entry/united-states-win-womens-4x100-medley-gold_us_57afd40fe4b007c36e4f0746"
            },
            {
                "headline": "German Team Doctor Recommends Olympians Drink A Beer After Competing",
                "short_description": "The country currently has 10 gold medals, the second-most of any nation.",
                "link": "https://www.huffingtonpost.com/entry/german-team-doctor-recommends-olympians-drink-a-beer-after-competing_us_5a8b8b20e4b09fc01e02a355"
            }
        ]
    } 
    ```

1. Run the UI and visit <http://localhost:5173> to search the news archives.

    ```sh
    cd news-app-ui
    npm run dev
    ```

    ```output
      VITE ready in 138 ms
      âžœ  Local:   http://localhost:5173/
    ```

![YugaNews Archives UI](/images/tutorials/ai/ai-ollama/yuganews_archives.png "YugaNews Archives UI")

## Review the application

The Node.js application relies on the [nomic-embed-text model](https://ollama.com/library/nomic-embed-text), running in Ollama, to generate text embeddings for a user input. These embeddings are then used to query YugabyteDB using similarity search to generate a response.

```js
# index.js
const express = require("express");
const ollama = require("ollama"); // Ollama Node.js client
const { Pool } = require("@yugabytedb/pg"); // the YugabyteDB Smart Driver for Node.js

...

// /api/search endpoint
app.get("/api/search", async (req, res) => {
  const query = req.query.q;
  const category = req.query.category;

  if (!query) {
    return res.status(400).json({ error: "Query parameter is required" });
  }

  try {
    // Generate text embeddings using Ollama API
    const data = {
      model: "nomic-embed-text",
      prompt: query,
    };

    const resp = await ollama.default.embeddings(data);
    const embeddings = `[${resp.embedding}]`;

    const results = await pool.query(
      "SELECT headline, short_description, link from news_stories where category = $2 ORDER BY embeddings <=> $1 LIMIT 5",
      [embeddings, category]
    );

    res.json({ data: results.rows });
  } catch (error) {
    console.error(
      "Error generating embeddings or saving to the database:",
      error
    );
    res.status(500).json({ error: "Internal server error" });
  }
});

...
```

The `/api/search` endpoint specifies the embedding model and prompt to be sent to Ollama to generate a vector representation. This is done using the Ollama JavaScript library. The response is then used to execute a cosine similarity search against the dataset stored in YugabyteDB using pgvector. Latency is reduced by pre-filtering by news category, thus reducing the search space.

This application is quite straightforward. Before executing similarity searches, embeddings need to be generated for each news story and subsequently stored in the database. You can see how this is done in the `generate_embeddings.js` script.

```javascript
# generate_embeddings.py

const ollama = require("ollama");
const fs = require("fs");
const path = require("path");
const lineReader = require("line-reader");

...
const processJsonlFile = async (filePath) => {
  return new Promise((resolve, reject) => {
    let newsStories = [];
    let linesRead = 0;
    lineReader.eachLine(filePath, async (line, last, callback) => {
      try {
        console.log(linesRead);

        const jsonObject = JSON.parse(line);
        // Process each JSON object as needed
        const data = {
          model: "nomic-embed-text",
          prompt: `${jsonObject.headline} ${jsonObject.short_description}`,
        };

        const embeddings = await ollama.default.embeddings(data);
        jsonObject.embeddings = `[${embeddings.embedding}]`;
        newsStories.push(jsonObject);
        linesRead += 1;

        if (linesRead === 100 || last === true) {
          writeToCSV(newsStories);
          linesRead = 0;
          newsStories = [];
        }

        if (last === true) {
          return resolve();
        }
      } catch (error) {
        console.error("Error parsing JSON line:", error);
      }
      callback();
    });
  });
};

```

This script reads a CSV file with each line representing a news story. By generating embeddings for a string combining each story's headline and short description fields, we're able to provide enough data for similarity searches. These records are then appended to an output CSV file in batches, to later be coped to the database. It's important to note that similarity searches must be conducted using the same embedding model, so this process would need to be repeated if the desired model should change in the future.

## Wrap-up

Ollama allows you to use LLMs locally or on-premises, with a wide array of models for different use cases.

For more information about Ollama, see the [Ollama documentation](https://ollama.com).

To learn more on integrating LLMs with YugabyteDB, check out the [LangChain and OpenAI](../ai-langchain-openai/) tutorial.
