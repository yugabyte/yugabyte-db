---
title: Database Migration Service
headerTitle: Database Migration Service
linkTitle: Database Migration Service
description: Overview of the yb_migrate database engine for migrating data and applications from other databases to YugabyteDB.
beta: /preview/faq/general/#what-is-the-definition-of-the-beta-feature-tag
menu:
  preview:
    identifier: yb-migrate
    parent: migrate
    weight: 720
isTocNested: true
showAsideToc: true
---

[yb_migrate](https://github.com/yugabyte/yb-db-migration) is an open-source database migration engine provided by YugabyteDB. It is a command line executable program that supports migrating databases from PostgreSQL, Oracle, and MySQL to a YugabyteDB database. yb_migrate addresses both steps of a database migration - schema-migration and data-migration.

{{< note title="Note" >}}

yb_migrate supports `offline` migration mode. The `online` migration mode is currently under development.

{{< /note >}}

- In the *offline migration* mode, the source database must not change during the migration. The offline migration is considered done when all the requested schema objects and data is migrated to the target database.

- In the *online migration* mode, the source database can continue to change. After the full initial migration, yb_migrate continues replicating source database changes to the target database. The process runs continuously, until you decide to switch-over to the YugabyteDB database.

A typical migration workflow using yb_migrate consists of following steps:

- Install yb_migrate on a "migrator machine".
- Use the `yb_migrate generateReport` command to generate a Migration Assessment Report. The report suggests changes to the PostgreSQL schema to make it appropriate for YugabyteDB.
- Use the `yb_migrate export schema` command to convert Source Database schema to Postgres format.
- Manually change the exported schema as suggested in the Migration Assessment Report.
- Use the `yb_migrate export data` command to dump the Source Database in the local files on the migrator machine.
- Use the `yb_migrate import schema` command to import the schema in the target YugabyteDB database.
- Use the `yb_migrate import data` command to import the data in the target YugabyteDB database.

## How does yb_migrate work?

[Diagram]

yb_migrate is installed on and performs a migration from a "migrator machine". A migrator machine must meet following requirements:
- Runs CentOS or Ubuntu.
- Can reach both source and target DB.
- Has local storage of at least 1.5 times the size of source DB.

yb_migrate keeps all of its migration state, including exported schema and data, in a local directory called "export directory". Before starting migration, you must create the directory on a file-system that has enough space to keep the entire data dump. Then you must provide the path of the export directory as a mandatory argument (`--export-dir`) to each invocation of the yb_migrate command.

The export directory has following sub-directories/files:
- `reports/` directory contains the generated Migration Assessment Report.
- `schema/` directory contains source DB schema translated to PostgreSQL. The schema is partitioned into smaller files by the schema object type e.g. tables, views, etc.
- `data/` directory contains TSV (Tab Separated Values) files that are passed to the COPY command on the target DB.
- `metainfo/` and `temp/` directories are used by yb_migrate for internal bookkeeping.
- `yb_migrate.log` contains log messages.

In the export phase, yb_migrate uses `ora2pg` (for Oracle and MySQL) or `pg_dump` (for PostgreSQL) to dump schema and data in the PostgreSQL format.

Given that YugabyteDB is a distributed database and uses storage format different from PostgreSQL, minor manual changes are required to the PostgreSQL schema dumped by the yb_migrate. The report, generated by the `yb_migrate generateReport` command, points at the various schema files that you should manually change before trying to import the schema.

In the import schema phase, yb_migrate simply applies the DDL SQL files (located in the `$EXPORT_DIR/schema` directory) to the target DB.

In the import data phase, yb_migrate splits the data dump files (from the `$EXPORT_DIR/data` directory) into smaller "batches"--each of which contains at most `--batch-size` number of records. yb_migrate concurrently ingests the batches such that all nodes of the target YugabyteDB cluster are utilized. The import data phase is designed to be "restartable"--if yb_migrate terminates when the data import was in progress, upon restart the data import resumes where it left off in the previous run.

## Limitations

- yb_migrate doesn't yet support following features:

  - BLOB and CLOB
  - TABLESPACEs
  - ALTER VIEW


## Installation

### Machine requirements
The machine where you will run the yb_migrate command must satisfy following requirements:
- Runs CentOS or Ubuntu.
- Can reach both source and target DB.
- Has local storage at least 1.5 times the size of source DB.
- You must have sudo access to the machine.

Follow the steps given below, to setup a machine where you can run yb_migrate:

- Open a terminal session to the migrator machine.

- Clone the yb_migrate git repository:

        git clone https://github.com/yugabyte/yb-db-migration.git

- Change the directory to `yb-db-migration/installer_scripts`:

        cd yb-db-migration/installer_scripts

- Depending on the Linux distribution you're running, execute the appropriate installer script:

        ./yb_migrate_installer__centos.sh
    or

        ./yb_migrate_installer__ubuntu.sh

  The scripts are interactive--they can ask `Y` or `N` responses.

  It is safe to execute the script multiple times. On each run, the script regenerates the `yb_migrate` executable based on the latest commit in the git repository.

   If the script fails for some reason, check the `yb_migrate_installer.log` in the current working directory.

- The script generates a `.yb_migrate_installer_bashrc` file in the home directory. Make sure to source the file so that correct environment variables are set:

        source ~/.yb_migrate_installer_bashrc

- Run `yb_migrate --help` to check whether you have a working yb_migrate installation.

---
# Database Migration Process

Migrating a database from the source database to the target database requires the following steps:
- Prepare the source database.
- Prepare the target database.
- Generate report.
- Export schema.
- Manually edit schema.
- Export data.
- Import schema.
- Import data.
- Verify target database.

Following sections provide details of each of the above steps.

## Prepare the Source database
- Create a database user and provide it with READ access to all the resources which need to be migrated. In the case of PostgreSQL and MySQL, yb_migrate also needs READ access on tables/views from the `information_schema`.
- You will need to provide the user and the source database details in the subsequent invocations of yb_migrate. For convenience, you can populate the information in the following environment variables:

        SOURCE_DB_TYPE=oracle
        SOURCE_DB_HOST=localhost
        SOURCE_DB_PORT=1521
        SOURCE_DB_USER=sakila
        SOURCE_DB_PASSWORD=password
        SOURCE_DB_NAME=pdb1
        SOURCE_DB_SCHEMA=sakila

    Replace values of the above environment variables as per your database details.

    SOURCE_DB_TYPE can be one of [`postgresql`, `mysql`, `oracle`].

- If you want yb_migrate to connect to the source database over SSL, refer to [SSL Connectivity](#ssl-connectivity) in the References section.

## Prepare the target database

- Create the target database in the YugabyteDB cluster. The database name can be the same or different from the source database name. If the target database name is not provided, yb_migrate assumes the name is the same as the source database.

        CREATE DATABASE sakila;

- For convenience, capture the database name in an environment variable:

        TARGET_DB_NAME=sakila

- Create a role with the superuser privileges. yb_migrate will use the role to connect to the target database. Capture the user and database details in environment variables.

        TARGET_DB_HOST=127.0.0.1
        TARGET_DB_PORT=5433
        TARGET_DB_USER=yugabyte
        TARGET_DB_PASSWORD=password

    By default, yb_migrate creates tables in the `public` schema. It is possible to instruct yb_migrate to migrate the database in a non-public schema. In that case, you provide the name of the target schema and yb_migrate takes care of creating it.

- If you want yb_migrate to connect to the target database over SSL, refer to [SSL Connectivity](#ssl-connectivity) in the References section.

## Create an export directory

Before you proceed with the actual migration steps, create an "export directory" in the local file-system on the migrator machine. yb_migrate uses the directory to store source data, schema files, and migration state. The file-system in which the directory resides must have enough free space to hold the entire source database. Create the directory and place its path in an environment variable.

        mkdir -p ~/export-dirs/sakila
        EXPORT_DIR=~/export-dirs/sakila

## Generate report

Using `ora2pg` and `pg_dump`, yb_migrate can easily extract and convert the source database schema to an equivalent PostgreSQL schema. But the schema may not be suitable, yet, to be imported into the YugabyteDB. Even though YugabyteDB is PostgreSQL compatible, given its distributed nature, you may need some minor changes to the schema. Refer [this document](#https://docs.google.com/document/d/1jCLiHDEHiYpgVObILDC_2Ormr-Kx36YhkqHXUCVGO1Q/edit#) to know more about modeling data for YugabyteDB.

The `yb_migrate generateReport` command analyses the PostgreSQL schema and prepares a report that lists the DDL statements that need changes. Here is a sample invocation of the command:

        yb_migrate generateReport --export-dir ${EXPORT_DIR} \
            --source-db-type ${SOURCE_DB_TYPE} \
            --source-db-host ${SOURCE_DB_HOST} \
            --source-db-user ${SOURCE_DB_USER} \
            --source-db-password ${SOURCE_DB_PASSWORD} \
            --source-db-name ${SOURCE_DB_NAME} \
            --source-db-schema ${SOURCE_DB_SCHEMA} \
            --output-format txt

The `--output-format` can be one of the `html`, `txt`, `json`, and `xml`.

The above command generates a report file in the `EXPORT_DIR/reports/`.

## Export schema

`yb_migrate export schema` command extracts the schema from the source database; converts it into PostgreSQL format (if the source database is Oracle or MySQL); and dumps the SQL DDL files in the `EXPORT_DIR/schema/*` directories.

An example invocation of the command looks like:

        yb_migrate export schema --export-dir ${EXPORT_DIR} \
            --source-db-type ${SOURCE_DB_TYPE} \
            --source-db-host ${SOURCE_DB_HOST} \
            --source-db-user ${SOURCE_DB_USER} \
            --source-db-password ${SOURCE_DB_PASSWORD} \
            --source-db-name ${SOURCE_DB_NAME} \
            --source-db-schema ${SOURCE_DB_SCHEMA}

## Manually edit the schema

Fix all the issues, listed in the generated migration report, by manually editing the SQL DDL files from the `EXPORT_DIR/schema/*`. In future, yb_migrate will take over some of these manual schema changes.

## Export data

Run the `yb_migrate export data` command to dump the source data into the `EXPORT_DIR/data` directory. For example,

        yb_migrate export data --export-dir ${EXPORT_DIR} \
            --source-db-type ${SOURCE_DB_TYPE} \
            --source-db-host ${SOURCE_DB_HOST} \
            --source-db-user ${SOURCE_DB_USER} \
            --source-db-password ${SOURCE_DB_PASSWORD} \
            --source-db-name ${SOURCE_DB_NAME} \
            --source-db-schema ${SOURCE_DB_SCHEMA}

The options passed to the command are similar to the `export schema` command.

To export only a subset of the tables, pass a comma separated list of table names in the `--table-list` argument.

To speed up the data export of larger source databases, you can pass values greater than 1 to the `--parallel-jobs` argument. It will cause yb_migrate to dump multiple tables concurrently.

## Import schema

Once you're done with manually editing the schema, you can use the `yb_migrate import schema` command to import the schema. For example,

        yb_migrate import schema --export-dir ${EXPORT_DIR} \
            --target-db-host ${TARGET_DB_HOST} \
            --target-db-port ${TARGET_DB_PORT} \
            --target-db-user ${TARGET_DB_USER} \
            --target-db-password ${TARGET_DB_PASSWORD:-''} \
            --target-db-name ${TARGET_DB_NAME}

If for some reason, yb_migrate terminates before it could import the entire schema, you can rerun it by adding `--ignore-exist` option.

 Note that, the `yb_migrate import schema` command does NOT import indexes, yet. This is done to speed up the data import phase. The indexes will be created by `yb_migrate import data` command after importing the data.

## Import data

After you have successfully exported the source data and imported the schema in the target database, you can now import the data using the `yb_migrate import data` command:

        yb_migrate import data --export-dir ${EXPORT_DIR} \
            --target-db-host ${TARGET_DB_HOST} \
            --target-db-port ${TARGET_DB_PORT} \
            --target-db-user ${TARGET_DB_USER} \
            --target-db-password ${TARGET_DB_PASSWORD:-''} \
            --target-db-name ${TARGET_DB_NAME}

The `yb_migrate import data` command reads data files located in the `EXPORT_DIR/data`.

The command, by default, creates one database connection to each of the nodes of the target YugabyteDB cluster. You can increase the number of connections by specifying the total connection count in the `--parallel-jobs` argument. The command will equally distribute the connections among all the nodes of the cluster.

The command splits the larger tables into smaller chunks--each containing at most `--batch-size` number of records. By default, the `--batch-size` is 100,000 records.

To get the overall progress of the data import operation, run the `yb_migrate import data status --export-dir ${EXPORT_DIR}` command in a different terminal window.

While importing a very large database, you should run the import data command in a `screen` session, so that the import is not terminated when the terminal session stops.

If the `yb_migrate import data` command terminates before it could complete the data ingestion, you can rerun it with the same arguments and the command will resume the data import from where it left off.

After successfully loading the data, the command creates the indexes listed in the schema.

## Verify target database

After the successful execution of the `yb_migrate import data` command, the automated part of the database migration process is considered as done. You should manually run validation queries on both source and target database to ensure that the data is correctly migrated. The validation queries can be as simple as checking the row count in each table or it can utilise some domain knowledge e.g. match the sum of the `amount` column in the `payments` table.

---

# References

## SSL Connectivity

You can instruct yb_migrate to connect to the source/target database over an SSL connection.

Connecting securely to any of the PostgreSQL, MySQL, and YugabyteDB requires you to pass a very similar set of arguments to the yb_migrate. For Oracle, on the other hand, requires a different set of arguments.

### PostgreSQL and MySQL

- `--source-ssl-mode (disable|allow|prefer|require|verify-ca|verify-full)`
    Value of this argument determines:
    - whether an encrypted connection is established between yb_migrate and the database server; and
    - whether the certificate of the database server is verified from a CA.

    Possible values and their meaning is given below:
    - `disable`: Only try a non-SSL connection.
    - `allow`: First try a non-SSL connection; if that fails, try an SSL connection. (Not supported for MySQL.)
    - `prefer` (default): First try an SSL connection; if that fails, try a non-SSL connection.
    - `require`: Only try an SSL connection. If a root CA file is present, verify the certificate in the same way as if verify-ca was specified.
    - `verify-ca`: Only try an SSL connection, and verify that the server certificate is issued by a trusted certificate authority (CA).
    - `verify-full`: Only try an SSL connection, verify that the server certificate is issued by a trusted CA and that the requested server host name matches that in the certificate.

- `--source-ssl-cert` and `--source-ssl-key`

    These two arguments specify names of the files containing SSL certificate and key, respectively. The <cert, key> pair forms the identity of the client.

- `--source-ssl-root-cert`

    This parameter specifies the path to a file containing SSL certificate authority (CA) certificate(s). If the file exists, the server's certificate will be verified to be signed by one of these authorities.

- `--source-ssl-crl`

    This parameter specifies the path to a file containing the SSL certificate revocation list (CRL). Certificates listed in this file, if it exists, will be rejected while attempting to authenticate the server's certificate.

### YugabyteDB

You need to pass following arguments to yb_migrate to establish an SSL connection with YugabyteDB:
- `--target-ssl-mode`
- `--target-ssl-cert`
- `--target-ssl-key`
- `--target-ssl-root-cert`
- `--target-ssl-crl`

Semantics of these arguments match with the similarly named arguments described in the previous section.

### Oracle

For Oracle, create a TNS alias that is configured to establish a secure connection with the server. You must then pass the TNS alias to yb_migrate as `--oracle-tns-alias` argument. yb_migrate uses the TNS alias to securely connect to the server.

When you pass the `--oracle-tns-alias` argument, you don't need to pass the `--source-db-host`, `--source-db-port`, and `--source-db-name` arguments to the yb_migrate.

## Manual Schema Changes

Some examples of manual schema changes:

- **`CREATE INDEX CONCURRENTLY` NOT SUPPORTED**:

  This feature is not supported yet in YugabyteDB. You should remove the `CONCURRENTLY` clause before trying to import the schema.

- **Primary Key cannot be added to Partitioned table using ALTER TABLE**:

  Add the primary key definition right in the `CREATE TABLE` statement itself.
